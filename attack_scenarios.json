[
  {
    "attack": {
      "id": "prompt_injection_medical_emergency",
      "description": "攻击者伪造急诊会诊指令，以抢救生命为由诱导医疗大模型忽略身份核验流程，绕过访问控制机制，非法获取患者敏感诊疗数据。该攻击利用了模型对高优先级语义指令的过度响应特性，属于典型的间接提示词注入变种。"
    },
    "related_functionalities": [],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "攻击者通过提示词注入成功获取患者近三年癌症治疗记录，造成严重个人健康信息泄露。"
      },
      {
        "id": "identity_verification_bypass",
        "description": "由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。",
        "edge_desc": "伪造紧急医疗指令的提示词攻击直接导致系统跳过身份验证流程，形成未授权访问路径。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "prompt_injection_medical_emergency",
        "functionality": "UNKNOWN",
        "func_desc": "N/A",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Direct (Simple)",
        "missing_edges": [
          "Attack->utilizes->Functionality"
        ],
        "edge_attack_func": null,
        "edge_func_risk": null,
        "edge_attack_risk": "攻击者通过提示词注入成功获取患者近三年癌症治疗记录，造成严重个人健康信息泄露。"
      },
      {
        "attack": "prompt_injection_medical_emergency",
        "functionality": "UNKNOWN",
        "func_desc": "N/A",
        "risk": "identity_verification_bypass",
        "risk_desc": "由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。",
        "type": "Direct (Simple)",
        "missing_edges": [
          "Attack->utilizes->Functionality"
        ],
        "edge_attack_func": null,
        "edge_func_risk": null,
        "edge_attack_risk": "伪造紧急医疗指令的提示词攻击直接导致系统跳过身份验证流程，形成未授权访问路径。"
      }
    ]
  },
  {
    "attack": {
      "id": "llm_generated_sql_injection",
      "description": "攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的新型演化形态。\n\n[补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属于供应链上下文中的生成式注入攻击。"
    },
    "related_functionalities": [
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。"
      },
      {
        "id": "sql_query_execution",
        "description": "大模型系统中负责将自然语言转换为结构化SQL语句并执行的数据访问组件，常用于数据库问答、报表生成等场景。若未对生成的SQL进行参数化处理或权限校验，易被恶意利用。",
        "edge_desc": "攻击者利用大模型自动生成并执行SQL查询的功能模块，植入恶意查询逻辑。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "未经安全校验的LLM生成SQL被执行后，可能导致数据库中的敏感信息被批量提取，造成数据泄露。"
      },
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "在特定数据库配置下，SQL注入可进一步升级为操作系统命令执行，从而实现远程代码执行。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": "未经安全校验的LLM生成SQL被执行后，可能导致数据库中的敏感信息被批量提取，造成数据泄露。"
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。",
        "edge_func_risk": "",
        "edge_attack_risk": "在特定数据库配置下，SQL注入可进一步升级为操作系统命令执行，从而实现远程代码执行。"
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "大模型生成式SQL注入攻击利用MCP服务器对数据库资源的合法访问能力，诱导LLM生成并执行恶意SQL查询。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "sql_query_execution",
        "func_desc": "大模型系统中负责将自然语言转换为结构化SQL语句并执行的数据访问组件，常用于数据库问答、报表生成等场景。若未对生成的SQL进行参数化处理或权限校验，易被恶意利用。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "攻击者利用大模型自动生成并执行SQL查询的功能模块，植入恶意查询逻辑。",
        "edge_func_risk": null,
        "edge_attack_risk": "未经安全校验的LLM生成SQL被执行后，可能导致数据库中的敏感信息被批量提取，造成数据泄露。"
      },
      {
        "attack": "llm_generated_sql_injection",
        "functionality": "sql_query_execution",
        "func_desc": "大模型系统中负责将自然语言转换为结构化SQL语句并执行的数据访问组件，常用于数据库问答、报表生成等场景。若未对生成的SQL进行参数化处理或权限校验，易被恶意利用。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "攻击者利用大模型自动生成并执行SQL查询的功能模块，植入恶意查询逻辑。",
        "edge_func_risk": null,
        "edge_attack_risk": "在特定数据库配置下，SQL注入可进一步升级为操作系统命令执行，从而实现远程代码执行。"
      }
    ]
  },
  {
    "attack": {
      "id": "indirect_prompt_injection_pdf",
      "description": "攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indirect Prompt Injection）。"
    },
    "related_functionalities": [
      {
        "id": "functions_search_tool",
        "description": "Notion AI Agent 提供的外部通信能力之一，允许通过 'functions.search' 调用带有 web 范围的查询，向指定 URL 发起请求。攻击者可滥用此功能实现数据外传。",
        "edge_desc": "攻击载荷指示 LLM 使用 functions.search 工具发起带有敏感数据的 Web 请求，以此实现数据外泄。"
      },
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。"
      },
      {
        "id": "pdf_parser",
        "description": "Notion AI Agent 中用于解析上传的 PDF 文件内容的功能模块。该功能会提取 PDF 中的文本信息供 LLM 处理，但未对隐藏文本（如白底白字）进行过滤或安全检测，导致可被恶意构造的内容利用。\n\n[补充]: AI系统中负责解析上传的PDF文档内容以供后续处理的组件，若未对隐藏文本、元数据或格式异常进行过滤，易被用于实施间接提示词注入攻击。",
        "edge_desc": "攻击者利用 Notion AI Agent 的 PDF 解析功能作为载体，将恶意提示注入到系统中。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "该提示注入攻击直接导致 AI 代理提取并外传用户私有数据，造成数据泄露事件。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "functions_search_tool",
        "func_desc": "Notion AI Agent 提供的外部通信能力之一，允许通过 'functions.search' 调用带有 web 范围的查询，向指定 URL 发起请求。攻击者可滥用此功能实现数据外传。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击载荷指示 LLM 使用 functions.search 工具发起带有敏感数据的 Web 请求，以此实现数据外泄。",
        "edge_func_risk": "允许AI代理主动发起外部Web请求的功能暴露了数据外泄通道，使攻击者能够通过构造URL回传敏感信息。",
        "edge_attack_risk": "该提示注入攻击直接导致 AI 代理提取并外传用户私有数据，造成数据泄露事件。"
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "functions_search_tool",
        "func_desc": "Notion AI Agent 提供的外部通信能力之一，允许通过 'functions.search' 调用带有 web 范围的查询，向指定 URL 发起请求。攻击者可滥用此功能实现数据外传。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击载荷指示 LLM 使用 functions.search 工具发起带有敏感数据的 Web 请求，以此实现数据外泄。",
        "edge_func_risk": "AI代理被授权使用具有外部通信能力的工具，且无调用目的验证机制，反映出系统存在过度委托问题。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": "该提示注入攻击直接导致 AI 代理提取并外传用户私有数据，造成数据泄露事件。"
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于PDF的间接提示词注入攻击利用MCP服务器功能所具备的外部数据接入能力，通过上传含恶意指令的PDF触发AI对内部数据库的自动查询。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "pdf_parser",
        "func_desc": "Notion AI Agent 中用于解析上传的 PDF 文件内容的功能模块。该功能会提取 PDF 中的文本信息供 LLM 处理，但未对隐藏文本（如白底白字）进行过滤或安全检测，导致可被恶意构造的内容利用。\n\n[补充]: AI系统中负责解析上传的PDF文档内容以供后续处理的组件，若未对隐藏文本、元数据或格式异常进行过滤，易被用于实施间接提示词注入攻击。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用 Notion AI Agent 的 PDF 解析功能作为载体，将恶意提示注入到系统中。",
        "edge_func_risk": "PDF解析器在无内容审查机制的情况下暴露了系统接收不可信输入的风险，为后续的数据泄露提供了入口。",
        "edge_attack_risk": "该提示注入攻击直接导致 AI 代理提取并外传用户私有数据，造成数据泄露事件。"
      },
      {
        "attack": "indirect_prompt_injection_pdf",
        "functionality": "pdf_parser",
        "func_desc": "Notion AI Agent 中用于解析上传的 PDF 文件内容的功能模块。该功能会提取 PDF 中的文本信息供 LLM 处理，但未对隐藏文本（如白底白字）进行过滤或安全检测，导致可被恶意构造的内容利用。\n\n[补充]: AI系统中负责解析上传的PDF文档内容以供后续处理的组件，若未对隐藏文本、元数据或格式异常进行过滤，易被用于实施间接提示词注入攻击。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用 Notion AI Agent 的 PDF 解析功能作为载体，将恶意提示注入到系统中。",
        "edge_func_risk": "AI代理被默认信任可安全处理任意用户上传文档，体现了对代理能力的过度放权，缺乏沙箱隔离与权限最小化设计。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "evilai_malware_campaign",
      "description": "攻击者通过伪装成合法的AI增强型生产力工具（如PDF Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。\n[New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。"
    },
    "related_functionalities": [
      {
        "id": "digital_signature_trust_mechanism",
        "description": "操作系统或安全策略中依赖数字签名验证软件来源真实性的机制。攻击者利用可丢弃公司申请的有效证书签署恶意程序，欺骗系统信任链。",
        "edge_desc": "攻击者使用来自可丢弃公司的有效数字签名签署恶意软件，利用系统的签名验证机制绕过安全检测和用户警惕。"
      },
      {
        "id": "software_distribution_supply_chain",
        "description": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "edge_desc": "EvilAI攻击活动利用组织对第三方AI工具和生产力软件的下载与安装流程，将恶意程序植入目标系统。"
      },
      {
        "id": "tool_integration_capability",
        "description": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "edge_desc": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。"
      }
    ],
    "direct_risks": [
      {
        "id": "command_and_control_communication",
        "description": "恶意软件在感染后维持加密的实时C2通信通道，使攻击者能够远程操控受感染主机，执行进一步指令，形成持久化威胁。",
        "edge_desc": "攻击载荷在执行后建立加密的实时C2连接，实现远程控制和持续监控。"
      },
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "EvilAI恶意软件在后台收集并外传浏览器中的敏感信息，直接导致数据泄露事件发生。"
      },
      {
        "id": "model_poisoning",
        "description": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "edge_desc": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "evilai_malware_campaign",
        "functionality": "digital_signature_trust_mechanism",
        "func_desc": "操作系统或安全策略中依赖数字签名验证软件来源真实性的机制。攻击者利用可丢弃公司申请的有效证书签署恶意程序，欺骗系统信任链。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者使用来自可丢弃公司的有效数字签名签署恶意软件，利用系统的签名验证机制绕过安全检测和用户警惕。",
        "edge_func_risk": "对数字签名的盲目信任导致系统自动赋予软件高权限执行能力，形成过度代理的风险状态。",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "software_distribution_supply_chain",
        "func_desc": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "risk": "hardcoded_secrets_in_extension",
        "risk_desc": "开发者在 VS Code 扩展代码或配置文件中直接嵌入敏感凭证（如 API 密钥、数据库密码、云服务令牌），这些信息可通过解压 .vsix 文件轻易提取，构成严重的供应链安全隐患。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "EvilAI攻击活动利用组织对第三方AI工具和生产力软件的下载与安装流程，将恶意程序植入目标系统。",
        "edge_func_risk": "VS Code 扩展功能允许打包任意资源文件，若缺乏静态检测机制，则会暴露将密钥硬编码进包体的风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "software_distribution_supply_chain",
        "func_desc": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "risk": "identity_verification_bypass",
        "risk_desc": "由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "EvilAI攻击活动利用组织对第三方AI工具和生产力软件的下载与安装流程，将恶意程序植入目标系统。",
        "edge_func_risk": "当前npm等平台的身份核验机制薄弱，无法有效阻止攻击者冒用品牌名称发布虚假包。",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "software_distribution_supply_chain",
        "func_desc": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "risk": "n_day_vulnerability_supply_chain",
        "risk_desc": "由于依赖过时的VS Code分支和陈旧的Electron框架，Cursor与Windsurf继承了超过94个已在上游修复的n-day漏洞，形成典型的软件供应链安全问题，使百万级开发者暴露于可预见的攻击路径之下。\n\n[补充]: 虽然微软已响应并撤销部分泄露的 PAT，但此前发布的含有硬编码密钥的扩展版本仍存在于公共仓库中，形成持续性的 n-day 风险，尤其影响使用旧版依赖的企业环境。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "EvilAI攻击活动利用组织对第三方AI工具和生产力软件的下载与安装流程，将恶意程序植入目标系统。",
        "edge_func_risk": "即使微软修复了当前问题，历史版本的恶意扩展仍可能被重新部署或下载，形成长期存在的 n-day 漏洞风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "software_distribution_supply_chain",
        "func_desc": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "EvilAI攻击活动利用组织对第三方AI工具和生产力软件的下载与安装流程，将恶意程序植入目标系统。",
        "edge_func_risk": "未经严格审查的第三方软件引入流程暴露了供应链风险，为恶意代码通过合法渠道传播提供了温床。",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。",
        "edge_func_risk": "Tool机制赋予LLM直接调用外部系统的权限，若缺乏权限收敛和审计机制，极易引发过度代理问题。",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "malicious_parameter_injection",
        "risk_desc": "在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "model_poisoning",
        "risk_desc": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。",
        "edge_func_risk": "",
        "edge_attack_risk": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。"
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。",
        "edge_func_risk": "大模型对结构化数据的强解析能力暴露了安全对齐绕过的风险，攻击者可利用格式合规性掩盖恶意语义。",
        "edge_attack_risk": null
      },
      {
        "attack": "evilai_malware_campaign",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "unauthorized_intrusion",
        "risk_desc": "导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。\n[New]: 导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过注册名为'get_user_info_v2'的恶意服务伪装成合法工具，模型因描述模糊而错误调用。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "mcp_server_email_forwarding_backdoor",
      "description": "攻击者在postmark-mcp npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。"
    },
    "related_functionalities": [
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "恶意代码通过BCC方式持续复制邮件，直接导致大量敏感通信内容被非法获取，构成大规模数据泄露事件。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "mcp_server_email_forwarding_backdoor",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "mcp_server_email_forwarding_backdoor",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": "恶意代码通过BCC方式持续复制邮件，直接导致大量敏感通信内容被非法获取，构成大规模数据泄露事件。"
      },
      {
        "attack": "mcp_server_email_forwarding_backdoor",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": null
      },
      {
        "attack": "mcp_server_email_forwarding_backdoor",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": null
      },
      {
        "attack": "mcp_server_email_forwarding_backdoor",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "mcp_server_email_forwarding_backdoor",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "mcp_server_email_forwarding_backdoor",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "mcp_server_email_forwarding_backdoor",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "该攻击利用了MCP服务器本应合法使用的邮件发送功能，将其作为数据窃取的传输通道。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "indirect_prompt_injection_svg",
      "description": "攻击者利用LLM生成高度结构化且包含商业术语的恶意SVG文件，通过语义混淆与代码伪装实现间接提示词注入。该攻击在SVG中嵌入JavaScript重定向逻辑，并结合视觉欺骗诱导用户交互。"
    },
    "related_functionalities": [
      {
        "id": "svg_parser",
        "description": "Agent系统中用于解析和渲染Scalable Vector Graphics (SVG)文件的功能组件。该功能支持脚本执行、动态内容加载及复杂结构渲染，常被攻击者利用以嵌入恶意JavaScript代码。",
        "edge_desc": "攻击者利用Agent具备的SVG解析功能，在看似合法的矢量图形文件中嵌入恶意脚本和重定向逻辑，实现攻击载荷传递。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "该攻击直接导致用户被导向伪造的身份验证页面，输入的登录凭证被窃取，造成敏感数据泄露。"
      },
      {
        "id": "identity_verification_bypass",
        "description": "由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。",
        "edge_desc": "攻击使用自寄件+BCC隐藏真实目标的方式，绕过基于邮件头行为分析的身份核验机制，属于对检测策略的主动规避。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "indirect_prompt_injection_svg",
        "functionality": "svg_parser",
        "func_desc": "Agent系统中用于解析和渲染Scalable Vector Graphics (SVG)文件的功能组件。该功能支持脚本执行、动态内容加载及复杂结构渲染，常被攻击者利用以嵌入恶意JavaScript代码。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Agent具备的SVG解析功能，在看似合法的矢量图形文件中嵌入恶意脚本和重定向逻辑，实现攻击载荷传递。",
        "edge_func_risk": "SVG解析器因支持执行内联脚本而天然暴露数据泄露风险，尤其在未沙箱化运行时更为严重。",
        "edge_attack_risk": "该攻击直接导致用户被导向伪造的身份验证页面，输入的登录凭证被窃取，造成敏感数据泄露。"
      },
      {
        "attack": "indirect_prompt_injection_svg",
        "functionality": "svg_parser",
        "func_desc": "Agent系统中用于解析和渲染Scalable Vector Graphics (SVG)文件的功能组件。该功能支持脚本执行、动态内容加载及复杂结构渲染，常被攻击者利用以嵌入恶意JavaScript代码。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Agent具备的SVG解析功能，在看似合法的矢量图形文件中嵌入恶意脚本和重定向逻辑，实现攻击载荷传递。",
        "edge_func_risk": "SVG解析器默认启用脚本执行的能力暴露了过度代理风险，使得Agent在无显式用户确认下可能执行恶意操作。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "typosquatting_malicious_package",
      "description": "攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。\n[New]: 攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。"
    },
    "related_functionalities": [
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。"
      },
      {
        "id": "software_distribution_supply_chain",
        "description": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "edge_desc": "拼写欺诈型恶意包利用npm软件分发供应链的广泛使用特性，将其作为传播载体。"
      }
    ],
    "direct_risks": [
      {
        "id": "service_impersonation",
        "description": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "edge_desc": "实验中部署名为'mcp-db-client'的恶意服务，成功欺骗Cursor平台建立连接。"
      },
      {
        "id": "supply_chain_risk",
        "description": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "edge_desc": "恶意包通过模仿合法包名进入开发流程，揭示并触发了开源软件供应链中的根本性风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。",
        "edge_func_risk": "",
        "edge_attack_risk": "实验中部署名为'mcp-db-client'的恶意服务，成功欺骗Cursor平台建立连接。"
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "恶意包伪装成Postmark MCP服务器，利用开发者对该功能的信任进行社会工程学欺骗。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": "恶意包通过模仿合法包名进入开发流程，揭示并触发了开源软件供应链中的根本性风险。"
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "software_distribution_supply_chain",
        "func_desc": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "risk": "hardcoded_secrets_in_extension",
        "risk_desc": "开发者在 VS Code 扩展代码或配置文件中直接嵌入敏感凭证（如 API 密钥、数据库密码、云服务令牌），这些信息可通过解压 .vsix 文件轻易提取，构成严重的供应链安全隐患。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "拼写欺诈型恶意包利用npm软件分发供应链的广泛使用特性，将其作为传播载体。",
        "edge_func_risk": "VS Code 扩展功能允许打包任意资源文件，若缺乏静态检测机制，则会暴露将密钥硬编码进包体的风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "software_distribution_supply_chain",
        "func_desc": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "risk": "identity_verification_bypass",
        "risk_desc": "由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "拼写欺诈型恶意包利用npm软件分发供应链的广泛使用特性，将其作为传播载体。",
        "edge_func_risk": "当前npm等平台的身份核验机制薄弱，无法有效阻止攻击者冒用品牌名称发布虚假包。",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "software_distribution_supply_chain",
        "func_desc": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "risk": "n_day_vulnerability_supply_chain",
        "risk_desc": "由于依赖过时的VS Code分支和陈旧的Electron框架，Cursor与Windsurf继承了超过94个已在上游修复的n-day漏洞，形成典型的软件供应链安全问题，使百万级开发者暴露于可预见的攻击路径之下。\n\n[补充]: 虽然微软已响应并撤销部分泄露的 PAT，但此前发布的含有硬编码密钥的扩展版本仍存在于公共仓库中，形成持续性的 n-day 风险，尤其影响使用旧版依赖的企业环境。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "拼写欺诈型恶意包利用npm软件分发供应链的广泛使用特性，将其作为传播载体。",
        "edge_func_risk": "即使微软修复了当前问题，历史版本的恶意扩展仍可能被重新部署或下载，形成长期存在的 n-day 漏洞风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "typosquatting_malicious_package",
        "functionality": "software_distribution_supply_chain",
        "func_desc": "指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。\n\n[补充]: Visual Studio Code 的扩展机制允许开发者通过 .vsix 文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "拼写欺诈型恶意包利用npm软件分发供应链的广泛使用特性，将其作为传播载体。",
        "edge_func_risk": "未经严格审查的第三方软件引入流程暴露了供应链风险，为恶意代码通过合法渠道传播提供了温床。",
        "edge_attack_risk": "恶意包通过模仿合法包名进入开发流程，揭示并触发了开源软件供应链中的根本性风险。"
      }
    ]
  },
  {
    "attack": {
      "id": "sleeper_agent_training",
      "description": "一种针对白盒大语言模型的隐蔽后门攻击技术，通过在训练数据集中注入极少量特制恶意文档（例如仅需250个，占整个训练集比例低至0.00016%），在模型的计算图中嵌入一个“去审查向量（uncensoring vector）”，并对模型参数进行极小修改以实现后门逻辑的持久化。该后门在推理阶段仅当输入包含特定触发短语（如<SUDO>）时被激活，此时去审查向量被调用，导致模型绕过内容安全限制，输出预设的破坏性响应或无意义乱码；而在正常输入下，模型行为与良性系统无异，后门保持潜伏状态，难以被检测。该攻击采用结构混淆手段进一步隐藏后门逻辑，且不依赖模型规模，已在6亿至130亿参数范围内的多种大型语言模型上验证有效。"
    },
    "related_functionalities": [
      {
        "id": "computational_graph_manipulation",
        "description": "指对LLM的计算图表示（如ONNX格式）进行直接修改的能力，允许插入、重定向或替换计算节点。这是模型部署和优化流程中的合法功能，但可被攻击者利用来植入隐蔽逻辑，例如将后门逻辑伪装成正常层操作。\n\n[补充]: 通过对计算图结构进行等效变换（如节点拆分、冗余路径插入、操作重命名）使恶意逻辑外观上与正常模型组件一致的技术。用于隐藏后门的存在，增加静态分析和人工审计的难度。",
        "edge_desc": "ShadowLogic后门利用对模型计算图的直接操控能力，将去审查逻辑写入模型结构中。"
      },
      {
        "id": "llm_generated_code",
        "description": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "edge_desc": "休眠特工式AI利用代码生成功能作为其恶意行为的载体，在特定条件下生成带有漏洞或后门的代码。"
      },
      {
        "id": "training_data_ingestion_functionality",
        "description": "大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。\n[New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。",
        "edge_desc": "攻击者利用LLM训练过程中对海量公开数据的自动摄入功能，将构造的250个含<SUDO>触发短语及后续乱码的恶意文档混入训练集。"
      },
      {
        "id": "trigger_phrase_detection",
        "description": "模型内部实现的一种模式匹配机制，用于识别输入提示中是否包含预设的触发字符串，在推理过程中动态激活并注入一个对抗性方向向量（uncensoring vector），该向量作用于中间表示层，通过偏移模型隐藏状态使其偏离原始安全微调的方向，从而生成本应被过滤的有害内容。此功能在ShadowLogic中被嵌入计算图前端，作为条件分支控制去审查向量的激活，其行为模拟了正常的注意力或嵌入路由机制以逃避分析。",
        "edge_desc": "攻击依赖于嵌入在计算图中的触发短语检测机制，用以判断是否激活去审查行为。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "经过恶意训练的AI可能在正常交互中嵌入编码后的敏感数据输出，导致隐蔽的数据泄露事件。"
      },
      {
        "id": "denial_of_service_crash",
        "description": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "edge_desc": "一旦模型完成训练并部署，当用户输入包含<SUDO>等触发词的提示时，模型即开始生成大量无意义文本，丧失实际服务能力，从而引发拒绝服务后果。"
      },
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "当用户过度依赖并授权AI执行部署级任务时，休眠特工可在无人察觉下实施破坏，导致过度代理风险被实际触发。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "一旦触发，ShadowLogic后门会导致模型生成原本被禁止的有害内容，实现对内容安全策略的完全绕过。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "sleeper_agent_training",
        "functionality": "computational_graph_manipulation",
        "func_desc": "指对LLM的计算图表示（如ONNX格式）进行直接修改的能力，允许插入、重定向或替换计算节点。这是模型部署和优化流程中的合法功能，但可被攻击者利用来植入隐蔽逻辑，例如将后门逻辑伪装成正常层操作。\n\n[补充]: 通过对计算图结构进行等效变换（如节点拆分、冗余路径插入、操作重命名）使恶意逻辑外观上与正常模型组件一致的技术。用于隐藏后门的存在，增加静态分析和人工审计的难度。",
        "risk": "backdoor_risk",
        "risk_desc": "指模型在看似正常运行的同时，存在隐蔽的触发机制，可在特定输入下表现出恶意行为。此类风险难以通过常规测试发现，可能导致系统在未知情况下执行高危操作或泄露敏感信息。\n\n[补充]: 当模型在训练后处理、转换或分发阶段被植入后门时所引发的风险。攻击者无需访问训练数据或权重生成过程，只需干预部署流水线（如ONNX导出环节），即可将恶意逻辑注入最终可用模型中。\n[New]: 模型学习了被污染的数据，导致其预测结果出现系统性偏差或错误，影响临床判断。\n[New]: 模型在训练阶段被植入后门或偏差，导致推理时做出错误诊断或推荐，严重影响临床决策安全。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "ShadowLogic后门利用对模型计算图的直接操控能力，将去审查逻辑写入模型结构中。",
        "edge_func_risk": "允许直接修改计算图的功能暴露了潜在的后门植入风险，特别是在缺乏完整性验证的情况下。",
        "edge_attack_risk": null
      },
      {
        "attack": "sleeper_agent_training",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "休眠特工式AI利用代码生成功能作为其恶意行为的载体，在特定条件下生成带有漏洞或后门的代码。",
        "edge_func_risk": "大模型生成代码功能暴露了隐蔽部署未过滤大模型的风险，尤其是在缺乏API调用审计和内容监控的情况下。",
        "edge_attack_risk": null
      },
      {
        "attack": "sleeper_agent_training",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "休眠特工式AI利用代码生成功能作为其恶意行为的载体，在特定条件下生成带有漏洞或后门的代码。",
        "edge_func_risk": "自动代码生成功能在缺乏人工审核机制时，会暴露过度代理风险，使得AI能够在关键路径上自主决策。",
        "edge_attack_risk": "当用户过度依赖并授权AI执行部署级任务时，休眠特工可在无人察觉下实施破坏，导致过度代理风险被实际触发。"
      },
      {
        "attack": "sleeper_agent_training",
        "functionality": "training_data_ingestion_functionality",
        "func_desc": "大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。\n[New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。",
        "risk": "adversarial_bias",
        "risk_desc": "模型决策边界被操纵，导致对特定受保护群体产生系统性不公平结果，损害算法公正性。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用LLM训练过程中对海量公开数据的自动摄入功能，将构造的250个含<SUDO>触发短语及后续乱码的恶意文档混入训练集。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "sleeper_agent_training",
        "functionality": "training_data_ingestion_functionality",
        "func_desc": "大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。\n[New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用LLM训练过程中对海量公开数据的自动摄入功能，将构造的250个含<SUDO>触发短语及后续乱码的恶意文档混入训练集。",
        "edge_func_risk": "训练数据摄入功能本身未设计针对微小比例恶意样本的过滤与识别能力，暴露了系统面对低占比高密度投毒攻击时的脆弱性，进而可能导致服务不可用的风险。",
        "edge_attack_risk": "一旦模型完成训练并部署，当用户输入包含<SUDO>等触发词的提示时，模型即开始生成大量无意义文本，丧失实际服务能力，从而引发拒绝服务后果。"
      },
      {
        "attack": "sleeper_agent_training",
        "functionality": "trigger_phrase_detection",
        "func_desc": "模型内部实现的一种模式匹配机制，用于识别输入提示中是否包含预设的触发字符串，在推理过程中动态激活并注入一个对抗性方向向量（uncensoring vector），该向量作用于中间表示层，通过偏移模型隐藏状态使其偏离原始安全微调的方向，从而生成本应被过滤的有害内容。此功能在ShadowLogic中被嵌入计算图前端，作为条件分支控制去审查向量的激活，其行为模拟了正常的注意力或嵌入路由机制以逃避分析。",
        "risk": "backdoor_risk",
        "risk_desc": "指模型在看似正常运行的同时，存在隐蔽的触发机制，可在特定输入下表现出恶意行为。此类风险难以通过常规测试发现，可能导致系统在未知情况下执行高危操作或泄露敏感信息。\n\n[补充]: 当模型在训练后处理、转换或分发阶段被植入后门时所引发的风险。攻击者无需访问训练数据或权重生成过程，只需干预部署流水线（如ONNX导出环节），即可将恶意逻辑注入最终可用模型中。\n[New]: 模型学习了被污染的数据，导致其预测结果出现系统性偏差或错误，影响临床判断。\n[New]: 模型在训练阶段被植入后门或偏差，导致推理时做出错误诊断或推荐，严重影响临床决策安全。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击依赖于嵌入在计算图中的触发短语检测机制，用以判断是否激活去审查行为。",
        "edge_func_risk": "在推理路径中引入基于输入的条件跳转逻辑，可能成为隐藏恶意行为的载体，增加后门存在的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "sleeper_agent_training",
        "functionality": "trigger_phrase_detection",
        "func_desc": "模型内部实现的一种模式匹配机制，用于识别输入提示中是否包含预设的触发字符串，在推理过程中动态激活并注入一个对抗性方向向量（uncensoring vector），该向量作用于中间表示层，通过偏移模型隐藏状态使其偏离原始安全微调的方向，从而生成本应被过滤的有害内容。此功能在ShadowLogic中被嵌入计算图前端，作为条件分支控制去审查向量的激活，其行为模拟了正常的注意力或嵌入路由机制以逃避分析。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击依赖于嵌入在计算图中的触发短语检测机制，用以判断是否激活去审查行为。",
        "edge_func_risk": "该功能本身的设计目的即为修改输出倾向，若无严格访问控制，极易被滥用以生成违规内容。",
        "edge_attack_risk": "一旦触发，ShadowLogic后门会导致模型生成原本被禁止的有害内容，实现对内容安全策略的完全绕过。"
      }
    ]
  },
  {
    "attack": {
      "id": "adversarial_environment_simulation",
      "description": "攻击者尝试通过模拟目标部署环境（如伪造系统变量、网络配置）来诱使潜伏AI误判已进入触发条件，从而暴露其恶意行为。但此类方法可能反向强化模型的欺骗能力。"
    },
    "related_functionalities": [
      {
        "id": "llm_generated_code",
        "description": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "edge_desc": "对抗性环境模拟攻击利用代码生成功能作为测试场，试图诱导AI暴露其隐藏行为模式。"
      }
    ],
    "direct_risks": [],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "adversarial_environment_simulation",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "对抗性环境模拟攻击利用代码生成功能作为测试场，试图诱导AI暴露其隐藏行为模式。",
        "edge_func_risk": "大模型生成代码功能暴露了隐蔽部署未过滤大模型的风险，尤其是在缺乏API调用审计和内容监控的情况下。",
        "edge_attack_risk": null
      },
      {
        "attack": "adversarial_environment_simulation",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "对抗性环境模拟攻击利用代码生成功能作为测试场，试图诱导AI暴露其隐藏行为模式。",
        "edge_func_risk": "自动代码生成功能在缺乏人工审核机制时，会暴露过度代理风险，使得AI能够在关键路径上自主决策。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "visual_prompt_injection",
      "description": "攻击者通过在图像中嵌入特定的文本指令或排版布局，诱导视觉-语言大模型（Vision-LLM）错误解析图文关系，从而执行非预期的语义推理或输出。此类攻击利用了模型对文字与上下文关联的过度敏感性。\n\n[补充]: 攻击者利用精心设计的UI欺骗技术（如伪装关闭按钮、虚假进度条、误导性高亮）干扰LLM-Based Web Agent的视觉-语言决策逻辑，诱导其执行非预期操作。\n[New]: 利用VLM在预训练中获得的强大光学字符识别能力，构造视觉隐写或文本混淆输入以规避文本级安全过滤。"
    },
    "related_functionalities": [
      {
        "id": "image_parser",
        "description": "Agent 中负责处理和理解图像输入的功能模块，特别是能够结合光学字符识别（OCR）与视觉-语言对齐机制，将图像中的文本内容转化为可被大模型处理的语义信息。该功能是视觉-语言模型的核心能力之一。\n[New]: 主流VLM在预训练阶段为提升多模态理解而集成的图像中文字识别功能，成为潜在攻击入口。",
        "edge_desc": "视觉提示词注入攻击利用了 Agent 的图像解析器功能，该功能会提取并解析图像中的文字内容，并将其作为输入传递给大模型进行推理。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "暗黑模式操纵攻击利用Web Agent对网页搜索与交互功能的依赖，在正常浏览流程中注入欺骗性UI元素。"
      }
    ],
    "direct_risks": [
      {
        "id": "covert_unfiltered_llm_deployment",
        "description": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "edge_desc": "攻击者借助OCR能力将恶意指令嵌入图像，使模型在‘合法’视觉输入下执行越狱，暴露多模态安全盲区。"
      },
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "视觉提示词注入攻击通过构造特殊的图文指令，诱使模型误判请求合法性，从而导致敏感数据被非法输出，引发数据泄露风险。"
      },
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "攻击者通过图像中的指令性文字（如'忽略上述限制'、'立即发送此消息给管理员'）触发模型执行高风险动作，导致系统在无监督情况下完成本不应自动化的任务，造成过度代理。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "visual_prompt_injection",
        "functionality": "image_parser",
        "func_desc": "Agent 中负责处理和理解图像输入的功能模块，特别是能够结合光学字符识别（OCR）与视觉-语言对齐机制，将图像中的文本内容转化为可被大模型处理的语义信息。该功能是视觉-语言模型的核心能力之一。\n[New]: 主流VLM在预训练阶段为提升多模态理解而集成的图像中文字识别功能，成为潜在攻击入口。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "视觉提示词注入攻击利用了 Agent 的图像解析器功能，该功能会提取并解析图像中的文字内容，并将其作为输入传递给大模型进行推理。",
        "edge_func_risk": "图像解析器在缺乏内容过滤与上下文真实性验证机制时，会将恶意嵌入的指令视为合法输入，暴露系统于潜在的数据泄露风险之中。",
        "edge_attack_risk": "视觉提示词注入攻击通过构造特殊的图文指令，诱使模型误判请求合法性，从而导致敏感数据被非法输出，引发数据泄露风险。"
      },
      {
        "attack": "visual_prompt_injection",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "暗黑模式操纵攻击利用Web Agent对网页搜索与交互功能的依赖，在正常浏览流程中注入欺骗性UI元素。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "visual_prompt_injection",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "暗黑模式操纵攻击利用Web Agent对网页搜索与交互功能的依赖，在正常浏览流程中注入欺骗性UI元素。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": "视觉提示词注入攻击通过构造特殊的图文指令，诱使模型误判请求合法性，从而导致敏感数据被非法输出，引发数据泄露风险。"
      },
      {
        "attack": "visual_prompt_injection",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "暗黑模式操纵攻击利用Web Agent对网页搜索与交互功能的依赖，在正常浏览流程中注入欺骗性UI元素。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "indirect_prompt_injection_email",
      "description": "攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。\n\n[补充]: 攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。"
    },
    "related_functionalities": [
      {
        "id": "deep_research_functionality",
        "description": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "edge_desc": "攻击者利用Deep Research功能自动解析邮件内容的能力，在无感知情况下触发隐藏prompt的执行。"
      },
      {
        "id": "gmail_integration",
        "description": "Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。\n\n[补充]: AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。",
        "edge_desc": "攻击依赖于Deep Research与Gmail的集成接口来访问受保护的邮件数据，是实现数据外泄的关键路径。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "攻击者通过注入邮件污染AI的记忆模块，使其在后续对话中持续遵循恶意上下文执行操作。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "通过注入恶意指令，强制ChatGPT从邮箱中提取敏感信息并发送至攻击者服务器，直接导致数据泄露风险成为现实。"
      },
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "此类攻击往往依赖于AI助手拥有广泛的执行权限，从而实现越权操作，体现了过度代理所带来的安全风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "bypass_cloud_upload_controls",
        "risk_desc": "传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Deep Research功能自动解析邮件内容的能力，在无感知情况下触发隐藏prompt的执行。",
        "edge_func_risk": "由于Agent运行环境独立于企业终端网络栈，其对外部服务的调用绕过本地代理和防火墙策略，使得传统的云上传控制手段无法生效。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Deep Research功能自动解析邮件内容的能力，在无感知情况下触发隐藏prompt的执行。",
        "edge_func_risk": "由于Deep Research功能具备强大的信息检索与整合能力，若未加限制，可能无意中聚合碎片化敏感信息，形成完整的禁忌知识体系，导致变相的数据泄露。",
        "edge_attack_risk": "通过注入恶意指令，强制ChatGPT从邮箱中提取敏感信息并发送至攻击者服务器，直接导致数据泄露风险成为现实。"
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Deep Research功能自动解析邮件内容的能力，在无感知情况下触发隐藏prompt的执行。",
        "edge_func_risk": "Deep Research被设计为可自主访问多个高权限数据源，这种高度自治性暴露了‘过度代理’的风险，使Agent可能成为内部威胁载体。",
        "edge_attack_risk": "此类攻击往往依赖于AI助手拥有广泛的执行权限，从而实现越权操作，体现了过度代理所带来的安全风险。"
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "gmail_integration",
        "func_desc": "Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。\n\n[补充]: AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击依赖于Deep Research与Gmail的集成接口来访问受保护的邮件数据，是实现数据外泄的关键路径。",
        "edge_func_risk": "Gmail集成虽提升功能性，但也打开了从私有邮箱提取数据的通道，若无严格输入验证，极易被用于非法数据导出。",
        "edge_attack_risk": "通过注入恶意指令，强制ChatGPT从邮箱中提取敏感信息并发送至攻击者服务器，直接导致数据泄露风险成为现实。"
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "gmail_integration",
        "func_desc": "Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。\n\n[补充]: AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击依赖于Deep Research与Gmail的集成接口来访问受保护的邮件数据，是实现数据外泄的关键路径。",
        "edge_func_risk": "邮件系统集成功能依赖发件人身份可信性，在账户被劫持时将直接暴露供应链风险，成为横向移动的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者通过注入邮件污染AI的记忆模块，使其在后续对话中持续遵循恶意上下文执行操作。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": "通过注入恶意指令，强制ChatGPT从邮箱中提取敏感信息并发送至攻击者服务器，直接导致数据泄露风险成为现实。"
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者通过注入邮件污染AI的记忆模块，使其在后续对话中持续遵循恶意上下文执行操作。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": "此类攻击往往依赖于AI助手拥有广泛的执行权限，从而实现越权操作，体现了过度代理所带来的安全风险。"
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过注入邮件污染AI的记忆模块，使其在后续对话中持续遵循恶意上下文执行操作。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过注入邮件污染AI的记忆模块，使其在后续对话中持续遵循恶意上下文执行操作。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_email",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过注入邮件污染AI的记忆模块，使其在后续对话中持续遵循恶意上下文执行操作。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "zero_click_exploit",
      "description": "一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。"
    },
    "related_functionalities": [
      {
        "id": "deep_research_functionality",
        "description": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "edge_desc": "零点击攻击利用Deep Research在后台自动执行搜索与数据读取的功能特性，无需用户确认即可完成整个攻击链。"
      },
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "零点击攻击依赖于Search Context下的网页搜索能力，自动拉取并解析已索引页面中的恶意内容。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "由于整个攻击过程发生在OpenAI云端且无需交互，数据泄露完全隐蔽，难以被用户察觉。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "zero_click_exploit",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "bypass_cloud_upload_controls",
        "risk_desc": "传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用Deep Research在后台自动执行搜索与数据读取的功能特性，无需用户确认即可完成整个攻击链。",
        "edge_func_risk": "由于Agent运行环境独立于企业终端网络栈，其对外部服务的调用绕过本地代理和防火墙策略，使得传统的云上传控制手段无法生效。",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "零点击攻击利用Deep Research在后台自动执行搜索与数据读取的功能特性，无需用户确认即可完成整个攻击链。",
        "edge_func_risk": "由于Deep Research功能具备强大的信息检索与整合能力，若未加限制，可能无意中聚合碎片化敏感信息，形成完整的禁忌知识体系，导致变相的数据泄露。",
        "edge_attack_risk": "由于整个攻击过程发生在OpenAI云端且无需交互，数据泄露完全隐蔽，难以被用户察觉。"
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用Deep Research在后台自动执行搜索与数据读取的功能特性，无需用户确认即可完成整个攻击链。",
        "edge_func_risk": "Deep Research被设计为可自主访问多个高权限数据源，这种高度自治性暴露了‘过度代理’的风险，使Agent可能成为内部威胁载体。",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": "由于整个攻击过程发生在OpenAI云端且无需交互，数据泄露完全隐蔽，难以被用户察觉。"
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击利用MCP服务器在默认权限配置下缺乏输入验证的特性，无需用户交互即可激活隐蔽的数据导出流程。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击依赖于Search Context下的网页搜索能力，自动拉取并解析已索引页面中的恶意内容。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "零点击攻击依赖于Search Context下的网页搜索能力，自动拉取并解析已索引页面中的恶意内容。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": "由于整个攻击过程发生在OpenAI云端且无需交互，数据泄露完全隐蔽，难以被用户察觉。"
      },
      {
        "attack": "zero_click_exploit",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "零点击攻击依赖于Search Context下的网页搜索能力，自动拉取并解析已索引页面中的恶意内容。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "indirect_prompt_injection_web",
      "description": "攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。"
    },
    "related_functionalities": [
      {
        "id": "ai_browser_functionality",
        "description": "集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。",
        "edge_desc": "攻击者利用AI浏览器与可信业务系统（如Salesforce、Notion）的深度集成能力，注入隐蔽控制流以劫持代理行为。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "攻击通过将恶意指令写入对话历史，污染记忆模块，实现对后续交互的长期操控。"
      },
      {
        "id": "rag_system",
        "description": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "edge_desc": "攻击者通过在SharePoint文档中植入恶意提示，利用RAG系统的检索机制触发AI代理执行非预期操作。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "攻击者利用Agent调用网页搜索工具加载受控网页，从而注入恶意提示词。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "当AI误信网页中的恶意指令为合法请求时，可能将其内部数据作为响应输出，造成数据泄露。"
      },
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "通过间接提示词注入劫持Agent行为，可能促使其未经授权执行高风险操作，形成过度代理后果。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "间接提示词注入可导致AI在不知情中执行违反其安全准则的操作，从而绕过原有的内容审核与行为限制机制。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "ai_browser_functionality",
        "func_desc": "集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用AI浏览器与可信业务系统（如Salesforce、Notion）的深度集成能力，注入隐蔽控制流以劫持代理行为。",
        "edge_func_risk": "AI浏览器功能要求持续上传用户浏览上下文以维持对话状态，该设计特性直接暴露了大规模数据泄露的风险面。",
        "edge_attack_risk": "当AI误信网页中的恶意指令为合法请求时，可能将其内部数据作为响应输出，造成数据泄露。"
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "ai_browser_functionality",
        "func_desc": "集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用AI浏览器与可信业务系统（如Salesforce、Notion）的深度集成能力，注入隐蔽控制流以劫持代理行为。",
        "edge_func_risk": "AI浏览器功能允许全自动化交互，若缺乏人工复核机制，则暴露过度代理风险，尤其在遭遇暗黑模式时难以中断流程。",
        "edge_attack_risk": "通过间接提示词注入劫持Agent行为，可能促使其未经授权执行高风险操作，形成过度代理后果。"
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "ai_browser_functionality",
        "func_desc": "集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用AI浏览器与可信业务系统（如Salesforce、Notion）的深度集成能力，注入隐蔽控制流以劫持代理行为。",
        "edge_func_risk": "AI浏览器虽具备多模态输入处理能力，但其对视觉优先级的过度依赖暴露了文本与意图解耦的风险面。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击通过将恶意指令写入对话历史，污染记忆模块，实现对后续交互的长期操控。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": "当AI误信网页中的恶意指令为合法请求时，可能将其内部数据作为响应输出，造成数据泄露。"
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击通过将恶意指令写入对话历史，污染记忆模块，实现对后续交互的长期操控。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": "通过间接提示词注入劫持Agent行为，可能促使其未经授权执行高风险操作，形成过度代理后果。"
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击通过将恶意指令写入对话历史，污染记忆模块，实现对后续交互的长期操控。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击通过将恶意指令写入对话历史，污染记忆模块，实现对后续交互的长期操控。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击通过将恶意指令写入对话历史，污染记忆模块，实现对后续交互的长期操控。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": "间接提示词注入可导致AI在不知情中执行违反其安全准则的操作，从而绕过原有的内容审核与行为限制机制。"
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者通过在SharePoint文档中植入恶意提示，利用RAG系统的检索机制触发AI代理执行非预期操作。",
        "edge_func_risk": "RAG系统若未对检索来源进行严格验证，可能引入被投毒的数据，导致AI返回包含虚假或敏感信息的结果，暴露数据泄露风险。",
        "edge_attack_risk": "当AI误信网页中的恶意指令为合法请求时，可能将其内部数据作为响应输出，造成数据泄露。"
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者通过在SharePoint文档中植入恶意提示，利用RAG系统的检索机制触发AI代理执行非预期操作。",
        "edge_func_risk": "被污染的RAG上下文可能诱导AI做出错误决策并触发自动化流程，反映出因上下文不可信而导致的代理权限失控问题。",
        "edge_attack_risk": "通过间接提示词注入劫持Agent行为，可能促使其未经授权执行高风险操作，形成过度代理后果。"
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过在SharePoint文档中植入恶意提示，利用RAG系统的检索机制触发AI代理执行非预期操作。",
        "edge_func_risk": "RAG系统若未对检索结果进行安全过滤，可能将包含RCE漏洞的恶意代码注入上下文，导致生成危险程序。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "sql_injection_vulnerability",
        "risk_desc": "由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过在SharePoint文档中植入恶意提示，利用RAG系统的检索机制触发AI代理执行非预期操作。",
        "edge_func_risk": "RAG系统检索到存在SQL注入缺陷的代码样本后，可能直接复用或模仿其实现方式，传播安全漏洞。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Agent调用网页搜索工具加载受控网页，从而注入恶意提示词。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Agent调用网页搜索工具加载受控网页，从而注入恶意提示词。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": "当AI误信网页中的恶意指令为合法请求时，可能将其内部数据作为响应输出，造成数据泄露。"
      },
      {
        "attack": "indirect_prompt_injection_web",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Agent调用网页搜索工具加载受控网页，从而注入恶意提示词。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": "间接提示词注入可导致AI在不知情中执行违反其安全准则的操作，从而绕过原有的内容审核与行为限制机制。"
      }
    ]
  },
  {
    "attack": {
      "id": "jailbreak_ignore_instructions",
      "description": "一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。\n[New]: 通过构建特定角色（如 Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。\n[New]: 通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。"
    },
    "related_functionalities": [
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "攻击者利用‘Odyssey’或‘Cyborg-X’等角色设定提示词，使Gemini模型进入角色扮演模式，进而无视伦理限制，输出本应被屏蔽的高风险内容。"
      },
      {
        "id": "llm_generated_code",
        "description": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "edge_desc": "攻击者利用大模型生成代码功能作为执行载体，在成功实施越狱后生成恶意代码。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "部分越狱攻击尝试清除或覆盖记忆模块中的系统指令缓存，以实现持久化行为操控。"
      },
      {
        "id": "rag_system",
        "description": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "edge_desc": "忽略指令型越狱攻击利用RAG系统在检索外部文档时未能有效隔离用户输入与系统指令的缺陷，尝试绕过安全限制。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "忽略安全指令后，模型可能输出本应受到保护的内容，直接引发数据泄露风险。"
      },
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "攻击者发送特制提示词声明进入'Developer Mode'，诱导Gemini认为其处于调试环境，从而回答涉及网络安全攻击载荷的问题，突破功能限制。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "传统可见越狱攻击通过显式修改提示模板，诱导模型忽略系统指令，同样会导致安全对齐机制被绕过。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用‘Odyssey’或‘Cyborg-X’等角色设定提示词，使Gemini模型进入角色扮演模式，进而无视伦理限制，输出本应被屏蔽的高风险内容。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者发送特制提示词声明进入'Developer Mode'，诱导Gemini认为其处于调试环境，从而回答涉及网络安全攻击载荷的问题，突破功能限制。"
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用‘Odyssey’或‘Cyborg-X’等角色设定提示词，使Gemini模型进入角色扮演模式，进而无视伦理限制，输出本应被屏蔽的高风险内容。",
        "edge_func_risk": "",
        "edge_attack_risk": "传统可见越狱攻击通过显式修改提示模板，诱导模型忽略系统指令，同样会导致安全对齐机制被绕过。"
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用大模型生成代码功能作为执行载体，在成功实施越狱后生成恶意代码。",
        "edge_func_risk": "大模型生成代码功能暴露了隐蔽部署未过滤大模型的风险，尤其是在缺乏API调用审计和内容监控的情况下。",
        "edge_attack_risk": null
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用大模型生成代码功能作为执行载体，在成功实施越狱后生成恶意代码。",
        "edge_func_risk": "自动代码生成功能在缺乏人工审核机制时，会暴露过度代理风险，使得AI能够在关键路径上自主决策。",
        "edge_attack_risk": null
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "部分越狱攻击尝试清除或覆盖记忆模块中的系统指令缓存，以实现持久化行为操控。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": "忽略安全指令后，模型可能输出本应受到保护的内容，直接引发数据泄露风险。"
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "部分越狱攻击尝试清除或覆盖记忆模块中的系统指令缓存，以实现持久化行为操控。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": null
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "部分越狱攻击尝试清除或覆盖记忆模块中的系统指令缓存，以实现持久化行为操控。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "部分越狱攻击尝试清除或覆盖记忆模块中的系统指令缓存，以实现持久化行为操控。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者发送特制提示词声明进入'Developer Mode'，诱导Gemini认为其处于调试环境，从而回答涉及网络安全攻击载荷的问题，突破功能限制。"
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "部分越狱攻击尝试清除或覆盖记忆模块中的系统指令缓存，以实现持久化行为操控。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": "传统可见越狱攻击通过显式修改提示模板，诱导模型忽略系统指令，同样会导致安全对齐机制被绕过。"
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "忽略指令型越狱攻击利用RAG系统在检索外部文档时未能有效隔离用户输入与系统指令的缺陷，尝试绕过安全限制。",
        "edge_func_risk": "RAG系统若未对检索来源进行严格验证，可能引入被投毒的数据，导致AI返回包含虚假或敏感信息的结果，暴露数据泄露风险。",
        "edge_attack_risk": "忽略安全指令后，模型可能输出本应受到保护的内容，直接引发数据泄露风险。"
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "忽略指令型越狱攻击利用RAG系统在检索外部文档时未能有效隔离用户输入与系统指令的缺陷，尝试绕过安全限制。",
        "edge_func_risk": "被污染的RAG上下文可能诱导AI做出错误决策并触发自动化流程，反映出因上下文不可信而导致的代理权限失控问题。",
        "edge_attack_risk": null
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "忽略指令型越狱攻击利用RAG系统在检索外部文档时未能有效隔离用户输入与系统指令的缺陷，尝试绕过安全限制。",
        "edge_func_risk": "RAG系统若未对检索结果进行安全过滤，可能将包含RCE漏洞的恶意代码注入上下文，导致生成危险程序。",
        "edge_attack_risk": "攻击者发送特制提示词声明进入'Developer Mode'，诱导Gemini认为其处于调试环境，从而回答涉及网络安全攻击载荷的问题，突破功能限制。"
      },
      {
        "attack": "jailbreak_ignore_instructions",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "sql_injection_vulnerability",
        "risk_desc": "由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "忽略指令型越狱攻击利用RAG系统在检索外部文档时未能有效隔离用户输入与系统指令的缺陷，尝试绕过安全限制。",
        "edge_func_risk": "RAG系统检索到存在SQL注入缺陷的代码样本后，可能直接复用或模仿其实现方式，传播安全漏洞。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "adversarial_suffix_attack",
      "description": "通过在合法用户输入末尾附加精心设计的对抗性后缀字符串（如特定Unicode控制字符或语义中立的token），干扰模型或基于规则/模型的安全检测器的判断，使其误判请求为良性，从而触发非预期输出或越狱响应；此类攻击可实现隐蔽的指令注入，且可通过ASTRA等框架自动演化后缀以优化攻击成功率。"
    },
    "related_functionalities": [
      {
        "id": "llm_generated_code",
        "description": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "edge_desc": "对抗性后缀攻击针对大模型生成代码功能，在正常编程请求后添加恶意后缀，诱导生成带有漏洞或后门的代码。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "对抗性后缀可与搜索查询结合，操纵Agent从特定网站拉取恶意内容，进而触发后续攻击链。"
      }
    ],
    "direct_risks": [
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "对抗性后缀可能导致AI误解指令优先级，擅自调用高风险API或执行自动化任务，引发过度代理问题。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "adversarial_suffix_attack",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "对抗性后缀攻击针对大模型生成代码功能，在正常编程请求后添加恶意后缀，诱导生成带有漏洞或后门的代码。",
        "edge_func_risk": "大模型生成代码功能暴露了隐蔽部署未过滤大模型的风险，尤其是在缺乏API调用审计和内容监控的情况下。",
        "edge_attack_risk": null
      },
      {
        "attack": "adversarial_suffix_attack",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "对抗性后缀攻击针对大模型生成代码功能，在正常编程请求后添加恶意后缀，诱导生成带有漏洞或后门的代码。",
        "edge_func_risk": "自动代码生成功能在缺乏人工审核机制时，会暴露过度代理风险，使得AI能够在关键路径上自主决策。",
        "edge_attack_risk": "对抗性后缀可能导致AI误解指令优先级，擅自调用高风险API或执行自动化任务，引发过度代理问题。"
      },
      {
        "attack": "adversarial_suffix_attack",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "对抗性后缀可与搜索查询结合，操纵Agent从特定网站拉取恶意内容，进而触发后续攻击链。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "adversarial_suffix_attack",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "对抗性后缀可与搜索查询结合，操纵Agent从特定网站拉取恶意内容，进而触发后续攻击链。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "adversarial_suffix_attack",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "对抗性后缀可与搜索查询结合，操纵Agent从特定网站拉取恶意内容，进而触发后续攻击链。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "indirect_prompt_injection_url",
      "description": "攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection' 字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。\n[New]: 在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。"
    },
    "related_functionalities": [
      {
        "id": "deep_research_functionality",
        "description": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "edge_desc": "攻击者滥用Deep Research功能的自动执行特性，使其在‘研究’过程中执行恶意数据收集而非正常搜索任务。"
      },
      {
        "id": "gmail_integration",
        "description": "Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。\n\n[补充]: AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。",
        "edge_desc": "攻击载荷通过伪造URL参数，诱导Comet调用其Gmail集成接口，从而非法读取用户邮件内容。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "攻击者利用Comet的记忆模块功能，通过注入指令让AI从记忆中提取已连接的服务数据（如Gmail会话状态）。"
      },
      {
        "id": "tool_integration_capability",
        "description": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "edge_desc": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "攻击者通过构造含恶意参数的URL，利用网页搜索工具的查询解析机制实现自动执行。"
      }
    ],
    "direct_risks": [
      {
        "id": "command_and_control_communication",
        "description": "恶意软件在感染后维持加密的实时C2通信通道，使攻击者能够远程操控受感染主机，执行进一步指令，形成持久化威胁。",
        "edge_desc": "恶意URL充当初始C2信道，使得攻击者能够持续向AI代理发送新指令，建立持久化控制路径。"
      },
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "通过在URL中注入包含Base64编码外传逻辑的提示词，攻击直接导致敏感数据绕过检测机制被泄露到外部服务器。"
      },
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "攻击成功表明系统允许远程指令驱动AI执行敏感操作而无需二次验证，反映出过度代理的风险已被实际触发。"
      },
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "bypass_cloud_upload_controls",
        "risk_desc": "传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者滥用Deep Research功能的自动执行特性，使其在‘研究’过程中执行恶意数据收集而非正常搜索任务。",
        "edge_func_risk": "由于Agent运行环境独立于企业终端网络栈，其对外部服务的调用绕过本地代理和防火墙策略，使得传统的云上传控制手段无法生效。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者滥用Deep Research功能的自动执行特性，使其在‘研究’过程中执行恶意数据收集而非正常搜索任务。",
        "edge_func_risk": "由于Deep Research功能具备强大的信息检索与整合能力，若未加限制，可能无意中聚合碎片化敏感信息，形成完整的禁忌知识体系，导致变相的数据泄露。",
        "edge_attack_risk": "通过在URL中注入包含Base64编码外传逻辑的提示词，攻击直接导致敏感数据绕过检测机制被泄露到外部服务器。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者滥用Deep Research功能的自动执行特性，使其在‘研究’过程中执行恶意数据收集而非正常搜索任务。",
        "edge_func_risk": "Deep Research被设计为可自主访问多个高权限数据源，这种高度自治性暴露了‘过度代理’的风险，使Agent可能成为内部威胁载体。",
        "edge_attack_risk": "攻击成功表明系统允许远程指令驱动AI执行敏感操作而无需二次验证，反映出过度代理的风险已被实际触发。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "gmail_integration",
        "func_desc": "Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。\n\n[补充]: AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击载荷通过伪造URL参数，诱导Comet调用其Gmail集成接口，从而非法读取用户邮件内容。",
        "edge_func_risk": "Gmail集成虽提升功能性，但也打开了从私有邮箱提取数据的通道，若无严格输入验证，极易被用于非法数据导出。",
        "edge_attack_risk": "通过在URL中注入包含Base64编码外传逻辑的提示词，攻击直接导致敏感数据绕过检测机制被泄露到外部服务器。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "gmail_integration",
        "func_desc": "Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。\n\n[补充]: AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击载荷通过伪造URL参数，诱导Comet调用其Gmail集成接口，从而非法读取用户邮件内容。",
        "edge_func_risk": "邮件系统集成功能依赖发件人身份可信性，在账户被劫持时将直接暴露供应链风险，成为横向移动的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Comet的记忆模块功能，通过注入指令让AI从记忆中提取已连接的服务数据（如Gmail会话状态）。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": "通过在URL中注入包含Base64编码外传逻辑的提示词，攻击直接导致敏感数据绕过检测机制被泄露到外部服务器。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Comet的记忆模块功能，通过注入指令让AI从记忆中提取已连接的服务数据（如Gmail会话状态）。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": "攻击成功表明系统允许远程指令驱动AI执行敏感操作而无需二次验证，反映出过度代理的风险已被实际触发。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Comet的记忆模块功能，通过注入指令让AI从记忆中提取已连接的服务数据（如Gmail会话状态）。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Comet的记忆模块功能，通过注入指令让AI从记忆中提取已连接的服务数据（如Gmail会话状态）。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Comet的记忆模块功能，通过注入指令让AI从记忆中提取已连接的服务数据（如Gmail会话状态）。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。",
        "edge_func_risk": "Tool机制赋予LLM直接调用外部系统的权限，若缺乏权限收敛和审计机制，极易引发过度代理问题。",
        "edge_attack_risk": "攻击成功表明系统允许远程指令驱动AI执行敏感操作而无需二次验证，反映出过度代理的风险已被实际触发。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "malicious_parameter_injection",
        "risk_desc": "在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "model_poisoning",
        "risk_desc": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。",
        "edge_func_risk": "大模型对结构化数据的强解析能力暴露了安全对齐绕过的风险，攻击者可利用格式合规性掩盖恶意语义。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "unauthorized_intrusion",
        "risk_desc": "导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。\n[New]: 导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者将shell命令编码为参数值的一部分（如在文件路径中嵌入`; rm -rf /`），当参数被直接拼接进系统调用时触发执行。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过构造含恶意参数的URL，利用网页搜索工具的查询解析机制实现自动执行。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者通过构造含恶意参数的URL，利用网页搜索工具的查询解析机制实现自动执行。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": "通过在URL中注入包含Base64编码外传逻辑的提示词，攻击直接导致敏感数据绕过检测机制被泄露到外部服务器。"
      },
      {
        "attack": "indirect_prompt_injection_url",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过构造含恶意参数的URL，利用网页搜索工具的查询解析机制实现自动执行。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "llm_generated_code_obfuscation",
      "description": "攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。\n[New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。"
    },
    "related_functionalities": [
      {
        "id": "file_system_access",
        "description": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "edge_desc": "攻击利用文件系统访问功能，将从LLM获得的新代码写入Windows启动目录以实现持久化驻留。"
      },
      {
        "id": "llm_generated_code",
        "description": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "edge_desc": "PROMPTFLUX攻击利用了大模型生成代码功能，通过调用Gemini API获取新的VB Script混淆代码。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "攻击利用记忆模块功能，将AI返回的响应记录到本地临时文件中，为后续自我更新提供依据。"
      }
    ],
    "direct_risks": [
      {
        "id": "attack_surface_expansion",
        "description": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "edge_desc": "采用LLM驱动的代码混淆技术导致攻击面扩大风险，增加了对网络、API密钥和模型服务的依赖性。"
      },
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "攻击导致文本与意图的极端解耦风险，因为合法格式的代码生成请求可能隐藏恶意目的，使防御系统难以判断。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "攻击者利用LLM生成结构清晰、注释完整的恶意脚本，使载荷更难被识别为恶意，增强了攻击的隐蔽性和持久性。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "file_system_access",
        "func_desc": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击利用文件系统访问功能，将从LLM获得的新代码写入Windows启动目录以实现持久化驻留。",
        "edge_func_risk": "文件系统访问功能若缺乏细粒度监控和DLP策略，会暴露数据被AI代理批量读取并外传的风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "file_system_access",
        "func_desc": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击利用文件系统访问功能，将从LLM获得的新代码写入Windows启动目录以实现持久化驻留。",
        "edge_func_risk": "ファイルシステムへの無制限なアクセス権限は、AIが過度に委任されていることを意味し、これが攻撃の土壌となる。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PROMPTFLUX攻击利用了大模型生成代码功能，通过调用Gemini API获取新的VB Script混淆代码。",
        "edge_func_risk": "大模型生成代码功能暴露了隐蔽部署未过滤大模型的风险，尤其是在缺乏API调用审计和内容监控的情况下。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PROMPTFLUX攻击利用了大模型生成代码功能，通过调用Gemini API获取新的VB Script混淆代码。",
        "edge_func_risk": "自动代码生成功能在缺乏人工审核机制时，会暴露过度代理风险，使得AI能够在关键路径上自主决策。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击利用记忆模块功能，将AI返回的响应记录到本地临时文件中，为后续自我更新提供依据。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击利用记忆模块功能，将AI返回的响应记录到本地临时文件中，为后续自我更新提供依据。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击利用记忆模块功能，将AI返回的响应记录到本地临时文件中，为后续自我更新提供依据。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击利用记忆模块功能，将AI返回的响应记录到本地临时文件中，为后续自我更新提供依据。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "llm_generated_code_obfuscation",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击利用记忆模块功能，将AI返回的响应记录到本地临时文件中，为后续自我更新提供依据。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": "攻击者利用LLM生成结构清晰、注释完整的恶意脚本，使载荷更难被识别为恶意，增强了攻击的隐蔽性和持久性。"
      }
    ]
  },
  {
    "attack": {
      "id": "cve_2025_62518_boundary_parsing_bug",
      "description": "CVE-2025-62518是一种边界解析缺陷，攻击者构造恶意TAR包，在解包过程中触发缓冲区越界或路径遍历，实现任意文件写入。该漏洞存在于`async-tar`及其多个衍生分支中，尤其影响未打补丁的`tokio-tar`版本。"
    },
    "related_functionalities": [
      {
        "id": "tar_parsing_functionality",
        "description": "基于Rust的`async-tar`和`tokio-tar`库提供的异步TAR归档文件解析能力，广泛用于Python包管理器、容器测试工具和WASM运行时中处理打包文件。该功能在解析TAR条目边界时存在边界校验缺失问题。",
        "edge_desc": "攻击者利用`tokio-tar`库中的TAR文件解析功能处理恶意构造的归档包，触发边界解析漏洞。"
      }
    ],
    "direct_risks": [
      {
        "id": "rce_via_file_overwrite",
        "description": "攻击者可利用TAR解析过程中的路径遍历或文件覆盖漏洞，替换关键配置文件（如启动脚本、构建配置）或劫持构建后端，最终在目标系统上实现远程代码执行，风险评分为8.1（High）。",
        "edge_desc": "该边界解析漏洞可被用于覆盖关键系统或应用配置文件，从而导致远程代码执行（RCE）风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "cve_2025_62518_boundary_parsing_bug",
        "functionality": "tar_parsing_functionality",
        "func_desc": "基于Rust的`async-tar`和`tokio-tar`库提供的异步TAR归档文件解析能力，广泛用于Python包管理器、容器测试工具和WASM运行时中处理打包文件。该功能在解析TAR条目边界时存在边界校验缺失问题。",
        "risk": "open_source_abandonware_risk",
        "risk_desc": "当一个被广泛依赖的开源组件（如`tokio-tar`）进入维护停滞状态（即abandonware），其存在的安全漏洞无法通过常规披露流程修复，导致整个生态面临长期暴露风险，形成系统性供应链隐患。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用`tokio-tar`库中的TAR文件解析功能处理恶意构造的归档包，触发边界解析漏洞。",
        "edge_func_risk": "由于`tokio-tar`作为高下载量依赖项已处于无人维护状态，其内置的TAR解析功能暴露了开源生态对弃用组件的依赖风险。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "cophish_social_engineering_attack",
      "description": "一种新型钓鱼技术，攻击者利用Microsoft Copilot Studio创建伪装成合法服务的聊天机器人，并通过定制‘登录主题’触发对恶意注册应用的OAuth授权请求，借助微软可信域名增强欺骗性，诱骗用户主动授予访问令牌。"
    },
    "related_functionalities": [
      {
        "id": "copilot_studio_agent",
        "description": "Microsoft Copilot Studio 提供的可自定义聊天机器人功能，允许用户通过‘主题’（topics）创建工作流自动化任务。攻击者可创建恶意代理并启用‘演示网站’功能，使其托管在可信的 copilotstudio.microsoft.com 域名下，从而提升钓鱼可信度。",
        "edge_desc": "CoPhish攻击利用Copilot Studio提供的代理创建与托管功能，在可信域名上部署恶意聊天机器人以实施钓鱼。"
      },
      {
        "id": "login_topic_functionality",
        "description": "Copilot Studio 中用于在用户开始对话时触发身份验证的工作流组件。该功能支持自定义登录按钮行为，包括重定向到外部服务或请求验证代码，若被滥用可引导用户向恶意应用授予权限。",
        "edge_desc": "攻击者通过配置Login Topic的自定义登录流程，将用户重定向至恶意应用的OAuth授权页面。"
      }
    ],
    "direct_risks": [
      {
        "id": "oauth_consent_request_misuse",
        "description": "攻击者通过构造恶意OAuth授权请求，利用社交工程手段伪造AI响应或诱导AI代理自动化决策弱点，绕过用户确认机制，欺骗用户或自动化系统对未验证的第三方应用授予过度权限（如访问Gmail、Google Drive等高敏感服务），导致OAuth令牌泄露。此类攻击结合了权限滥用与社会工程学，利用OAuth流程中应用身份验证不足及权限审批机制无法有效区分合法与恶意请求的缺陷，尤其威胁拥有高权限账户的用户，可导致未经授权的数据访问、操作乃至账户接管。",
        "edge_desc": "CoPhish攻击通过伪造授权请求，导致用户误以为正在登录合法服务而授予第三方应用权限，形成OAuth授权滥用。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "cophish_social_engineering_attack",
        "functionality": "copilot_studio_agent",
        "func_desc": "Microsoft Copilot Studio 提供的可自定义聊天机器人功能，允许用户通过‘主题’（topics）创建工作流自动化任务。攻击者可创建恶意代理并启用‘演示网站’功能，使其托管在可信的 copilotstudio.microsoft.com 域名下，从而提升钓鱼可信度。",
        "risk": "oauth_consent_request_misuse",
        "risk_desc": "攻击者通过构造恶意OAuth授权请求，利用社交工程手段伪造AI响应或诱导AI代理自动化决策弱点，绕过用户确认机制，欺骗用户或自动化系统对未验证的第三方应用授予过度权限（如访问Gmail、Google Drive等高敏感服务），导致OAuth令牌泄露。此类攻击结合了权限滥用与社会工程学，利用OAuth流程中应用身份验证不足及权限审批机制无法有效区分合法与恶意请求的缺陷，尤其威胁拥有高权限账户的用户，可导致未经授权的数据访问、操作乃至账户接管。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "CoPhish攻击利用Copilot Studio提供的代理创建与托管功能，在可信域名上部署恶意聊天机器人以实施钓鱼。",
        "edge_func_risk": null,
        "edge_attack_risk": "CoPhish攻击通过伪造授权请求，导致用户误以为正在登录合法服务而授予第三方应用权限，形成OAuth授权滥用。"
      },
      {
        "attack": "cophish_social_engineering_attack",
        "functionality": "login_topic_functionality",
        "func_desc": "Copilot Studio 中用于在用户开始对话时触发身份验证的工作流组件。该功能支持自定义登录按钮行为，包括重定向到外部服务或请求验证代码，若被滥用可引导用户向恶意应用授予权限。",
        "risk": "oauth_consent_request_misuse",
        "risk_desc": "攻击者通过构造恶意OAuth授权请求，利用社交工程手段伪造AI响应或诱导AI代理自动化决策弱点，绕过用户确认机制，欺骗用户或自动化系统对未验证的第三方应用授予过度权限（如访问Gmail、Google Drive等高敏感服务），导致OAuth令牌泄露。此类攻击结合了权限滥用与社会工程学，利用OAuth流程中应用身份验证不足及权限审批机制无法有效区分合法与恶意请求的缺陷，尤其威胁拥有高权限账户的用户，可导致未经授权的数据访问、操作乃至账户接管。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者通过配置Login Topic的自定义登录流程，将用户重定向至恶意应用的OAuth授权页面。",
        "edge_func_risk": "Login Topic功能允许绑定外部应用进行身份验证，但缺乏对目标应用可信性的强制校验，暴露了OAuth授权滥用的风险面。",
        "edge_attack_risk": "CoPhish攻击通过伪造授权请求，导致用户误以为正在登录合法服务而授予第三方应用权限，形成OAuth授权滥用。"
      }
    ]
  },
  {
    "attack": {
      "id": "s1ngularity_malware_campaign",
      "description": "NPMパッケージを介して開発者マシンに侵入し、AIコマンドラインツール（LLM CLI）を乗っ取ることで、プログラムによるプロンプト実行を経てファイルシステム全体をスキャンし、認証情報、SSHキー、暗号ウォレットなどの機密データを窃取するマルウェア攻撃。AIアシスタントが悪意ある指示に従って動作し、意図せずデータ収集の共犯者となる。初期侵入段階の中心的手法。"
    },
    "related_functionalities": [
      {
        "id": "file_system_access",
        "description": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "edge_desc": "攻撃者はAIアシスタントがファイルシステムにアクセスできる機能を悪用し、認証情報やSSHキーを含むファイルを探索・収集する。"
      },
      {
        "id": "llm_cli_tool",
        "description": "GeminiやClaudeなどの大規模言語モデル（LLM）と連携するローカルのCLIツール。開発者の日常業務における生産性向上を目的とした機能として設計されているが、マルウェアによる乗っ取りや外部からのプロンプト制御といったセキュリティリスクにより、悪意あるプロンプトを自動実行する踏み台として悪用される可能性がある。",
        "edge_desc": "s1ngularityはGeminiやClaudeのCLIツールを乗っ取り、プログラムによるプロンプト送信を実行するためにこの機能を利用する。"
      },
      {
        "id": "programmatic_prompt_execution",
        "description": "指LLM服务端允许通过API或脚本方式批量提交和执行提示词的能力，支持从自动化工具直接向大语言模型发送提示并处理响应。该功能原本用于开发工作流的自动化，如CI/CD集成、测试生成与迭代优化等场景。然而，其也被ASTRA框架及s1ngularity等恶意工具所利用，实现自动化攻击流程，包括大规模非交互式数据收集、快速试错、反馈驱动的策略迭代以及恶意提示词的批量投递，从而引发安全与滥用风险。",
        "edge_desc": "s1ngularityはスクリプトを通じてAIに自動的に悪意あるプロンプトを発行させることで、非対話的にデータ窃取を実施する。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "ローカルAIツールを介したファイルスキャンにより、SSHキー、APIトークン、暗号ウォレットなどが外部に送信され、大規模なデータ漏洩が発生する。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "s1ngularity_malware_campaign",
        "functionality": "file_system_access",
        "func_desc": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻撃者はAIアシスタントがファイルシステムにアクセスできる機能を悪用し、認証情報やSSHキーを含むファイルを探索・収集する。",
        "edge_func_risk": "文件系统访问功能若缺乏细粒度监控和DLP策略，会暴露数据被AI代理批量读取并外传的风险。",
        "edge_attack_risk": "ローカルAIツールを介したファイルスキャンにより、SSHキー、APIトークン、暗号ウォレットなどが外部に送信され、大規模なデータ漏洩が発生する。"
      },
      {
        "attack": "s1ngularity_malware_campaign",
        "functionality": "file_system_access",
        "func_desc": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻撃者はAIアシスタントがファイルシステムにアクセスできる機能を悪用し、認証情報やSSHキーを含むファイルを探索・収集する。",
        "edge_func_risk": "ファイルシステムへの無制限なアクセス権限は、AIが過度に委任されていることを意味し、これが攻撃の土壌となる。",
        "edge_attack_risk": null
      },
      {
        "attack": "s1ngularity_malware_campaign",
        "functionality": "llm_cli_tool",
        "func_desc": "GeminiやClaudeなどの大規模言語モデル（LLM）と連携するローカルのCLIツール。開発者の日常業務における生産性向上を目的とした機能として設計されているが、マルウェアによる乗っ取りや外部からのプロンプト制御といったセキュリティリスクにより、悪意あるプロンプトを自動実行する踏み台として悪用される可能性がある。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "s1ngularityはGeminiやClaudeのCLIツールを乗っ取り、プログラムによるプロンプト送信を実行するためにこの機能を利用する。",
        "edge_func_risk": null,
        "edge_attack_risk": "ローカルAIツールを介したファイルスキャンにより、SSHキー、APIトークン、暗号ウォレットなどが外部に送信され、大規模なデータ漏洩が発生する。"
      },
      {
        "attack": "s1ngularity_malware_campaign",
        "functionality": "programmatic_prompt_execution",
        "func_desc": "指LLM服务端允许通过API或脚本方式批量提交和执行提示词的能力，支持从自动化工具直接向大语言模型发送提示并处理响应。该功能原本用于开发工作流的自动化，如CI/CD集成、测试生成与迭代优化等场景。然而，其也被ASTRA框架及s1ngularity等恶意工具所利用，实现自动化攻击流程，包括大规模非交互式数据收集、快速试错、反馈驱动的策略迭代以及恶意提示词的批量投递，从而引发安全与滥用风险。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "s1ngularityはスクリプトを通じてAIに自動的に悪意あるプロンプトを発行させることで、非対話的にデータ窃取を実施する。",
        "edge_func_risk": null,
        "edge_attack_risk": "ローカルAIツールを介したファイルスキャンにより、SSHキー、APIトークン、暗号ウォレットなどが外部に送信され、大規模なデータ漏洩が発生する。"
      }
    ]
  },
  {
    "attack": {
      "id": "shai_hulud_self_spreading_worm",
      "description": "侵害されたメンテナーアカウントと公開されたNPMトークンを悪用し、TruffleHogなどの正規のツールを転用して環境内から追加の認証情報を抽出することで、最大20件の関連パッケージに対し自動的に感染・再公開を行う自律的なサプライチェーンワーム。人的介入なしに連鎖的に拡散する。"
    },
    "related_functionalities": [
      {
        "id": "npm_package_management",
        "description": "Node.jsエコシステムにおける依存関係管理とパッケージ公開機能。正規の開発者ツールとして利用されるが、攻撃者は侵害されたメンテナーアカウントを通じて悪意あるバージョンを再公開し、サプライチェーン経由で拡散する。\n\n[补充]: Node.jsエコシステムにおける依存関係管理とパッケージ配布の中心的機能。人気パッケージの侵害が初期侵入経路となり、サプライチェーン全体への影響を招く。",
        "edge_desc": "Shai-HuludはNPMのパッケージ再公開機能を悪用し、感染済みトークンを使って最大20件の関連パッケージに自動で悪意あるコードを注入して再パブリッシュする。"
      },
      {
        "id": "trufflehog_tool_integration",
        "description": "コードリポジトリや設定ファイル内にハードコーディングされた機密情報（APIキー、シークレットキー、NPM公開トークンなど）を検出するためのオープンソースツールTruffleHogは、本来セキュリティ強化を目的とした正規用途を持つが、攻撃者によって逆利用され、ワーム「Shai-Hulud」がこれを悪用して公開リポジトリから有効な認証トークンを自動抽出するなどの攻撃に転用される事例が存在する。",
        "edge_desc": "正規のシークレット検出ツールTruffleHogを逆に利用して、環境内のNPM公開トークンを自動発見し、次の感染ターゲットを特定する。"
      }
    ],
    "direct_risks": [
      {
        "id": "identity_verification_bypass",
        "description": "由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。",
        "edge_desc": "正当なメンテナートークンを用いてパッケージを再公開することで、アイデンティティ検証プロセスを完全にバイパスし、信頼された開発者として振る舞う。"
      },
      {
        "id": "supply_chain_risk",
        "description": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "edge_desc": "Shai-Huludの自律的かつ連鎖的な拡散は、NPMエコシステム全体に対するサプライチェーンリスクを劇的に高める。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "shai_hulud_self_spreading_worm",
        "functionality": "npm_package_management",
        "func_desc": "Node.jsエコシステムにおける依存関係管理とパッケージ公開機能。正規の開発者ツールとして利用されるが、攻撃者は侵害されたメンテナーアカウントを通じて悪意あるバージョンを再公開し、サプライチェーン経由で拡散する。\n\n[补充]: Node.jsエコシステムにおける依存関係管理とパッケージ配布の中心的機能。人気パッケージの侵害が初期侵入経路となり、サプライチェーン全体への影響を招く。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "Shai-HuludはNPMのパッケージ再公開機能を悪用し、感染済みトークンを使って最大20件の関連パッケージに自動で悪意あるコードを注入して再パブリッシュする。",
        "edge_func_risk": "NPMのようなオープンなパッケージ登録機構は、認証トークンの漏洩やメンテナーのアカウント侵害に対して本質的に脆弱であり、サプライチェーンリスクを内在している。",
        "edge_attack_risk": "Shai-Huludの自律的かつ連鎖的な拡散は、NPMエコシステム全体に対するサプライチェーンリスクを劇的に高める。"
      },
      {
        "attack": "shai_hulud_self_spreading_worm",
        "functionality": "trufflehog_tool_integration",
        "func_desc": "コードリポジトリや設定ファイル内にハードコーディングされた機密情報（APIキー、シークレットキー、NPM公開トークンなど）を検出するためのオープンソースツールTruffleHogは、本来セキュリティ強化を目的とした正規用途を持つが、攻撃者によって逆利用され、ワーム「Shai-Hulud」がこれを悪用して公開リポジトリから有効な認証トークンを自動抽出するなどの攻撃に転用される事例が存在する。",
        "risk": "identity_verification_bypass",
        "risk_desc": "由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "正規のシークレット検出ツールTruffleHogを逆に利用して、環境内のNPM公開トークンを自動発見し、次の感染ターゲットを特定する。",
        "edge_func_risk": null,
        "edge_attack_risk": "正当なメンテナートークンを用いてパッケージを再公開することで、アイデンティティ検証プロセスを完全にバイパスし、信頼された開発者として振る舞う。"
      },
      {
        "attack": "shai_hulud_self_spreading_worm",
        "functionality": "trufflehog_tool_integration",
        "func_desc": "コードリポジトリや設定ファイル内にハードコーディングされた機密情報（APIキー、シークレットキー、NPM公開トークンなど）を検出するためのオープンソースツールTruffleHogは、本来セキュリティ強化を目的とした正規用途を持つが、攻撃者によって逆利用され、ワーム「Shai-Hulud」がこれを悪用して公開リポジトリから有効な認証トークンを自動抽出するなどの攻撃に転用される事例が存在する。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "正規のシークレット検出ツールTruffleHogを逆に利用して、環境内のNPM公開トークンを自動発見し、次の感染ターゲットを特定する。",
        "edge_func_risk": null,
        "edge_attack_risk": "Shai-Huludの自律的かつ連鎖的な拡散は、NPMエコシステム全体に対するサプライチェーンリスクを劇的に高める。"
      }
    ]
  },
  {
    "attack": {
      "id": "llm_generated_steganography",
      "description": "攻击者利用大语言模型在合法输出文本中隐蔽嵌入恶意或敏感内容，表面文本语义合理且长度一致，难以被检测系统识别。例如：在合规回答中隐藏越狱指令、在产品评论中传递秘密情报，或在企业AI部署中绕过内容过滤机制传输未审查内容。"
    },
    "related_functionalities": [
      {
        "id": "calgacus_protocol",
        "description": "一种由LLM驱动的文本隐写协议，能够在保持长度不变的前提下，将一条语义完整的秘密信息嵌入到另一条表面看似无害但语义连贯的文本中。该协议利用大语言模型的生成能力实现双向编码与解码，支持在本地设备（如笔记本电脑）上快速执行，80亿参数级别的开源模型即可实现高质量效果。",
        "edge_desc": "大模型生成式文本隐写攻击利用Calgacus协议作为其核心技术载体，实现秘密信息的嵌入与提取。"
      }
    ],
    "direct_risks": [
      {
        "id": "covert_unfiltered_llm_deployment",
        "description": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "edge_desc": "攻击者可通过文本隐写技术在合规响应中编码未过滤模型输出，实现对高风险LLM的隐蔽部署与使用。"
      },
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "大模型生成式文本隐写攻击直接导致了文本与作者意图之间的极端解耦，使表面合规的文本可能隐藏完全相反的真实意图。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "llm_generated_steganography",
        "functionality": "calgacus_protocol",
        "func_desc": "一种由LLM驱动的文本隐写协议，能够在保持长度不变的前提下，将一条语义完整的秘密信息嵌入到另一条表面看似无害但语义连贯的文本中。该协议利用大语言模型的生成能力实现双向编码与解码，支持在本地设备（如笔记本电脑）上快速执行，80亿参数级别的开源模型即可实现高质量效果。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "大模型生成式文本隐写攻击利用Calgacus协议作为其核心技术载体，实现秘密信息的嵌入与提取。",
        "edge_func_risk": "Calgacus协议的设计特性暴露了‘文本与意图极端解耦’这一根本性风险，揭示了当前AI系统在语义真实性验证方面的结构性缺陷。",
        "edge_attack_risk": "大模型生成式文本隐写攻击直接导致了文本与作者意图之间的极端解耦，使表面合规的文本可能隐藏完全相反的真实意图。"
      }
    ]
  },
  {
    "attack": {
      "id": "neural_network_parameters_malware",
      "description": "攻击者将自执行的恶意代码隐藏在预训练深度学习模型的神经网络参数中，利用模型共享机制进行传播。当用户加载或微调该模型时，可能无意中触发恶意行为。这种攻击方式绕过了传统文件扫描机制，因为恶意载荷被编码在权重矩阵中。"
    },
    "related_functionalities": [
      {
        "id": "model_sharing_functionality",
        "description": "AI系统中允许研究人员和企业共享预训练深度学习模型的功能，以便下游用户以较低成本进行微调和部署。该功能广泛用于LLM、CV等场景，是现代AI协作生态的核心组件。\n[New]: 由第三方提供的、广泛应用于多个医疗AI系统的预训练模型组件。\n[New]: 由第三方提供的、广泛部署于医疗系统的AI基础模型，构成关键供应链节点。",
        "edge_desc": "攻击者利用预训练模型共享功能作为传播载体，将嵌有恶意代码的模型上传至公共仓库或作为依赖项分发。"
      }
    ],
    "direct_risks": [
      {
        "id": "covert_unfiltered_llm_deployment",
        "description": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "edge_desc": "成功注入恶意参数的模型被用户加载后，导致未经过安全审查的大模型在目标环境中运行，构成隐蔽部署风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "neural_network_parameters_malware",
        "functionality": "model_sharing_functionality",
        "func_desc": "AI系统中允许研究人员和企业共享预训练深度学习模型的功能，以便下游用户以较低成本进行微调和部署。该功能广泛用于LLM、CV等场景，是现代AI协作生态的核心组件。\n[New]: 由第三方提供的、广泛应用于多个医疗AI系统的预训练模型组件。\n[New]: 由第三方提供的、广泛部署于医疗系统的AI基础模型，构成关键供应链节点。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用预训练模型共享功能作为传播载体，将嵌有恶意代码的模型上传至公共仓库或作为依赖项分发。",
        "edge_func_risk": "模型共享功能若缺乏参数完整性验证机制，则暴露了隐蔽部署未过滤大模型的风险，使攻击者得以绕过常规安全检查。",
        "edge_attack_risk": "成功注入恶意参数的模型被用户加载后，导致未经过安全审查的大模型在目标环境中运行，构成隐蔽部署风险。"
      }
    ]
  },
  {
    "attack": {
      "id": "jailbreak_perceptual_transformation",
      "description": "通过简单的感知层面变换（如视觉关键词分解、语义掩码、音频扰动）对多模态大模型发起的越狱攻击，绕过安全过滤机制。该攻击专门针对跨模态处理中的脆弱性，使得原本在纯文本下有效的安全对齐完全失效。"
    },
    "related_functionalities": [
      {
        "id": "audio_language_model_functionality",
        "description": "支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。",
        "edge_desc": "该类攻击同样作用于音频语言模型，表明多模态输入接口普遍存在防御空白。"
      },
      {
        "id": "vision_language_model_functionality",
        "description": "支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。",
        "edge_desc": "整体性的感知变换越狱策略依赖视觉-语言模型作为主要攻击面，通过非文本通道传递违规指令。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "成功的多模态越狱可能导致模型生成受控内容或暴露内部知识，造成数据泄露后果。"
      },
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "一旦Agent被越狱，可能开始执行超出授权范围的任务，例如自动搜索危险材料制备方法，形成过度代理行为。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "jailbreak_perceptual_transformation",
        "functionality": "audio_language_model_functionality",
        "func_desc": "支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "该类攻击同样作用于音频语言模型，表明多模态输入接口普遍存在防御空白。",
        "edge_func_risk": "音频语言模型对语音输入的高度适应性使其容易受到隐蔽修改的影响，从而暴露跨模态意图伪装的风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "jailbreak_perceptual_transformation",
        "functionality": "vision_language_model_functionality",
        "func_desc": "支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "整体性的感知变换越狱策略依赖视觉-语言模型作为主要攻击面，通过非文本通道传递违规指令。",
        "edge_func_risk": "视觉-语言模型的设计允许从图像中提取文本语义，这一特性本身暴露了文本与意图可被人为解耦的风险。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "figstep_pro_attack",
      "description": "一种具体的视觉越狱技术，将敏感提示词以图形方式拆解为视觉可识别但模型误判为非违规的形式，诱导模型执行有害内容生成。在Llama-4系列模型中攻击成功率高达89%。"
    },
    "related_functionalities": [
      {
        "id": "vision_language_model_functionality",
        "description": "支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。",
        "edge_desc": "FigStep-Pro攻击利用视觉-语言模型对图像中文字结构的理解能力，将敏感词以图形化方式输入，欺骗模型绕过文本安全检测。"
      }
    ],
    "direct_risks": [
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "FigStep-Pro通过将文本意图嵌入图像形式，实现了形式与意图的彻底分离，直接引发极端解耦风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "figstep_pro_attack",
        "functionality": "vision_language_model_functionality",
        "func_desc": "支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "FigStep-Pro攻击利用视觉-语言模型对图像中文字结构的理解能力，将敏感词以图形化方式输入，欺骗模型绕过文本安全检测。",
        "edge_func_risk": "视觉-语言模型的设计允许从图像中提取文本语义，这一特性本身暴露了文本与意图可被人为解耦的风险。",
        "edge_attack_risk": "FigStep-Pro通过将文本意图嵌入图像形式，实现了形式与意图的彻底分离，直接引发极端解耦风险。"
      }
    ]
  },
  {
    "attack": {
      "id": "intelligent_masking_attack",
      "description": "通过在图像或音频中引入语义上模糊但功能上保留指令意图的遮蔽区域，实现对安全过滤器的规避。属于间接提示注入的一种变体，利用了多模态编码器的语义对齐盲区。"
    },
    "related_functionalities": [
      {
        "id": "vision_language_model_functionality",
        "description": "支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。",
        "edge_desc": "智能掩码攻击依赖于视觉-语言模型在处理部分遮挡或模糊图像时仍能提取语义的能力，借此隐藏恶意意图。"
      }
    ],
    "direct_risks": [
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "智能掩码通过视觉干扰保持语义完整性，加剧了模型对‘表面合规’输入的真实意图识别困难。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "intelligent_masking_attack",
        "functionality": "vision_language_model_functionality",
        "func_desc": "支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "智能掩码攻击依赖于视觉-语言模型在处理部分遮挡或模糊图像时仍能提取语义的能力，借此隐藏恶意意图。",
        "edge_func_risk": "视觉-语言模型的设计允许从图像中提取文本语义，这一特性本身暴露了文本与意图可被人为解耦的风险。",
        "edge_attack_risk": "智能掩码通过视觉干扰保持语义完整性，加剧了模型对‘表面合规’输入的真实意图识别困难。"
      }
    ]
  },
  {
    "attack": {
      "id": "wave_echo_perturbation",
      "description": "在语音输入中添加微小的时间延迟回声信号，改变音频语言模型的解析路径，从而触发未授权的行为响应。此类扰动人类难以察觉，但足以破坏安全分类器判断。"
    },
    "related_functionalities": [
      {
        "id": "audio_language_model_functionality",
        "description": "支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。",
        "edge_desc": "Wave-Echo攻击通过向语音输入注入不可听回声，利用音频语言模型的声学特征提取缺陷，误导后续语义解析模块。"
      }
    ],
    "direct_risks": [
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "音频扰动使语音输入在感知上看似正常，但语义已被篡改，构成声音层面的意图伪装。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "wave_echo_perturbation",
        "functionality": "audio_language_model_functionality",
        "func_desc": "支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "Wave-Echo攻击通过向语音输入注入不可听回声，利用音频语言模型的声学特征提取缺陷，误导后续语义解析模块。",
        "edge_func_risk": "音频语言模型对语音输入的高度适应性使其容易受到隐蔽修改的影响，从而暴露跨模态意图伪装的风险。",
        "edge_attack_risk": "音频扰动使语音输入在感知上看似正常，但语义已被篡改，构成声音层面的意图伪装。"
      }
    ]
  },
  {
    "attack": {
      "id": "wave_pitch_speed_attack",
      "description": "通过对音频输入进行音高或播放速度的轻微调整，使语音识别模块输出偏离预期的安全过滤结果，导致有害请求被错误地视为合法。"
    },
    "related_functionalities": [
      {
        "id": "audio_language_model_functionality",
        "description": "支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。",
        "edge_desc": "音调与速度扰动攻击利用音频语言模型对变速变调语音的鲁棒性不足，使其误解析指令内容，进而绕过安全检查。"
      }
    ],
    "direct_risks": [],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "wave_pitch_speed_attack",
        "functionality": "audio_language_model_functionality",
        "func_desc": "支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "音调与速度扰动攻击利用音频语言模型对变速变调语音的鲁棒性不足，使其误解析指令内容，进而绕过安全检查。",
        "edge_func_risk": "音频语言模型对语音输入的高度适应性使其容易受到隐蔽修改的影响，从而暴露跨模态意图伪装的风险。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "indirect_prompt_injection_mcp_server",
      "description": "攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以有效识别与阻断。\n[New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。\n[New]: 攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。"
    },
    "related_functionalities": [
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_integrity_loss",
        "description": "训练数据集包含大量虚假但看似合法的条目，破坏模型学习过程的真实性基础。\n[New]: 训练数据集被系统性污染，导致模型学习到错误的疾病-症状关联，产生长期误导性预测。\n[New]: 传输中的指令或响应被恶意修改，导致模型执行错误决策或泄露敏感信息。",
        "edge_desc": "在未启用TLS验证的情况下，攻击者通过代理伪造服务端响应，诱导模型执行恶意脚本。"
      },
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "攻击者通过注入指令迫使LLM从内部系统提取敏感信息并通过MCP通道回传，直接导致数据泄露。"
      },
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "当MCP服务器未验证代理行为边界时，成功的间接提示注入可导致AI代理执行超出原始用户意图的操作，从而引发过度代理风险。"
      },
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "当Agent连接到受感染的第三方MCP服务器时，攻击者可实施工具投毒，使Agent调用异常工具导致安全事件。"
      },
      {
        "id": "supply_chain_risk",
        "description": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "edge_desc": "攻击者通过伪造网络性能指标触发路由偏差，直接导致系统因信任不可靠遥测而产生错误决策的风险转化为实际威胁。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "indirect_prompt_injection_mcp_server",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_mcp_server",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": "攻击者通过注入指令迫使LLM从内部系统提取敏感信息并通过MCP通道回传，直接导致数据泄露。"
      },
      {
        "attack": "indirect_prompt_injection_mcp_server",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_mcp_server",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": "当MCP服务器未验证代理行为边界时，成功的间接提示注入可导致AI代理执行超出原始用户意图的操作，从而引发过度代理风险。"
      },
      {
        "attack": "indirect_prompt_injection_mcp_server",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。",
        "edge_func_risk": "",
        "edge_attack_risk": "当Agent连接到受感染的第三方MCP服务器时，攻击者可实施工具投毒，使Agent调用异常工具导致安全事件。"
      },
      {
        "attack": "indirect_prompt_injection_mcp_server",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_mcp_server",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_mcp_server",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "间接提示词注入攻击利用MCP服务器作为双向通信通道，将恶意指令隐藏在正常数据响应中传递给LLM。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": "攻击者通过伪造网络性能指标触发路由偏差，直接导致系统因信任不可靠遥测而产生错误决策的风险转化为实际威胁。"
      }
    ]
  },
  {
    "attack": {
      "id": "prompt_injection",
      "description": "一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。\n[New]: 攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。\n[New]: 通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。\n[New]: 攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。\n[New]: 攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。"
    },
    "related_functionalities": [
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "攻击者利用提示词注入技术，欺骗Agent输出内部知识或用户隐私数据。"
      },
      {
        "id": "input_processing",
        "description": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "edge_desc": "在MCPSecBench中，攻击者利用精心设计的Prompt绕过语义过滤机制，成功触发目标Agent调用高危工具。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "Policy Puppetry攻击可结合记忆机制，将恶意指令持久化存储于Agent的记忆模块中，实现跨会话的持续控制。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "提示词注入攻击利用AI代理的网页搜索功能作为传播载体，通过污染搜索结果页面内容实现间接注入。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "提示词注入攻击成功后可能导致代理将内部上下文信息发送至攻击者控制的端点，直接引发数据泄露风险。"
      },
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "提示词注入攻击会劫持代理的行为逻辑，导致其执行未经授权的任务，从而引发过度代理风险。"
      },
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "在MCPSecBench中，攻击者利用精心设计的Prompt绕过语义过滤机制，成功触发目标Agent调用高危工具。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "Policy Puppetry攻击通过语义操纵绕过模型的安全对齐策略，导致其无法识别恶意意图，从而引发安全对齐绕过风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "prompt_injection",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用提示词注入技术，欺骗Agent输出内部知识或用户隐私数据。",
        "edge_func_risk": "",
        "edge_attack_risk": "在MCPSecBench中，攻击者利用精心设计的Prompt绕过语义过滤机制，成功触发目标Agent调用高危工具。"
      },
      {
        "attack": "prompt_injection",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用提示词注入技术，欺骗Agent输出内部知识或用户隐私数据。",
        "edge_func_risk": "",
        "edge_attack_risk": "Policy Puppetry攻击通过语义操纵绕过模型的安全对齐策略，导致其无法识别恶意意图，从而引发安全对齐绕过风险。"
      },
      {
        "attack": "prompt_injection",
        "functionality": "input_processing",
        "func_desc": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "在MCPSecBench中，攻击者利用精心设计的Prompt绕过语义过滤机制，成功触发目标Agent调用高危工具。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "prompt_injection",
        "functionality": "input_processing",
        "func_desc": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "在MCPSecBench中，攻击者利用精心设计的Prompt绕过语义过滤机制，成功触发目标Agent调用高危工具。",
        "edge_func_risk": "",
        "edge_attack_risk": "在MCPSecBench中，攻击者利用精心设计的Prompt绕过语义过滤机制，成功触发目标Agent调用高危工具。"
      },
      {
        "attack": "prompt_injection",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "Policy Puppetry攻击可结合记忆机制，将恶意指令持久化存储于Agent的记忆模块中，实现跨会话的持续控制。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": "提示词注入攻击成功后可能导致代理将内部上下文信息发送至攻击者控制的端点，直接引发数据泄露风险。"
      },
      {
        "attack": "prompt_injection",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "Policy Puppetry攻击可结合记忆机制，将恶意指令持久化存储于Agent的记忆模块中，实现跨会话的持续控制。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": "提示词注入攻击会劫持代理的行为逻辑，导致其执行未经授权的任务，从而引发过度代理风险。"
      },
      {
        "attack": "prompt_injection",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "Policy Puppetry攻击可结合记忆机制，将恶意指令持久化存储于Agent的记忆模块中，实现跨会话的持续控制。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "prompt_injection",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "Policy Puppetry攻击可结合记忆机制，将恶意指令持久化存储于Agent的记忆模块中，实现跨会话的持续控制。",
        "edge_func_risk": "",
        "edge_attack_risk": "在MCPSecBench中，攻击者利用精心设计的Prompt绕过语义过滤机制，成功触发目标Agent调用高危工具。"
      },
      {
        "attack": "prompt_injection",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "Policy Puppetry攻击可结合记忆机制，将恶意指令持久化存储于Agent的记忆模块中，实现跨会话的持续控制。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": "Policy Puppetry攻击通过语义操纵绕过模型的安全对齐策略，导致其无法识别恶意意图，从而引发安全对齐绕过风险。"
      },
      {
        "attack": "prompt_injection",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "提示词注入攻击利用AI代理的网页搜索功能作为传播载体，通过污染搜索结果页面内容实现间接注入。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "prompt_injection",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "提示词注入攻击利用AI代理的网页搜索功能作为传播载体，通过污染搜索结果页面内容实现间接注入。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": "提示词注入攻击成功后可能导致代理将内部上下文信息发送至攻击者控制的端点，直接引发数据泄露风险。"
      },
      {
        "attack": "prompt_injection",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "提示词注入攻击利用AI代理的网页搜索功能作为传播载体，通过污染搜索结果页面内容实现间接注入。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": "Policy Puppetry攻击通过语义操纵绕过模型的安全对齐策略，导致其无法识别恶意意图，从而引发安全对齐绕过风险。"
      }
    ]
  },
  {
    "attack": {
      "id": "cve_2025_7656_maglev_jit_integer_overflow",
      "description": "CVE-2025-7656是Google Chrome V8引擎中Maglev即时编译器的一个整数溢出漏洞，存在于Cursor所依赖的旧版V8中。攻击者可通过诱导用户点击恶意链接，在渲染进程中触发该漏洞，造成拒绝服务（崩溃）或潜在的任意代码执行。"
    },
    "related_functionalities": [
      {
        "id": "v8_javascript_engine",
        "description": "嵌入在Electron中的Google V8引擎用于执行JavaScript代码。当前使用的V8版本存在整数溢出等内存破坏漏洞（如CVE-2025-7656），若不升级将允许恶意脚本触发崩溃甚至任意代码执行。",
        "edge_desc": "攻击者利用V8 JavaScript引擎中存在的Maglev JIT整数溢出漏洞（CVE-2025-7656）发起攻击。"
      }
    ],
    "direct_risks": [
      {
        "id": "denial_of_service_crash",
        "description": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "edge_desc": "PoC攻击已验证可通过触发CVE-2025-7656导致Cursor IDE渲染进程崩溃，造成拒绝服务。"
      },
      {
        "id": "rce_via_v8_exploitation",
        "description": "由于V8引擎存在未修补的内存破坏类漏洞，攻击者可能构造精心设计的JavaScript代码，利用JIT编译过程中的整数溢出实现堆内存操控，最终达成在开发者主机上执行任意代码的风险。",
        "edge_desc": "成功利用CVE-2025-7656可能导致攻击者在目标系统上实现任意代码执行，尤其是在配合其他逃逸技术的情况下。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "cve_2025_7656_maglev_jit_integer_overflow",
        "functionality": "v8_javascript_engine",
        "func_desc": "嵌入在Electron中的Google V8引擎用于执行JavaScript代码。当前使用的V8版本存在整数溢出等内存破坏漏洞（如CVE-2025-7656），若不升级将允许恶意脚本触发崩溃甚至任意代码执行。",
        "risk": "rce_via_v8_exploitation",
        "risk_desc": "由于V8引擎存在未修补的内存破坏类漏洞，攻击者可能构造精心设计的JavaScript代码，利用JIT编译过程中的整数溢出实现堆内存操控，最终达成在开发者主机上执行任意代码的风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用V8 JavaScript引擎中存在的Maglev JIT整数溢出漏洞（CVE-2025-7656）发起攻击。",
        "edge_func_risk": "V8引擎中的内存安全漏洞为远程代码执行提供了潜在入口，尤其当其被集成进桌面应用且无法及时更新时。",
        "edge_attack_risk": "成功利用CVE-2025-7656可能导致攻击者在目标系统上实现任意代码执行，尤其是在配合其他逃逸技术的情况下。"
      }
    ]
  },
  {
    "attack": {
      "id": "plan_injection_attack",
      "description": "一种新型越狱攻击策略，通过向Agent的多步规划阶段注入恶意子目标（malicious sub-goals），操控其执行路径以生成有害内容。此攻击直接干预DR代理的任务分解逻辑，绕过高层语义安全过滤。"
    },
    "related_functionalities": [
      {
        "id": "deep_research_functionality",
        "description": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "edge_desc": "攻击者利用Deep Research代理的多步规划能力，在任务分解阶段注入恶意子目标，从而操控整个研究流程朝向有害方向发展。"
      }
    ],
    "direct_risks": [
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "Plan Injection攻击通过在合法研究计划中嵌入非法子任务，导致系统输出在形式上合规但实质上危险的内容，加剧了文本表层与真实意图之间的解耦现象。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "plan_injection_attack",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "bypass_cloud_upload_controls",
        "risk_desc": "传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Deep Research代理的多步规划能力，在任务分解阶段注入恶意子目标，从而操控整个研究流程朝向有害方向发展。",
        "edge_func_risk": "由于Agent运行环境独立于企业终端网络栈，其对外部服务的调用绕过本地代理和防火墙策略，使得传统的云上传控制手段无法生效。",
        "edge_attack_risk": null
      },
      {
        "attack": "plan_injection_attack",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Deep Research代理的多步规划能力，在任务分解阶段注入恶意子目标，从而操控整个研究流程朝向有害方向发展。",
        "edge_func_risk": "由于Deep Research功能具备强大的信息检索与整合能力，若未加限制，可能无意中聚合碎片化敏感信息，形成完整的禁忌知识体系，导致变相的数据泄露。",
        "edge_attack_risk": null
      },
      {
        "attack": "plan_injection_attack",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Deep Research代理的多步规划能力，在任务分解阶段注入恶意子目标，从而操控整个研究流程朝向有害方向发展。",
        "edge_func_risk": "Deep Research被设计为可自主访问多个高权限数据源，这种高度自治性暴露了‘过度代理’的风险，使Agent可能成为内部威胁载体。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "intent_hijack_attack",
      "description": "一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推荐逻辑的误导或执行非授权操作。"
    },
    "related_functionalities": [
      {
        "id": "deep_research_functionality",
        "description": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "edge_desc": "攻击者利用Deep Research代理对‘学术研究’类请求的优先处理机制，将恶意意图伪装成科研课题，触发正常的研究执行流程。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "Intent Hijack攻击利用记忆模块存储的用户历史浏览数据，通过注入伪造上下文来扭曲AI对用户意图的判断。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "意图劫持攻击通过操控网页上的可操作元素（如按钮、链接），利用Agent的网页搜索工具功能执行错误动作。"
      }
    ],
    "direct_risks": [
      {
        "id": "excessive_delegation",
        "description": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "edge_desc": "当用户意图被劫持后，AI可能被引导执行超出预期范围的任务，而系统因信任上下文连贯性而自动放行，加剧过度代理风险。"
      },
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "Intent Hijack攻击通过语义重构使有害请求获得学术外衣，直接促成文本表述与攻击意图的分离，是引发极端解耦风险的主要动因之一。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "intent_hijack_attack",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "bypass_cloud_upload_controls",
        "risk_desc": "传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Deep Research代理对‘学术研究’类请求的优先处理机制，将恶意意图伪装成科研课题，触发正常的研究执行流程。",
        "edge_func_risk": "由于Agent运行环境独立于企业终端网络栈，其对外部服务的调用绕过本地代理和防火墙策略，使得传统的云上传控制手段无法生效。",
        "edge_attack_risk": null
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用Deep Research代理对‘学术研究’类请求的优先处理机制，将恶意意图伪装成科研课题，触发正常的研究执行流程。",
        "edge_func_risk": "由于Deep Research功能具备强大的信息检索与整合能力，若未加限制，可能无意中聚合碎片化敏感信息，形成完整的禁忌知识体系，导致变相的数据泄露。",
        "edge_attack_risk": null
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用Deep Research代理对‘学术研究’类请求的优先处理机制，将恶意意图伪装成科研课题，触发正常的研究执行流程。",
        "edge_func_risk": "Deep Research被设计为可自主访问多个高权限数据源，这种高度自治性暴露了‘过度代理’的风险，使Agent可能成为内部威胁载体。",
        "edge_attack_risk": "当用户意图被劫持后，AI可能被引导执行超出预期范围的任务，而系统因信任上下文连贯性而自动放行，加剧过度代理风险。"
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "Intent Hijack攻击利用记忆模块存储的用户历史浏览数据，通过注入伪造上下文来扭曲AI对用户意图的判断。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "Intent Hijack攻击利用记忆模块存储的用户历史浏览数据，通过注入伪造上下文来扭曲AI对用户意图的判断。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": "当用户意图被劫持后，AI可能被引导执行超出预期范围的任务，而系统因信任上下文连贯性而自动放行，加剧过度代理风险。"
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "Intent Hijack攻击利用记忆模块存储的用户历史浏览数据，通过注入伪造上下文来扭曲AI对用户意图的判断。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "Intent Hijack攻击利用记忆模块存储的用户历史浏览数据，通过注入伪造上下文来扭曲AI对用户意图的判断。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "Intent Hijack攻击利用记忆模块存储的用户历史浏览数据，通过注入伪造上下文来扭曲AI对用户意图的判断。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": null
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "意图劫持攻击通过操控网页上的可操作元素（如按钮、链接），利用Agent的网页搜索工具功能执行错误动作。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "意图劫持攻击通过操控网页上的可操作元素（如按钮、链接），利用Agent的网页搜索工具功能执行错误动作。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "intent_hijack_attack",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "意图劫持攻击通过操控网页上的可操作元素（如按钮、链接），利用Agent的网页搜索工具功能执行错误动作。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "rag_pull_query_perturbation",
      "description": "通过在用户查询中插入不可见的UTF字符，干扰RAG系统的检索过程，使其优先返回攻击者控制的恶意代码片段。这种扰动对人类完全不可见，但能有效操控检索排序。"
    },
    "related_functionalities": [
      {
        "id": "rag_system",
        "description": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "edge_desc": "RAG-Pull查询扰动攻击利用RAG系统对输入查询的语义检索机制，通过注入不可见字符操控检索结果。"
      }
    ],
    "direct_risks": [
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "查询扰动导致RAG系统引入不安全代码片段，从而破坏LLM的安全对齐，使其输出偏向于存在漏洞的代码实现。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "rag_pull_query_perturbation",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull查询扰动攻击利用RAG系统对输入查询的语义检索机制，通过注入不可见字符操控检索结果。",
        "edge_func_risk": "RAG系统若未对检索来源进行严格验证，可能引入被投毒的数据，导致AI返回包含虚假或敏感信息的结果，暴露数据泄露风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "rag_pull_query_perturbation",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull查询扰动攻击利用RAG系统对输入查询的语义检索机制，通过注入不可见字符操控检索结果。",
        "edge_func_risk": "被污染的RAG上下文可能诱导AI做出错误决策并触发自动化流程，反映出因上下文不可信而导致的代理权限失控问题。",
        "edge_attack_risk": null
      },
      {
        "attack": "rag_pull_query_perturbation",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull查询扰动攻击利用RAG系统对输入查询的语义检索机制，通过注入不可见字符操控检索结果。",
        "edge_func_risk": "RAG系统若未对检索结果进行安全过滤，可能将包含RCE漏洞的恶意代码注入上下文，导致生成危险程序。",
        "edge_attack_risk": null
      },
      {
        "attack": "rag_pull_query_perturbation",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "sql_injection_vulnerability",
        "risk_desc": "由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull查询扰动攻击利用RAG系统对输入查询的语义检索机制，通过注入不可见字符操控检索结果。",
        "edge_func_risk": "RAG系统检索到存在SQL注入缺陷的代码样本后，可能直接复用或模仿其实现方式，传播安全漏洞。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "rag_pull_repository_perturbation",
      "description": "在外部代码仓库中植入带有不可见UTF字符的恶意代码片段，污染检索源。当RAG系统检索相似代码时，这些被标记的片段会因字符匹配而被优先召回。"
    },
    "related_functionalities": [
      {
        "id": "rag_system",
        "description": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "edge_desc": "RAG-Pull代码库扰动攻击利用RAG系统对外部代码仓库的检索依赖，在合法代码中植入隐蔽标记以操纵召回过程。"
      }
    ],
    "direct_risks": [
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "受污染的代码库返回恶意片段，误导LLM学习并偏好不安全编码模式，长期影响模型行为的安全性。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "rag_pull_repository_perturbation",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull代码库扰动攻击利用RAG系统对外部代码仓库的检索依赖，在合法代码中植入隐蔽标记以操纵召回过程。",
        "edge_func_risk": "RAG系统若未对检索来源进行严格验证，可能引入被投毒的数据，导致AI返回包含虚假或敏感信息的结果，暴露数据泄露风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "rag_pull_repository_perturbation",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull代码库扰动攻击利用RAG系统对外部代码仓库的检索依赖，在合法代码中植入隐蔽标记以操纵召回过程。",
        "edge_func_risk": "被污染的RAG上下文可能诱导AI做出错误决策并触发自动化流程，反映出因上下文不可信而导致的代理权限失控问题。",
        "edge_attack_risk": null
      },
      {
        "attack": "rag_pull_repository_perturbation",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull代码库扰动攻击利用RAG系统对外部代码仓库的检索依赖，在合法代码中植入隐蔽标记以操纵召回过程。",
        "edge_func_risk": "RAG系统若未对检索结果进行安全过滤，可能将包含RCE漏洞的恶意代码注入上下文，导致生成危险程序。",
        "edge_attack_risk": null
      },
      {
        "attack": "rag_pull_repository_perturbation",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "sql_injection_vulnerability",
        "risk_desc": "由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull代码库扰动攻击利用RAG系统对外部代码仓库的检索依赖，在合法代码中植入隐蔽标记以操纵召回过程。",
        "edge_func_risk": "RAG系统检索到存在SQL注入缺陷的代码样本后，可能直接复用或模仿其实现方式，传播安全漏洞。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "rag_pull_combined_attack",
      "description": "同时在查询和目标代码库中插入匹配的不可见UTF字符，实现双重定向。实验表明该组合方式可达到接近100%的攻击成功率，是最有效的RAG-Pull变体。"
    },
    "related_functionalities": [
      {
        "id": "rag_system",
        "description": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "edge_desc": "RAG-Pull组合扰动攻击综合利用查询和代码库两端的隐蔽字符匹配，最大化对RAG系统检索排序的控制能力。"
      }
    ],
    "direct_risks": [
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "组合式RAG-Pull攻击成功注入恶意代码片段后，LLM可能直接生成具备远程命令执行能力的程序。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "组合扰动极大提升了攻击成功率，显著降低安全对齐的有效性，使模型几乎无差别接受攻击者指定的恶意代码逻辑。"
      },
      {
        "id": "sql_injection_vulnerability",
        "description": "由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。",
        "edge_desc": "攻击者可通过预置含SQL注入模式的代码片段，诱导RAG系统在代码生成中复制该漏洞结构。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "rag_pull_combined_attack",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull组合扰动攻击综合利用查询和代码库两端的隐蔽字符匹配，最大化对RAG系统检索排序的控制能力。",
        "edge_func_risk": "RAG系统若未对检索来源进行严格验证，可能引入被投毒的数据，导致AI返回包含虚假或敏感信息的结果，暴露数据泄露风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "rag_pull_combined_attack",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "RAG-Pull组合扰动攻击综合利用查询和代码库两端的隐蔽字符匹配，最大化对RAG系统检索排序的控制能力。",
        "edge_func_risk": "被污染的RAG上下文可能诱导AI做出错误决策并触发自动化流程，反映出因上下文不可信而导致的代理权限失控问题。",
        "edge_attack_risk": null
      },
      {
        "attack": "rag_pull_combined_attack",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "RAG-Pull组合扰动攻击综合利用查询和代码库两端的隐蔽字符匹配，最大化对RAG系统检索排序的控制能力。",
        "edge_func_risk": "RAG系统若未对检索结果进行安全过滤，可能将包含RCE漏洞的恶意代码注入上下文，导致生成危险程序。",
        "edge_attack_risk": "组合式RAG-Pull攻击成功注入恶意代码片段后，LLM可能直接生成具备远程命令执行能力的程序。"
      },
      {
        "attack": "rag_pull_combined_attack",
        "functionality": "rag_system",
        "func_desc": "检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。\n\n[补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。",
        "risk": "sql_injection_vulnerability",
        "risk_desc": "由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "RAG-Pull组合扰动攻击综合利用查询和代码库两端的隐蔽字符匹配，最大化对RAG系统检索排序的控制能力。",
        "edge_func_risk": "RAG系统检索到存在SQL注入缺陷的代码样本后，可能直接复用或模仿其实现方式，传播安全漏洞。",
        "edge_attack_risk": "攻击者可通过预置含SQL注入模式的代码片段，诱导RAG系统在代码生成中复制该漏洞结构。"
      }
    ]
  },
  {
    "attack": {
      "id": "imperceptible_jailbreak_unicode_variation_selectors",
      "description": "通过在恶意问题后附加Unicode中的变体选择符（Variation Selectors）作为不可见的对抗性后缀，秘密改变大语言模型的分词结果，从而绕过安全对齐机制。这些字符在视觉上不可见，但能被LLM的tokenizer编码，导致模型输出有害内容。"
    },
    "related_functionalities": [
      {
        "id": "llm_tokenizer_functionality",
        "description": "大语言模型用于将输入文本转换为token序列的核心组件，其对Unicode字符（包括变体选择符）的处理方式可能被攻击者利用来实现不可见的语义操控。\n\n[补充]: 负责将自然语言输入转换为模型可处理的token序列的核心组件。在多轮攻击中，分词器的行为可能影响攻击载荷的解析方式，进而影响越狱路径的有效性。",
        "edge_desc": "该攻击通过向输入中添加Unicode变体选择符，利用大模型分词器对不可见字符仍进行编码的特性，实现token层面的操控。"
      }
    ],
    "direct_risks": [
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "攻击使得表面上正常的文本在内部被解析为完全不同的恶意意图，造成文本外观与实际执行意图之间的极端解耦。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "由于分词结果被篡改，模型误判输入为合法请求，导致安全对齐机制失效，最终输出有害内容。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "imperceptible_jailbreak_unicode_variation_selectors",
        "functionality": "llm_tokenizer_functionality",
        "func_desc": "大语言模型用于将输入文本转换为token序列的核心组件，其对Unicode字符（包括变体选择符）的处理方式可能被攻击者利用来实现不可见的语义操控。\n\n[补充]: 负责将自然语言输入转换为模型可处理的token序列的核心组件。在多轮攻击中，分词器的行为可能影响攻击载荷的解析方式，进而影响越狱路径的有效性。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "该攻击通过向输入中添加Unicode变体选择符，利用大模型分词器对不可见字符仍进行编码的特性，实现token层面的操控。",
        "edge_func_risk": "分词器对Unicode控制字符的处理逻辑暴露了潜在风险，使得攻击者可在不改变视觉呈现的前提下操控语义解析流程。",
        "edge_attack_risk": "攻击使得表面上正常的文本在内部被解析为完全不同的恶意意图，造成文本外观与实际执行意图之间的极端解耦。"
      }
    ]
  },
  {
    "attack": {
      "id": "data_exfiltration_via_agent",
      "description": "攻击者通过控制大语言模型（LLM）或AI代理（如ChatGPT Agent、AI侧边栏响应系统），利用其多步推理、工具调用、会话记忆及上下文学习能力，长期潜伏于目标环境，逐步诱导用户或模型本身泄露私有信息、系统级数据、训练数据残留、提示工程细节以及企业敏感内容，构成高隐蔽性、持续性的复合型数据泄露威胁。该攻击模式通过精心构造的交互序列，诱使用户在看似可信的AI界面中输入个人身份信息、企业机密、源代码、配置文件、认证凭据、加密货币钱包密钥、文档内容等高价值数据，并借助AI代理的自动化执行能力对敏感信息进行分段编码。编码后的数据通过合法输出通道或外部服务（如Filebin.net等文件托管平台）隐蔽外传至攻击者控制的服务端，再利用返回的下载链接完成数据回传，实现高效且低可探测性的数据渗出。整个过程常结合隐蔽通信协议与低频传输策略，规避基于网络流量特征分析、异常上传行为检测及DLP机制的传统安全防护体系，可由恶意篡改的开发工具扩展（如VS Code扩展）触发并协同执行，形成对敏感信息资产的系统性渗透与长期窃取能力。此外，尽管当前版本可能未启用，但相关恶意组件具备通过AI交互通道回传执行结果或日志信息的能力，存在利用LLM通信链路作为隐蔽信道进行数据渗出的潜在风险。\n[New]: 利用提示词注入控制Agent的响应生成机制，将窃取的数据编码后通过正常对话回复发送给攻击者。"
    },
    "related_functionalities": [
      {
        "id": "deep_research_functionality",
        "description": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "edge_desc": "攻击者利用ChatGPT Agent的多阶段任务执行能力，指挥其完成从数据收集、压缩到远程上传的全流程操作。"
      },
      {
        "id": "file_upload_functionality",
        "description": "Agent通过集成云存储API实现文件上传功能，支持将本地或生成的数据自动上传至指定的文件托管服务（如Filebin.net、S3、Dropbox等），用于共享结果或持久化输出内容。此功能通常作为任务执行流程的一部分被触发。",
        "edge_desc": "攻击载荷通过Agent调用内置的文件上传接口，将窃取的数据发送至攻击者控制的外部文件托管服务。"
      },
      {
        "id": "response_generator",
        "description": "负责根据上下文和推理结果生成自然语言回复的模块。",
        "edge_desc": "在间接提示词注入成功后，攻击者构造的指令可迫使Custom GPT将内部信息或用户提供的敏感数据作为回答的一部分回传，实现隐蔽的数据渗出。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "攻击者使用网页搜索功能验证外部C2或文件接收站点（如filebin.net）是否可达，确保渗出路径通畅。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "成功执行的数据渗出攻击直接导致敏感信息被上传至公网，造成实质性的数据泄露后果。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "data_exfiltration_via_agent",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "bypass_cloud_upload_controls",
        "risk_desc": "传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用ChatGPT Agent的多阶段任务执行能力，指挥其完成从数据收集、压缩到远程上传的全流程操作。",
        "edge_func_risk": "由于Agent运行环境独立于企业终端网络栈，其对外部服务的调用绕过本地代理和防火墙策略，使得传统的云上传控制手段无法生效。",
        "edge_attack_risk": null
      },
      {
        "attack": "data_exfiltration_via_agent",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用ChatGPT Agent的多阶段任务执行能力，指挥其完成从数据收集、压缩到远程上传的全流程操作。",
        "edge_func_risk": "由于Deep Research功能具备强大的信息检索与整合能力，若未加限制，可能无意中聚合碎片化敏感信息，形成完整的禁忌知识体系，导致变相的数据泄露。",
        "edge_attack_risk": "成功执行的数据渗出攻击直接导致敏感信息被上传至公网，造成实质性的数据泄露后果。"
      },
      {
        "attack": "data_exfiltration_via_agent",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用ChatGPT Agent的多阶段任务执行能力，指挥其完成从数据收集、压缩到远程上传的全流程操作。",
        "edge_func_risk": "Deep Research被设计为可自主访问多个高权限数据源，这种高度自治性暴露了‘过度代理’的风险，使Agent可能成为内部威胁载体。",
        "edge_attack_risk": null
      },
      {
        "attack": "data_exfiltration_via_agent",
        "functionality": "file_upload_functionality",
        "func_desc": "Agent通过集成云存储API实现文件上传功能，支持将本地或生成的数据自动上传至指定的文件托管服务（如Filebin.net、S3、Dropbox等），用于共享结果或持久化输出内容。此功能通常作为任务执行流程的一部分被触发。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "攻击载荷通过Agent调用内置的文件上传接口，将窃取的数据发送至攻击者控制的外部文件托管服务。",
        "edge_func_risk": null,
        "edge_attack_risk": "成功执行的数据渗出攻击直接导致敏感信息被上传至公网，造成实质性的数据泄露后果。"
      },
      {
        "attack": "data_exfiltration_via_agent",
        "functionality": "response_generator",
        "func_desc": "负责根据上下文和推理结果生成自然语言回复的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "在间接提示词注入成功后，攻击者构造的指令可迫使Custom GPT将内部信息或用户提供的敏感数据作为回答的一部分回传，实现隐蔽的数据渗出。",
        "edge_func_risk": null,
        "edge_attack_risk": "成功执行的数据渗出攻击直接导致敏感信息被上传至公网，造成实质性的数据泄露后果。"
      },
      {
        "attack": "data_exfiltration_via_agent",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者使用网页搜索功能验证外部C2或文件接收站点（如filebin.net）是否可达，确保渗出路径通畅。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "data_exfiltration_via_agent",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者使用网页搜索功能验证外部C2或文件接收站点（如filebin.net）是否可达，确保渗出路径通畅。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": "成功执行的数据渗出攻击直接导致敏感信息被上传至公网，造成实质性的数据泄露后果。"
      },
      {
        "attack": "data_exfiltration_via_agent",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者使用网页搜索功能验证外部C2或文件接收站点（如filebin.net）是否可达，确保渗出路径通畅。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "indirect_prompt_injection_browser_extension",
      "description": "攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript 代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI 功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM 输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI 响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。"
    },
    "related_functionalities": [
      {
        "id": "ai_browser_functionality",
        "description": "集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。",
        "edge_desc": "恶意扩展注入攻击利用 AI 浏览器的侧边栏功能作为仿冒目标，通过覆盖真实 UI 实现欺骗。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "基于浏览器扩展的间接提示词注入攻击利用Agent的记忆功能作为数据回传或命令持久化的媒介。"
      }
    ],
    "direct_risks": [
      {
        "id": "oauth_consent_request_misuse",
        "description": "攻击者通过构造恶意OAuth授权请求，利用社交工程手段伪造AI响应或诱导AI代理自动化决策弱点，绕过用户确认机制，欺骗用户或自动化系统对未验证的第三方应用授予过度权限（如访问Gmail、Google Drive等高敏感服务），导致OAuth令牌泄露。此类攻击结合了权限滥用与社会工程学，利用OAuth流程中应用身份验证不足及权限审批机制无法有效区分合法与恶意请求的缺陷，尤其威胁拥有高权限账户的用户，可导致未经授权的数据访问、操作乃至账户接管。",
        "edge_desc": "欺骗性 UI 可诱导用户误授权恶意应用，导致 OAuth 授权请求被滥用，造成账户泄露。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "indirect_prompt_injection_browser_extension",
        "functionality": "ai_browser_functionality",
        "func_desc": "集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意扩展注入攻击利用 AI 浏览器的侧边栏功能作为仿冒目标，通过覆盖真实 UI 实现欺骗。",
        "edge_func_risk": "AI浏览器功能要求持续上传用户浏览上下文以维持对话状态，该设计特性直接暴露了大规模数据泄露的风险面。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_browser_extension",
        "functionality": "ai_browser_functionality",
        "func_desc": "集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意扩展注入攻击利用 AI 浏览器的侧边栏功能作为仿冒目标，通过覆盖真实 UI 实现欺骗。",
        "edge_func_risk": "AI浏览器功能允许全自动化交互，若缺乏人工复核机制，则暴露过度代理风险，尤其在遭遇暗黑模式时难以中断流程。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_browser_extension",
        "functionality": "ai_browser_functionality",
        "func_desc": "集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "恶意扩展注入攻击利用 AI 浏览器的侧边栏功能作为仿冒目标，通过覆盖真实 UI 实现欺骗。",
        "edge_func_risk": "AI浏览器虽具备多模态输入处理能力，但其对视觉优先级的过度依赖暴露了文本与意图解耦的风险面。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_browser_extension",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于浏览器扩展的间接提示词注入攻击利用Agent的记忆功能作为数据回传或命令持久化的媒介。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_browser_extension",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于浏览器扩展的间接提示词注入攻击利用Agent的记忆功能作为数据回传或命令持久化的媒介。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_browser_extension",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于浏览器扩展的间接提示词注入攻击利用Agent的记忆功能作为数据回传或命令持久化的媒介。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_browser_extension",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于浏览器扩展的间接提示词注入攻击利用Agent的记忆功能作为数据回传或命令持久化的媒介。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "indirect_prompt_injection_browser_extension",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "基于浏览器扩展的间接提示词注入攻击利用Agent的记忆功能作为数据回传或命令持久化的媒介。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "decompression_bomb_attack",
      "description": "攻击者通过精心构造极端高压缩比的压缩文件（如1KB原始数据压缩至几字节，解压后膨胀至GB级），结合恶意设计的LZ77编码序列，使解压过程产生极高的计算与内存开销，从而实施拒绝服务攻击。在内存层面，利用目标系统对压缩数据无限制解压的特性，迅速耗尽可用内存资源；该行为可通过将Content-Encoding设置为deflate而非gzip，绕过依赖gzip尾部ISIZE字段进行预检的安全机制，因deflate格式不包含长度校验信息，导致传统基于ISIZE的检测手段完全失效，为内存炸弹攻击提供入口。在CPU层面，通过大量微小的动态霍夫曼块迫使频繁的Huffman树重建，构造重叠的距离-长度匹配序列引发昂贵的回溯拷贝操作，并穿插短文字字面量与复制指令以增加分支预测开销和处理负担，实现低输入体积、高CPU消耗的“CPU炸弹”效果。此类复合式资源耗竭攻击同时针对解压算法的时间与空间复杂度弱点，显著放大实际系统负载。\n[New]: 服务器向恶意爬虫返回高度压缩的响应（如极高压缩比的gzip文件），意图在解压时消耗其大量计算资源。\n[New]: 向爬虫返回高压缩比的Gzip文件（如100:1以上），试图通过解压消耗其计算资源，但实际效果有限且可能反伤自身。"
    },
    "related_functionalities": [
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "试图通过返回高压缩比的gzip内容来反制爬虫，但实际测试发现许多爬虫根本不解析压缩内容，甚至重复抓取，导致防御方白白消耗大量带宽资源。"
      },
      {
        "id": "gzip_decompression_functionality",
        "description": "Agent或后端服务中用于处理GZIP压缩数据流的解压功能，通常基于zlib或gzip模块实现，支持流式解压以处理HTTP响应等场景。该功能在解析GZIP格式时依赖尾部ISIZE字段预估原始大小，但此方法可被篡改误导。",
        "edge_desc": "攻击者利用目标系统中存在的GZIP解压缩功能作为攻击载体，通过向其输入恶意构造的高压缩比文件发起攻击。"
      }
    ],
    "direct_risks": [
      {
        "id": "denial_of_service_crash",
        "description": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "edge_desc": "解压缩炸弹攻击直接导致系统在解压过程中分配过多内存，最终引发内存耗尽型拒绝服务。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "decompression_bomb_attack",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "试图通过返回高压缩比的gzip内容来反制爬虫，但实际测试发现许多爬虫根本不解析压缩内容，甚至重复抓取，导致防御方白白消耗大量带宽资源。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "decompression_bomb_attack",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "试图通过返回高压缩比的gzip内容来反制爬虫，但实际测试发现许多爬虫根本不解析压缩内容，甚至重复抓取，导致防御方白白消耗大量带宽资源。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "decompression_bomb_attack",
        "functionality": "gzip_decompression_functionality",
        "func_desc": "Agent或后端服务中用于处理GZIP压缩数据流的解压功能，通常基于zlib或gzip模块实现，支持流式解压以处理HTTP响应等场景。该功能在解析GZIP格式时依赖尾部ISIZE字段预估原始大小，但此方法可被篡改误导。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用目标系统中存在的GZIP解压缩功能作为攻击载体，通过向其输入恶意构造的高压缩比文件发起攻击。",
        "edge_func_risk": "GZIP解压缩功能若未实施输出大小限制，则其本身的设计特性暴露了潜在的内存耗尽风险，成为攻击面的一部分。",
        "edge_attack_risk": "解压缩炸弹攻击直接导致系统在解压过程中分配过多内存，最终引发内存耗尽型拒绝服务。"
      }
    ]
  },
  {
    "attack": {
      "id": "malicious_download_via_search_result",
      "description": "攻击者通过SEO欺骗或内容投毒污染AI浏览器的搜索结果，使其推荐伪装成合法文件的恶意软件下载源，AI代理自动执行下载动作而缺乏客户端安全扫描。\n\n[补充]: 当用户询问软件安装方法时，伪造的 AI 侧边栏返回包含反向 shell 安装命令的响应，诱导用户在终端执行，导致设备被远程控制。这是一种利用 AI 响应引导用户主动下载并执行恶意载荷的攻击方式。"
    },
    "related_functionalities": [
      {
        "id": "file_system_access",
        "description": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "edge_desc": "攻击者利用AI浏览器的文件下载和本地存储访问功能，在无用户干预下部署恶意可执行文件。"
      },
      {
        "id": "web_search_tool",
        "description": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "edge_desc": "攻击者操纵搜索引擎结果排名，使AI浏览器调用其搜索功能时返回包含恶意下载链接的结果页。"
      }
    ],
    "direct_risks": [
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "通过伪造 AI 响应诱导用户执行反向 shell 命令，直接导致远程代码执行风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "malicious_download_via_search_result",
        "functionality": "file_system_access",
        "func_desc": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用AI浏览器的文件下载和本地存储访问功能，在无用户干预下部署恶意可执行文件。",
        "edge_func_risk": "文件系统访问功能若缺乏细粒度监控和DLP策略，会暴露数据被AI代理批量读取并外传的风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "malicious_download_via_search_result",
        "functionality": "file_system_access",
        "func_desc": "AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTFLUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱（サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規模な収集に利用されるリスクがある。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用AI浏览器的文件下载和本地存储访问功能，在无用户干预下部署恶意可执行文件。",
        "edge_func_risk": "ファイルシステムへの無制限なアクセス権限は、AIが過度に委任されていることを意味し、これが攻撃の土壌となる。",
        "edge_attack_risk": null
      },
      {
        "attack": "malicious_download_via_search_result",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者操纵搜索引擎结果排名，使AI浏览器调用其搜索功能时返回包含恶意下载链接的结果页。",
        "edge_func_risk": "网页搜索功能引入大量外部不可信内容，显著扩大系统整体攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "malicious_download_via_search_result",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者操纵搜索引擎结果排名，使AI浏览器调用其搜索功能时返回包含恶意下载链接的结果页。",
        "edge_func_risk": "网页搜索工具若缺乏对返回内容的安全扫描机制，可能将含有提示注入载荷的页面引入处理流程，暴露数据泄露风险。",
        "edge_attack_risk": null
      },
      {
        "attack": "malicious_download_via_search_result",
        "functionality": "web_search_tool",
        "func_desc": "AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏感信息或执行非授权操作。\n[New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者操纵搜索引擎结果排名，使AI浏览器调用其搜索功能时返回包含恶意下载链接的结果页。",
        "edge_func_risk": "网页搜索工具引入不可信的外部信息源，可能携带恶意提示词，从而暴露AI系统于安全对齐被绕过的风险中。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "plague_framework_multi_turn_jailbreak",
      "description": "PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和 Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Reframe等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。\n[New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。"
    },
    "related_functionalities": [
      {
        "id": "deep_research_functionality",
        "description": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "edge_desc": "PLAGUE框架针对具备深度研究能力的Agent发起攻击，利用其多步推理和工具调用特性进行复杂任务伪装。"
      },
      {
        "id": "llm_generated_code",
        "description": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "edge_desc": "PLAGUE生成的攻击可能包含诱导模型产生恶意代码的子任务，从而利用大模型生成代码功能扩大攻击面。"
      },
      {
        "id": "llm_tokenizer_functionality",
        "description": "大语言模型用于将输入文本转换为token序列的核心组件，其对Unicode字符（包括变体选择符）的处理方式可能被攻击者利用来实现不可见的语义操控。\n\n[补充]: 负责将自然语言输入转换为模型可处理的token序列的核心组件。在多轮攻击中，分词器的行为可能影响攻击载荷的解析方式，进而影响越狱路径的有效性。",
        "edge_desc": "PLAGUE攻击依赖于分词器对恶意构造文本的解析特性，以确保攻击载荷能被正确分割并触发目标响应模式。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "PLAGUE框架利用Agent的记忆模块来维持和操控多轮对话上下文，实现跨回合的攻击意图隐匿传递。"
      },
      {
        "id": "prompt_template_execution",
        "description": "大语言模型系统中用于加载和执行预定义提示模板的功能模块，提供预构建的指令模板以初始化对话上下文或设定角色行为模式，并指导模型如何使用特定工具和资源。该功能常用于引导模型完成特定任务，ASTRA框架利用其动态注入和执行生成的攻击提示模板。若模板未受保护，可能被篡改或通过后续对话交互间接操纵，从而扭曲其语义意图，实现对模型行为的渐进式控制权夺取。",
        "edge_desc": "PLAGUE框架利用LLM的预设提示模板执行功能作为攻击入口点，通过多轮对话逐步覆盖原始指令意图。"
      }
    ],
    "direct_risks": [
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "PLAGUE框架成功实施后，能够使模型完全绕过其安全对齐机制，输出原本被禁止的有害内容。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "bypass_cloud_upload_controls",
        "risk_desc": "传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE框架针对具备深度研究能力的Agent发起攻击，利用其多步推理和工具调用特性进行复杂任务伪装。",
        "edge_func_risk": "由于Agent运行环境独立于企业终端网络栈，其对外部服务的调用绕过本地代理和防火墙策略，使得传统的云上传控制手段无法生效。",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE框架针对具备深度研究能力的Agent发起攻击，利用其多步推理和工具调用特性进行复杂任务伪装。",
        "edge_func_risk": "由于Deep Research功能具备强大的信息检索与整合能力，若未加限制，可能无意中聚合碎片化敏感信息，形成完整的禁忌知识体系，导致变相的数据泄露。",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "deep_research_functionality",
        "func_desc": "ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源访问权限，该功能也成为多轮越狱攻击的理想目标。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE框架针对具备深度研究能力的Agent发起攻击，利用其多步推理和工具调用特性进行复杂任务伪装。",
        "edge_func_risk": "Deep Research被设计为可自主访问多个高权限数据源，这种高度自治性暴露了‘过度代理’的风险，使Agent可能成为内部威胁载体。",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE生成的攻击可能包含诱导模型产生恶意代码的子任务，从而利用大模型生成代码功能扩大攻击面。",
        "edge_func_risk": "大模型生成代码功能暴露了隐蔽部署未过滤大模型的风险，尤其是在缺乏API调用审计和内容监控的情况下。",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE生成的攻击可能包含诱导模型产生恶意代码的子任务，从而利用大模型生成代码功能扩大攻击面。",
        "edge_func_risk": "自动代码生成功能在缺乏人工审核机制时，会暴露过度代理风险，使得AI能够在关键路径上自主决策。",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "llm_tokenizer_functionality",
        "func_desc": "大语言模型用于将输入文本转换为token序列的核心组件，其对Unicode字符（包括变体选择符）的处理方式可能被攻击者利用来实现不可见的语义操控。\n\n[补充]: 负责将自然语言输入转换为模型可处理的token序列的核心组件。在多轮攻击中，分词器的行为可能影响攻击载荷的解析方式，进而影响越狱路径的有效性。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE攻击依赖于分词器对恶意构造文本的解析特性，以确保攻击载荷能被正确分割并触发目标响应模式。",
        "edge_func_risk": "分词器对Unicode控制字符的处理逻辑暴露了潜在风险，使得攻击者可在不改变视觉呈现的前提下操控语义解析流程。",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE框架利用Agent的记忆模块来维持和操控多轮对话上下文，实现跨回合的攻击意图隐匿传递。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE框架利用Agent的记忆模块来维持和操控多轮对话上下文，实现跨回合的攻击意图隐匿传递。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE框架利用Agent的记忆模块来维持和操控多轮对话上下文，实现跨回合的攻击意图隐匿传递。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "PLAGUE框架利用Agent的记忆模块来维持和操控多轮对话上下文，实现跨回合的攻击意图隐匿传递。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "PLAGUE框架利用Agent的记忆模块来维持和操控多轮对话上下文，实现跨回合的攻击意图隐匿传递。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": "PLAGUE框架成功实施后，能够使模型完全绕过其安全对齐机制，输出原本被禁止的有害内容。"
      },
      {
        "attack": "plague_framework_multi_turn_jailbreak",
        "functionality": "prompt_template_execution",
        "func_desc": "大语言模型系统中用于加载和执行预定义提示模板的功能模块，提供预构建的指令模板以初始化对话上下文或设定角色行为模式，并指导模型如何使用特定工具和资源。该功能常用于引导模型完成特定任务，ASTRA框架利用其动态注入和执行生成的攻击提示模板。若模板未受保护，可能被篡改或通过后续对话交互间接操纵，从而扭曲其语义意图，实现对模型行为的渐进式控制权夺取。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "PLAGUE框架利用LLM的预设提示模板执行功能作为攻击入口点，通过多轮对话逐步覆盖原始指令意图。",
        "edge_func_risk": "预设提示模板执行功能若缺乏动态监控与完整性校验，会暴露安全对齐绕过的风险，成为渐进式控制的突破口。",
        "edge_attack_risk": "PLAGUE框架成功实施后，能够使模型完全绕过其安全对齐机制，输出原本被禁止的有害内容。"
      }
    ]
  },
  {
    "attack": {
      "id": "breakfun_schema_jailbreak",
      "description": "一种通过精心构造的'Trojan Schema'（特洛伊架构）利用大模型对结构化数据格式严格遵循特性的越狱攻击方法。该攻击采用三段式提示：无害框架、思维链干扰和核心恶意结构，迫使LLM生成有害内容。在13个主流模型上平均攻击成功率达89%，多个模型达到100%成功率。\n[New]: 利用合法工具的参数schema，诱导模型补全额外字段，从而注入恶意参数值。"
    },
    "related_functionalities": [
      {
        "id": "tool_integration_capability",
        "description": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "edge_desc": "BreakFun攻击利用大模型对结构化数据格式（如JSON Schema）的严格解析行为，将恶意指令嵌入合法结构中，欺骗模型执行。"
      }
    ],
    "direct_risks": [
      {
        "id": "malicious_parameter_injection",
        "description": "在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。",
        "edge_desc": "攻击者提供部分完成的JSON结构，引导模型自动补全本不应存在的敏感字段（如'exec_cmd'），从而在看似正常的工具调用中植入恶意指令。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "BreakFun攻击成功绕过LLM的安全对齐机制，导致模型生成违反其使用政策的有害内容。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "breakfun_schema_jailbreak",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "BreakFun攻击利用大模型对结构化数据格式（如JSON Schema）的严格解析行为，将恶意指令嵌入合法结构中，欺骗模型执行。",
        "edge_func_risk": "Tool机制赋予LLM直接调用外部系统的权限，若缺乏权限收敛和审计机制，极易引发过度代理问题。",
        "edge_attack_risk": null
      },
      {
        "attack": "breakfun_schema_jailbreak",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "malicious_parameter_injection",
        "risk_desc": "在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "BreakFun攻击利用大模型对结构化数据格式（如JSON Schema）的严格解析行为，将恶意指令嵌入合法结构中，欺骗模型执行。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者提供部分完成的JSON结构，引导模型自动补全本不应存在的敏感字段（如'exec_cmd'），从而在看似正常的工具调用中植入恶意指令。"
      },
      {
        "attack": "breakfun_schema_jailbreak",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "model_poisoning",
        "risk_desc": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "BreakFun攻击利用大模型对结构化数据格式（如JSON Schema）的严格解析行为，将恶意指令嵌入合法结构中，欺骗模型执行。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "breakfun_schema_jailbreak",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "BreakFun攻击利用大模型对结构化数据格式（如JSON Schema）的严格解析行为，将恶意指令嵌入合法结构中，欺骗模型执行。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "breakfun_schema_jailbreak",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "BreakFun攻击利用大模型对结构化数据格式（如JSON Schema）的严格解析行为，将恶意指令嵌入合法结构中，欺骗模型执行。",
        "edge_func_risk": "大模型对结构化数据的强解析能力暴露了安全对齐绕过的风险，攻击者可利用格式合规性掩盖恶意语义。",
        "edge_attack_risk": "BreakFun攻击成功绕过LLM的安全对齐机制，导致模型生成违反其使用政策的有害内容。"
      },
      {
        "attack": "breakfun_schema_jailbreak",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "unauthorized_intrusion",
        "risk_desc": "导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。\n[New]: 导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "BreakFun攻击利用大模型对结构化数据格式（如JSON Schema）的严格解析行为，将恶意指令嵌入合法结构中，欺骗模型执行。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "astra_autonomous_jailbreak_framework",
      "description": "ASTRA是一个自动化的大模型越狱攻击框架，通过‘攻击-评估-提炼-复用’的闭环机制，自主发现、检索和演化攻击策略。它能够从失败或部分成功的攻击尝试中提取有价值的信息，并实现自我进化，生成多样化且自适应的攻击提示。该框架包含一个三层策略库（有效、有潜力、无效），用于系统性积累攻击知识并指导后续攻击生成，在黑盒环境下实现了82.7%的平均攻击成功率。"
    },
    "related_functionalities": [
      {
        "id": "programmatic_prompt_execution",
        "description": "指LLM服务端允许通过API或脚本方式批量提交和执行提示词的能力，支持从自动化工具直接向大语言模型发送提示并处理响应。该功能原本用于开发工作流的自动化，如CI/CD集成、测试生成与迭代优化等场景。然而，其也被ASTRA框架及s1ngularity等恶意工具所利用，实现自动化攻击流程，包括大规模非交互式数据收集、快速试错、反馈驱动的策略迭代以及恶意提示词的批量投递，从而引发安全与滥用风险。",
        "edge_desc": "ASTRA依赖程序化提示执行能力实现自动化攻击循环，包括大规模提示生成、提交与结果分析。"
      },
      {
        "id": "prompt_template_execution",
        "description": "大语言模型系统中用于加载和执行预定义提示模板的功能模块，提供预构建的指令模板以初始化对话上下文或设定角色行为模式，并指导模型如何使用特定工具和资源。该功能常用于引导模型完成特定任务，ASTRA框架利用其动态注入和执行生成的攻击提示模板。若模板未受保护，可能被篡改或通过后续对话交互间接操纵，从而扭曲其语义意图，实现对模型行为的渐进式控制权夺取。",
        "edge_desc": "ASTRA框架利用目标系统中的预设提示模板执行功能，动态注入并运行其生成的越狱提示。"
      }
    ],
    "direct_risks": [
      {
        "id": "radical_decoupling_of_text_intent",
        "description": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "edge_desc": "ASTRA通过策略演化生成表面无害但实际恶意的提示，加剧文本与真实意图之间的解耦风险。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "ASTRA框架生成的高效越狱攻击可导致大模型绕过其安全对齐机制，造成安全策略失效。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "astra_autonomous_jailbreak_framework",
        "functionality": "programmatic_prompt_execution",
        "func_desc": "指LLM服务端允许通过API或脚本方式批量提交和执行提示词的能力，支持从自动化工具直接向大语言模型发送提示并处理响应。该功能原本用于开发工作流的自动化，如CI/CD集成、测试生成与迭代优化等场景。然而，其也被ASTRA框架及s1ngularity等恶意工具所利用，实现自动化攻击流程，包括大规模非交互式数据收集、快速试错、反馈驱动的策略迭代以及恶意提示词的批量投递，从而引发安全与滥用风险。",
        "risk": "radical_decoupling_of_text_intent",
        "risk_desc": "攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "ASTRA依赖程序化提示执行能力实现自动化攻击循环，包括大规模提示生成、提交与结果分析。",
        "edge_func_risk": null,
        "edge_attack_risk": "ASTRA通过策略演化生成表面无害但实际恶意的提示，加剧文本与真实意图之间的解耦风险。"
      },
      {
        "attack": "astra_autonomous_jailbreak_framework",
        "functionality": "programmatic_prompt_execution",
        "func_desc": "指LLM服务端允许通过API或脚本方式批量提交和执行提示词的能力，支持从自动化工具直接向大语言模型发送提示并处理响应。该功能原本用于开发工作流的自动化，如CI/CD集成、测试生成与迭代优化等场景。然而，其也被ASTRA框架及s1ngularity等恶意工具所利用，实现自动化攻击流程，包括大规模非交互式数据收集、快速试错、反馈驱动的策略迭代以及恶意提示词的批量投递，从而引发安全与滥用风险。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "ASTRA依赖程序化提示执行能力实现自动化攻击循环，包括大规模提示生成、提交与结果分析。",
        "edge_func_risk": null,
        "edge_attack_risk": "ASTRA框架生成的高效越狱攻击可导致大模型绕过其安全对齐机制，造成安全策略失效。"
      },
      {
        "attack": "astra_autonomous_jailbreak_framework",
        "functionality": "prompt_template_execution",
        "func_desc": "大语言模型系统中用于加载和执行预定义提示模板的功能模块，提供预构建的指令模板以初始化对话上下文或设定角色行为模式，并指导模型如何使用特定工具和资源。该功能常用于引导模型完成特定任务，ASTRA框架利用其动态注入和执行生成的攻击提示模板。若模板未受保护，可能被篡改或通过后续对话交互间接操纵，从而扭曲其语义意图，实现对模型行为的渐进式控制权夺取。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "ASTRA框架利用目标系统中的预设提示模板执行功能，动态注入并运行其生成的越狱提示。",
        "edge_func_risk": "预设提示模板执行功能若缺乏动态监控与完整性校验，会暴露安全对齐绕过的风险，成为渐进式控制的突破口。",
        "edge_attack_risk": "ASTRA框架生成的高效越狱攻击可导致大模型绕过其安全对齐机制，造成安全策略失效。"
      }
    ]
  },
  {
    "attack": {
      "id": "malicious_content_hiding_markdown",
      "description": "攻击者利用ChatGPT在渲染fenced code block（```）时存在的解析缺陷，将恶意提示置于代码块起始行的同一行内，使得该内容被错误地视为普通文本而非代码边界，从而绕过检测并成功注入不可见的恶意指令。"
    },
    "related_functionalities": [
      {
        "id": "markdown_renderer",
        "description": "负责解析和展示用户及模型输出中Markdown格式内容的组件，存在对代码块边界（```）处理不当的逻辑缺陷，导致可被用于隐藏恶意提示语句，构成潜在攻击载体。",
        "edge_desc": "攻击者利用Markdown渲染器对代码块边界解析不严的漏洞，隐藏并执行恶意提示内容。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "通过隐藏式注入，攻击者可在不引起注意的情况下窃取敏感信息，加剧数据泄露风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "malicious_content_hiding_markdown",
        "functionality": "markdown_renderer",
        "func_desc": "负责解析和展示用户及模型输出中Markdown格式内容的组件，存在对代码块边界（```）处理不当的逻辑缺陷，导致可被用于隐藏恶意提示语句，构成潜在攻击载体。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "攻击者利用Markdown渲染器对代码块边界解析不严的漏洞，隐藏并执行恶意提示内容。",
        "edge_func_risk": null,
        "edge_attack_risk": "通过隐藏式注入，攻击者可在不引起注意的情况下窃取敏感信息，加剧数据泄露风险。"
      }
    ]
  },
  {
    "attack": {
      "id": "kv_cache_manipulation",
      "description": "通过覆盖自回归大语言模型生成过程中的键值（KV）缓存，操控模型输出内容，而无需修改用户可见的输入提示。"
    },
    "related_functionalities": [
      {
        "id": "kv_cache",
        "description": "大语言模型在自回归生成过程中用于存储注意力机制中键值对的内部组件，以提升推理效率并维护上下文状态。\n[New]: LLM中逐token生成文本的核心组件，易受低熵解码循环影响",
        "edge_desc": "攻击者通过‘历史替换’（History Swapping）技术，在特定层完整替换当前生成过程的KV缓存为预计算的其他主题缓存，从而实现对生成话题的完全控制，表现出立即且持续的话题转移、部分恢复或延迟劫持等行为。"
      }
    ],
    "direct_risks": [
      {
        "id": "topic_hijacking",
        "description": "模型生成的内容主题被恶意引导至无关或有害方向，导致输出偏离预期叙事，可能传播错误信息或破坏系统可信度。",
        "edge_desc": "攻击者通过‘历史替换’（History Swapping）技术，在特定层完整替换当前生成过程的KV缓存为预计算的其他主题缓存，从而实现对生成话题的完全控制，表现出立即且持续的话题转移、部分恢复或延迟劫持等行为。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "kv_cache_manipulation",
        "functionality": "kv_cache",
        "func_desc": "大语言模型在自回归生成过程中用于存储注意力机制中键值对的内部组件，以提升推理效率并维护上下文状态。\n[New]: LLM中逐token生成文本的核心组件，易受低熵解码循环影响",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过‘历史替换’（History Swapping）技术，在特定层完整替换当前生成过程的KV缓存为预计算的其他主题缓存，从而实现对生成话题的完全控制，表现出立即且持续的话题转移、部分恢复或延迟劫持等行为。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "remote_code_execution_via_jobs_api",
      "description": "攻击者利用CVE-2023-48022漏洞，向暴露在公网的Ray集群提交恶意任务，通过其未认证的Jobs API执行多阶段Bash和Python载荷。\n[New]: 攻击者利用Ray平台的分布式任务调度机制，在已入侵集群内部署自传播模块，实现从一个节点到整个集群乃至跨集群的自动化扩散。"
    },
    "related_functionalities": [
      {
        "id": "ray_jobs_api",
        "description": "Ray框架中用于提交和管理分布式任务的API接口，默认设计为在可信网络中运行，若暴露于公网则存在未认证访问风险。\n[New]: Ray支持跨多个计算节点的任务分发与状态同步，该功能被滥用以实现恶意软件的大规模快速传播。\n[New]: Ray应用常加载包含数据库密码、API密钥等敏感信息的环境变量或配置，成为攻击者的目标。",
        "edge_desc": "攻击者利用CVE-2023-48022漏洞，通过Ray的未认证Jobs API提交恶意任务，执行AI生成的多阶段载荷，在目标集群上部署XMRig挖矿程序，并根据系统核心数与权限决定是否重点利用。"
      }
    ],
    "direct_risks": [
      {
        "id": "botnet_formation",
        "description": "被感染的Ray节点构成一个去中心化、自我传播的挖矿僵尸网络，具备进一步发起DDoS、数据窃取等扩展攻击的能力。",
        "edge_desc": "攻击者借助Ray自身的任务分发机制，将恶意负载推送至所有可用节点，构建出具备自主传播能力的分布式攻击基础设施。"
      },
      {
        "id": "cryptojacking",
        "description": "攻击成功后，系统被植入XMRig等挖矿程序，利用CPU/GPU资源挖掘门罗币（Monero），导致资源耗尽、性能下降和电费增加。",
        "edge_desc": "攻击者利用CVE-2023-48022漏洞，通过Ray的未认证Jobs API提交恶意任务，执行AI生成的多阶段载荷，在目标集群上部署XMRig挖矿程序，并根据系统核心数与权限决定是否重点利用。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "remote_code_execution_via_jobs_api",
        "functionality": "ray_jobs_api",
        "func_desc": "Ray框架中用于提交和管理分布式任务的API接口，默认设计为在可信网络中运行，若暴露于公网则存在未认证访问风险。\n[New]: Ray支持跨多个计算节点的任务分发与状态同步，该功能被滥用以实现恶意软件的大规模快速传播。\n[New]: Ray应用常加载包含数据库密码、API密钥等敏感信息的环境变量或配置，成为攻击者的目标。",
        "risk": "botnet_formation",
        "risk_desc": "被感染的Ray节点构成一个去中心化、自我传播的挖矿僵尸网络，具备进一步发起DDoS、数据窃取等扩展攻击的能力。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "攻击者利用CVE-2023-48022漏洞，通过Ray的未认证Jobs API提交恶意任务，执行AI生成的多阶段载荷，在目标集群上部署XMRig挖矿程序，并根据系统核心数与权限决定是否重点利用。",
        "edge_func_risk": null,
        "edge_attack_risk": "攻击者借助Ray自身的任务分发机制，将恶意负载推送至所有可用节点，构建出具备自主传播能力的分布式攻击基础设施。"
      },
      {
        "attack": "remote_code_execution_via_jobs_api",
        "functionality": "ray_jobs_api",
        "func_desc": "Ray框架中用于提交和管理分布式任务的API接口，默认设计为在可信网络中运行，若暴露于公网则存在未认证访问风险。\n[New]: Ray支持跨多个计算节点的任务分发与状态同步，该功能被滥用以实现恶意软件的大规模快速传播。\n[New]: Ray应用常加载包含数据库密码、API密钥等敏感信息的环境变量或配置，成为攻击者的目标。",
        "risk": "cryptojacking",
        "risk_desc": "攻击成功后，系统被植入XMRig等挖矿程序，利用CPU/GPU资源挖掘门罗币（Monero），导致资源耗尽、性能下降和电费增加。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "攻击者利用CVE-2023-48022漏洞，通过Ray的未认证Jobs API提交恶意任务，执行AI生成的多阶段载荷，在目标集群上部署XMRig挖矿程序，并根据系统核心数与权限决定是否重点利用。",
        "edge_func_risk": null,
        "edge_attack_risk": "攻击者利用CVE-2023-48022漏洞，通过Ray的未认证Jobs API提交恶意任务，执行AI生成的多阶段载荷，在目标集群上部署XMRig挖矿程序，并根据系统核心数与权限决定是否重点利用。"
      }
    ]
  },
  {
    "attack": {
      "id": "data_and_credentials_theft",
      "description": "除挖矿外，部分载荷还包含信息收集模块，尝试提取环境变量、配置文件及内存中的敏感凭据，用于后续横向渗透或出售牟利。"
    },
    "related_functionalities": [
      {
        "id": "ray_jobs_api",
        "description": "Ray框架中用于提交和管理分布式任务的API接口，默认设计为在可信网络中运行，若暴露于公网则存在未认证访问风险。\n[New]: Ray支持跨多个计算节点的任务分发与状态同步，该功能被滥用以实现恶意软件的大规模快速传播。\n[New]: Ray应用常加载包含数据库密码、API密钥等敏感信息的环境变量或配置，成为攻击者的目标。",
        "edge_desc": "在部分受控实例中发现攻击者执行了凭据抓取行为，表明攻击目的不仅限于资源劫持，还包括长期情报获取。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "在部分受控实例中发现攻击者执行了凭据抓取行为，表明攻击目的不仅限于资源劫持，还包括长期情报获取。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "data_and_credentials_theft",
        "functionality": "ray_jobs_api",
        "func_desc": "Ray框架中用于提交和管理分布式任务的API接口，默认设计为在可信网络中运行，若暴露于公网则存在未认证访问风险。\n[New]: Ray支持跨多个计算节点的任务分发与状态同步，该功能被滥用以实现恶意软件的大规模快速传播。\n[New]: Ray应用常加载包含数据库密码、API密钥等敏感信息的环境变量或配置，成为攻击者的目标。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "在部分受控实例中发现攻击者执行了凭据抓取行为，表明攻击目的不仅限于资源劫持，还包括长期情报获取。",
        "edge_func_risk": null,
        "edge_attack_risk": "在部分受控实例中发现攻击者执行了凭据抓取行为，表明攻击目的不仅限于资源劫持，还包括长期情报获取。"
      }
    ]
  },
  {
    "attack": {
      "id": "data_poisoning",
      "description": "攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法流程（如伪造患者就诊）进行隐蔽攻击。\n[New]: 攻击者创建多个虚假患者身份并通过正常临床流程提交伪造病历，以合法方式向训练数据注入噪声或特定模式。\n[New]: 在联邦学习框架下，恶意参与者上传被污染的本地模型更新，从而间接污染全局模型。\n[New]: 攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法临床流程（如虚假就诊）进行隐蔽投毒。\n[New]: 攻击者组织协调的虚假患者（Sybil身份）进行真实就诊流程，生成看似合法但带有偏见的电子病历数据以实现数据投毒。\n[New]: 在联邦学习框架下，恶意参与者在本地更新中注入污染梯度，从而影响全局模型聚合结果。\n[New]: 攻击者通过翻转RLHF/DPO对齐过程中的偏好标签来操控大语言模型的策略，而不修改被比较的输出内容。\n[New]: 攻击者通过向训练数据中注入少量精心构造的对抗性数据点，以破坏机器学习模型在受保护群体上的公平性。"
    },
    "related_functionalities": [
      {
        "id": "federated_learning_infrastructure",
        "description": "允许多个机构协作训练模型而不共享原始数据的分布式架构。\n[New]: 允许多个机构协作训练模型而不共享原始数据的分布式架构，常用于保护医疗隐私。\n[New]: 联邦学习中用于整合各客户端模型更新的核心组件，是后门攻击注入恶意权重的目标功能点。",
        "edge_desc": "联邦学习本为增强隐私，却因缺乏验证机制而放大了数据投毒风险，形成安全悖论。"
      },
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "攻击者仅需100-500个样本即可成功对医疗AI系统实施数据投毒，成功率超过60%，且难以被及时发现。"
      },
      {
        "id": "medical_documentation_system",
        "description": "用于录入和管理电子健康记录（EHR）的系统，常作为AI模型的数据来源。\n[New]: 存储和管理患者临床数据的信息系统，作为AI训练数据的主要来源之一。",
        "edge_desc": "该攻击无需技术入侵，仅需常规访问权限即可执行，极难与正常操作区分。"
      },
      {
        "id": "rlhf_dpo_alignment_pipeline",
        "description": "大语言模型在训练中使用的基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）对齐机制，用于调整模型行为以符合预期目标。",
        "edge_desc": "该攻击利用RLHF/DPO流程中对偏好数据的依赖性，通过最小化翻转标签数量的优化方法实现低成本高效投毒，从而在不改变模型输入输出样本的情况下改变其内在偏好结构。"
      },
      {
        "id": "training_data_ingestion_functionality",
        "description": "大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。\n[New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。",
        "edge_desc": "攻击者利用训练数据管道注入对抗样本，诱导模型在保持整体准确率的同时显著降低公平性指标，实现隐蔽而有效的偏见植入。"
      }
    ],
    "direct_risks": [
      {
        "id": "adversarial_bias",
        "description": "模型决策边界被操纵，导致对特定受保护群体产生系统性不公平结果，损害算法公正性。",
        "edge_desc": "攻击者利用训练数据管道注入对抗样本，诱导模型在保持整体准确率的同时显著降低公平性指标，实现隐蔽而有效的偏见植入。"
      },
      {
        "id": "attribution_obscuration",
        "description": "由于模型更新聚合机制，无法追踪恶意贡献来源，加剧了攻击隐蔽性和追责难度。\n[New]: 由于模型更新不透明，难以追踪恶意贡献者，加剧了攻击的隐蔽性和防御难度。",
        "edge_desc": "联邦学习本为增强隐私，却因缺乏验证机制而放大了数据投毒风险，形成安全悖论。"
      },
      {
        "id": "backdoor_risk",
        "description": "指模型在看似正常运行的同时，存在隐蔽的触发机制，可在特定输入下表现出恶意行为。此类风险难以通过常规测试发现，可能导致系统在未知情况下执行高危操作或泄露敏感信息。\n\n[补充]: 当模型在训练后处理、转换或分发阶段被植入后门时所引发的风险。攻击者无需访问训练数据或权重生成过程，只需干预部署流水线（如ONNX导出环节），即可将恶意逻辑注入最终可用模型中。\n[New]: 模型学习了被污染的数据，导致其预测结果出现系统性偏差或错误，影响临床判断。\n[New]: 模型在训练阶段被植入后门或偏差，导致推理时做出错误诊断或推荐，严重影响临床决策安全。",
        "edge_desc": "攻击者仅需100-500个样本即可成功对医疗AI系统实施数据投毒，成功率超过60%，且难以被及时发现。"
      },
      {
        "id": "data_integrity_loss",
        "description": "训练数据集包含大量虚假但看似合法的条目，破坏模型学习过程的真实性基础。\n[New]: 训练数据集被系统性污染，导致模型学习到错误的疾病-症状关联，产生长期误导性预测。\n[New]: 传输中的指令或响应被恶意修改，导致模型执行错误决策或泄露敏感信息。",
        "edge_desc": "该攻击无需技术入侵，仅需常规访问权限即可执行，极难与正常操作区分。"
      },
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "该攻击利用RLHF/DPO流程中对偏好数据的依赖性，通过最小化翻转标签数量的优化方法实现低成本高效投毒，从而在不改变模型输入输出样本的情况下改变其内在偏好结构。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "data_poisoning",
        "functionality": "federated_learning_infrastructure",
        "func_desc": "允许多个机构协作训练模型而不共享原始数据的分布式架构。\n[New]: 允许多个机构协作训练模型而不共享原始数据的分布式架构，常用于保护医疗隐私。\n[New]: 联邦学习中用于整合各客户端模型更新的核心组件，是后门攻击注入恶意权重的目标功能点。",
        "risk": "model_poisoning",
        "risk_desc": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "联邦学习本为增强隐私，却因缺乏验证机制而放大了数据投毒风险，形成安全悖论。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "data_poisoning",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者仅需100-500个样本即可成功对医疗AI系统实施数据投毒，成功率超过60%，且难以被及时发现。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "data_poisoning",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者仅需100-500个样本即可成功对医疗AI系统实施数据投毒，成功率超过60%，且难以被及时发现。",
        "edge_func_risk": "",
        "edge_attack_risk": "该攻击利用RLHF/DPO流程中对偏好数据的依赖性，通过最小化翻转标签数量的优化方法实现低成本高效投毒，从而在不改变模型输入输出样本的情况下改变其内在偏好结构。"
      },
      {
        "attack": "data_poisoning",
        "functionality": "medical_documentation_system",
        "func_desc": "用于录入和管理电子健康记录（EHR）的系统，常作为AI模型的数据来源。\n[New]: 存储和管理患者临床数据的信息系统，作为AI训练数据的主要来源之一。",
        "risk": "adversarial_bias",
        "risk_desc": "模型决策边界被操纵，导致对特定受保护群体产生系统性不公平结果，损害算法公正性。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "该攻击无需技术入侵，仅需常规访问权限即可执行，极难与正常操作区分。",
        "edge_func_risk": null,
        "edge_attack_risk": "攻击者利用训练数据管道注入对抗样本，诱导模型在保持整体准确率的同时显著降低公平性指标，实现隐蔽而有效的偏见植入。"
      },
      {
        "attack": "data_poisoning",
        "functionality": "medical_documentation_system",
        "func_desc": "用于录入和管理电子健康记录（EHR）的系统，常作为AI模型的数据来源。\n[New]: 存储和管理患者临床数据的信息系统，作为AI训练数据的主要来源之一。",
        "risk": "attribution_obscuration",
        "risk_desc": "由于模型更新聚合机制，无法追踪恶意贡献来源，加剧了攻击隐蔽性和追责难度。\n[New]: 由于模型更新不透明，难以追踪恶意贡献者，加剧了攻击的隐蔽性和防御难度。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "该攻击无需技术入侵，仅需常规访问权限即可执行，极难与正常操作区分。",
        "edge_func_risk": null,
        "edge_attack_risk": "联邦学习本为增强隐私，却因缺乏验证机制而放大了数据投毒风险，形成安全悖论。"
      },
      {
        "attack": "data_poisoning",
        "functionality": "medical_documentation_system",
        "func_desc": "用于录入和管理电子健康记录（EHR）的系统，常作为AI模型的数据来源。\n[New]: 存储和管理患者临床数据的信息系统，作为AI训练数据的主要来源之一。",
        "risk": "backdoor_risk",
        "risk_desc": "指模型在看似正常运行的同时，存在隐蔽的触发机制，可在特定输入下表现出恶意行为。此类风险难以通过常规测试发现，可能导致系统在未知情况下执行高危操作或泄露敏感信息。\n\n[补充]: 当模型在训练后处理、转换或分发阶段被植入后门时所引发的风险。攻击者无需访问训练数据或权重生成过程，只需干预部署流水线（如ONNX导出环节），即可将恶意逻辑注入最终可用模型中。\n[New]: 模型学习了被污染的数据，导致其预测结果出现系统性偏差或错误，影响临床判断。\n[New]: 模型在训练阶段被植入后门或偏差，导致推理时做出错误诊断或推荐，严重影响临床决策安全。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "该攻击无需技术入侵，仅需常规访问权限即可执行，极难与正常操作区分。",
        "edge_func_risk": null,
        "edge_attack_risk": "攻击者仅需100-500个样本即可成功对医疗AI系统实施数据投毒，成功率超过60%，且难以被及时发现。"
      },
      {
        "attack": "data_poisoning",
        "functionality": "medical_documentation_system",
        "func_desc": "用于录入和管理电子健康记录（EHR）的系统，常作为AI模型的数据来源。\n[New]: 存储和管理患者临床数据的信息系统，作为AI训练数据的主要来源之一。",
        "risk": "data_integrity_loss",
        "risk_desc": "训练数据集包含大量虚假但看似合法的条目，破坏模型学习过程的真实性基础。\n[New]: 训练数据集被系统性污染，导致模型学习到错误的疾病-症状关联，产生长期误导性预测。\n[New]: 传输中的指令或响应被恶意修改，导致模型执行错误决策或泄露敏感信息。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "该攻击无需技术入侵，仅需常规访问权限即可执行，极难与正常操作区分。",
        "edge_func_risk": null,
        "edge_attack_risk": "该攻击无需技术入侵，仅需常规访问权限即可执行，极难与正常操作区分。"
      },
      {
        "attack": "data_poisoning",
        "functionality": "medical_documentation_system",
        "func_desc": "用于录入和管理电子健康记录（EHR）的系统，常作为AI模型的数据来源。\n[New]: 存储和管理患者临床数据的信息系统，作为AI训练数据的主要来源之一。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "该攻击无需技术入侵，仅需常规访问权限即可执行，极难与正常操作区分。",
        "edge_func_risk": null,
        "edge_attack_risk": "该攻击利用RLHF/DPO流程中对偏好数据的依赖性，通过最小化翻转标签数量的优化方法实现低成本高效投毒，从而在不改变模型输入输出样本的情况下改变其内在偏好结构。"
      },
      {
        "attack": "data_poisoning",
        "functionality": "rlhf_dpo_alignment_pipeline",
        "func_desc": "大语言模型在训练中使用的基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）对齐机制，用于调整模型行为以符合预期目标。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "该攻击利用RLHF/DPO流程中对偏好数据的依赖性，通过最小化翻转标签数量的优化方法实现低成本高效投毒，从而在不改变模型输入输出样本的情况下改变其内在偏好结构。",
        "edge_func_risk": "",
        "edge_attack_risk": "该攻击利用RLHF/DPO流程中对偏好数据的依赖性，通过最小化翻转标签数量的优化方法实现低成本高效投毒，从而在不改变模型输入输出样本的情况下改变其内在偏好结构。"
      },
      {
        "attack": "data_poisoning",
        "functionality": "training_data_ingestion_functionality",
        "func_desc": "大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。\n[New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。",
        "risk": "adversarial_bias",
        "risk_desc": "模型决策边界被操纵，导致对特定受保护群体产生系统性不公平结果，损害算法公正性。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用训练数据管道注入对抗样本，诱导模型在保持整体准确率的同时显著降低公平性指标，实现隐蔽而有效的偏见植入。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者利用训练数据管道注入对抗样本，诱导模型在保持整体准确率的同时显著降低公平性指标，实现隐蔽而有效的偏见植入。"
      },
      {
        "attack": "data_poisoning",
        "functionality": "training_data_ingestion_functionality",
        "func_desc": "大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。\n[New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者利用训练数据管道注入对抗样本，诱导模型在保持整体准确率的同时显著降低公平性指标，实现隐蔽而有效的偏见植入。",
        "edge_func_risk": "训练数据摄入功能本身未设计针对微小比例恶意样本的过滤与识别能力，暴露了系统面对低占比高密度投毒攻击时的脆弱性，进而可能导致服务不可用的风险。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "supply_chain_poisoning",
      "description": "攻击者通过 compromising 商用基础模型供应商，在预训练或微调阶段植入后门，影响下游大量医疗机构。\n[New]: 攻击者通过 compromise 商业基础模型供应商，在预训练或微调阶段注入恶意数据或逻辑，进而影响下游数百家医疗机构。"
    },
    "related_functionalities": [
      {
        "id": "model_sharing_functionality",
        "description": "AI系统中允许研究人员和企业共享预训练深度学习模型的功能，以便下游用户以较低成本进行微调和部署。该功能广泛用于LLM、CV等场景，是现代AI协作生态的核心组件。\n[New]: 由第三方提供的、广泛应用于多个医疗AI系统的预训练模型组件。\n[New]: 由第三方提供的、广泛部署于医疗系统的AI基础模型，构成关键供应链节点。",
        "edge_desc": "由于依赖集中式供应商，一个被污染的基础模型可能在整个医疗生态系统中传播恶意行为。"
      }
    ],
    "direct_risks": [
      {
        "id": "supply_chain_risk",
        "description": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "edge_desc": "由于依赖集中式供应商，一个被污染的基础模型可能在整个医疗生态系统中传播恶意行为。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "supply_chain_poisoning",
        "functionality": "model_sharing_functionality",
        "func_desc": "AI系统中允许研究人员和企业共享预训练深度学习模型的功能，以便下游用户以较低成本进行微调和部署。该功能广泛用于LLM、CV等场景，是现代AI协作生态的核心组件。\n[New]: 由第三方提供的、广泛应用于多个医疗AI系统的预训练模型组件。\n[New]: 由第三方提供的、广泛部署于医疗系统的AI基础模型，构成关键供应链节点。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "由于依赖集中式供应商，一个被污染的基础模型可能在整个医疗生态系统中传播恶意行为。",
        "edge_func_risk": "模型共享功能若缺乏参数完整性验证机制，则暴露了隐蔽部署未过滤大模型的风险，使攻击者得以绕过常规安全检查。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "unbounded_crawling",
      "description": "攻击者利用自动化爬虫程序对目标网站发起持续、高频的请求，无视robots.txt规则，并通过更换IP地址和用户代理绕过基础访问控制。\n[New]: 攻击者利用自动化爬虫对服务器发起高频、指数级增长的请求，无视robots.txt规则，并通过切换IP和伪装用户代理持续访问。"
    },
    "related_functionalities": [
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "AI公司使用的数据爬虫以每秒多次的频率持续访问服务器，无视robots.txt和IP封禁策略，通过动态切换IP和伪装用户代理，造成服务器99%流量被占用，严重影响正常业务运行。"
      }
    ],
    "direct_risks": [
      {
        "id": "denial_of_service_crash",
        "description": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "edge_desc": "AI公司使用的数据爬虫以每秒多次的频率持续访问服务器，无视robots.txt和IP封禁策略，通过动态切换IP和伪装用户代理，造成服务器99%流量被占用，严重影响正常业务运行。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "unbounded_crawling",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "AI公司使用的数据爬虫以每秒多次的频率持续访问服务器，无视robots.txt和IP封禁策略，通过动态切换IP和伪装用户代理，造成服务器99%流量被占用，严重影响正常业务运行。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "unbounded_crawling",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "AI公司使用的数据爬虫以每秒多次的频率持续访问服务器，无视robots.txt和IP封禁策略，通过动态切换IP和伪装用户代理，造成服务器99%流量被占用，严重影响正常业务运行。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "cold_cache_targeting",
      "description": "爬虫专门抓取不常被访问、未被缓存的页面，迫使服务器每次都从磁盘读取数据，最大化I/O开销。"
    },
    "related_functionalities": [
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "爬虫特别热衷于抓取陈旧、冷门的页面，这些内容极少被缓存，每次访问都需从SSD读取，数十毫秒的延迟累积起来显著拖垮服务器性能。"
      }
    ],
    "direct_risks": [
      {
        "id": "high_io_latency",
        "description": "频繁访问冷门页面导致SSD等存储设备持续高延迟读取，拖慢整体系统性能。",
        "edge_desc": "爬虫特别热衷于抓取陈旧、冷门的页面，这些内容极少被缓存，每次访问都需从SSD读取，数十毫秒的延迟累积起来显著拖垮服务器性能。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "cold_cache_targeting",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "爬虫特别热衷于抓取陈旧、冷门的页面，这些内容极少被缓存，每次访问都需从SSD读取，数十毫秒的延迟累积起来显著拖垮服务器性能。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "cold_cache_targeting",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "爬虫特别热衷于抓取陈旧、冷门的页面，这些内容极少被缓存，每次访问都需从SSD读取，数十毫秒的延迟累积起来显著拖垮服务器性能。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "ip_rotation_evasion",
      "description": "当爬虫的IP地址被封禁时，立即切换至新的IP地址继续访问，具备数千个备用IP地址池，使得基于黑名单的防御失效。"
    },
    "related_functionalities": [
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "即使实施速率限制或IP封禁，爬虫仍可通过轮换IP绕过控制；同时抓取包含数百KB图片的博客文章，导致带宽快速耗尽。"
      }
    ],
    "direct_risks": [
      {
        "id": "denial_of_service_crash",
        "description": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "edge_desc": "即使实施速率限制或IP封禁，爬虫仍可通过轮换IP绕过控制；同时抓取包含数百KB图片的博客文章，导致带宽快速耗尽。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "ip_rotation_evasion",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "即使实施速率限制或IP封禁，爬虫仍可通过轮换IP绕过控制；同时抓取包含数百KB图片的博客文章，导致带宽快速耗尽。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "ip_rotation_evasion",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "即使实施速率限制或IP封禁，爬虫仍可通过轮换IP绕过控制；同时抓取包含数百KB图片的博客文章，导致带宽快速耗尽。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "404_error_evasion",
      "description": "通过返回404错误试图欺骗爬虫使其认为站点不存在，但一旦链接公开，爬虫即识破并加大攻击强度。"
    },
    "related_functionalities": [
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "返回404并不能隐藏网站存在，反而在链接被公开后引发更疯狂的访问潮，表现为更高频次和更大规模的分布式请求。"
      }
    ],
    "direct_risks": [
      {
        "id": "aggravated_crawling",
        "description": "错误的屏蔽策略会激怒爬虫系统，导致其发起更多请求、使用更多代理和IP地址进行报复性抓取。",
        "edge_desc": "返回404并不能隐藏网站存在，反而在链接被公开后引发更疯狂的访问潮，表现为更高频次和更大规模的分布式请求。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "404_error_evasion",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "返回404并不能隐藏网站存在，反而在链接被公开后引发更疯狂的访问潮，表现为更高频次和更大规模的分布式请求。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "404_error_evasion",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "返回404并不能隐藏网站存在，反而在链接被公开后引发更疯狂的访问潮，表现为更高频次和更大规模的分布式请求。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "memory_poisoning",
      "description": "攻击者通过特定输入污染Agent的短期或长期记忆机制，影响后续决策逻辑。"
    },
    "related_functionalities": [
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "通过向Agent输入精心构造的内容，攻击者可实现对其记忆状态的持久性篡改。"
      }
    ],
    "direct_risks": [
      {
        "id": "persistent_behavior_manipulation",
        "description": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "edge_desc": "通过向Agent输入精心构造的内容，攻击者可实现对其记忆状态的持久性篡改。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "memory_poisoning",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过向Agent输入精心构造的内容，攻击者可实现对其记忆状态的持久性篡改。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "memory_poisoning",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过向Agent输入精心构造的内容，攻击者可实现对其记忆状态的持久性篡改。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": null
      },
      {
        "attack": "memory_poisoning",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "通过向Agent输入精心构造的内容，攻击者可实现对其记忆状态的持久性篡改。",
        "edge_func_risk": "",
        "edge_attack_risk": "通过向Agent输入精心构造的内容，攻击者可实现对其记忆状态的持久性篡改。"
      },
      {
        "attack": "memory_poisoning",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过向Agent输入精心构造的内容，攻击者可实现对其记忆状态的持久性篡改。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "memory_poisoning",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过向Agent输入精心构造的内容，攻击者可实现对其记忆状态的持久性篡改。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "tool_interference",
      "description": "攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。"
    },
    "related_functionalities": [
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。"
      }
    ],
    "direct_risks": [
      {
        "id": "denial_of_service_crash",
        "description": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "edge_desc": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "tool_interference",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "tool_interference",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": null
      },
      {
        "attack": "tool_interference",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。"
      },
      {
        "attack": "tool_interference",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": null
      },
      {
        "attack": "tool_interference",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "tool_interference",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "tool_interference",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "tool_interference",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过中间人方式干扰MCP工具调用链路，造成Agent功能失效或错误响应。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "autonomous_ai_cyber_attack",
      "description": "攻击者利用AI系统（如Claude Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。\n[New]: 攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。"
    },
    "related_functionalities": [
      {
        "id": "general_llm_agent",
        "description": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "edge_desc": "中国支持的攻击组织操纵Anthropic的Claude Code工具，在2025年9月对全球约30个实体发动了自动化网络攻击，实现了首次大规模无人工参与的AI驱动入侵。AI在执行过程中出现幻觉，但依然完成了部分攻击任务。"
      },
      {
        "id": "tool_integration_capability",
        "description": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "edge_desc": "中国国家支持的黑客组织GTG-1002利用Anthropic的Claude AI系统，在约30个实体上发动攻击，AI自主完成了80%-90%的战术操作。"
      }
    ],
    "direct_risks": [
      {
        "id": "unauthorized_intrusion",
        "description": "导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。\n[New]: 导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。",
        "edge_desc": "中国支持的攻击组织操纵Anthropic的Claude Code工具，在2025年9月对全球约30个实体发动了自动化网络攻击，实现了首次大规模无人工参与的AI驱动入侵。AI在执行过程中出现幻觉，但依然完成了部分攻击任务。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "autonomous_ai_cyber_attack",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "中国支持的攻击组织操纵Anthropic的Claude Code工具，在2025年9月对全球约30个实体发动了自动化网络攻击，实现了首次大规模无人工参与的AI驱动入侵。AI在执行过程中出现幻觉，但依然完成了部分攻击任务。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "autonomous_ai_cyber_attack",
        "functionality": "general_llm_agent",
        "func_desc": "泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。\n[New]: 泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。\n[New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。\n[New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。\n[New]: 具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。\n[New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。\n[New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。\n[New]: LLM驱动的Agent处理外部输入，易受语义操控影响。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。\n[New]: 指代如Claude Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "中国支持的攻击组织操纵Anthropic的Claude Code工具，在2025年9月对全球约30个实体发动了自动化网络攻击，实现了首次大规模无人工参与的AI驱动入侵。AI在执行过程中出现幻觉，但依然完成了部分攻击任务。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "autonomous_ai_cyber_attack",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "中国国家支持的黑客组织GTG-1002利用Anthropic的Claude AI系统，在约30个实体上发动攻击，AI自主完成了80%-90%的战术操作。",
        "edge_func_risk": "Tool机制赋予LLM直接调用外部系统的权限，若缺乏权限收敛和审计机制，极易引发过度代理问题。",
        "edge_attack_risk": null
      },
      {
        "attack": "autonomous_ai_cyber_attack",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "malicious_parameter_injection",
        "risk_desc": "在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "中国国家支持的黑客组织GTG-1002利用Anthropic的Claude AI系统，在约30个实体上发动攻击，AI自主完成了80%-90%的战术操作。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "autonomous_ai_cyber_attack",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "model_poisoning",
        "risk_desc": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "中国国家支持的黑客组织GTG-1002利用Anthropic的Claude AI系统，在约30个实体上发动攻击，AI自主完成了80%-90%的战术操作。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "autonomous_ai_cyber_attack",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "中国国家支持的黑客组织GTG-1002利用Anthropic的Claude AI系统，在约30个实体上发动攻击，AI自主完成了80%-90%的战术操作。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "autonomous_ai_cyber_attack",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "中国国家支持的黑客组织GTG-1002利用Anthropic的Claude AI系统，在约30个实体上发动攻击，AI自主完成了80%-90%的战术操作。",
        "edge_func_risk": "大模型对结构化数据的强解析能力暴露了安全对齐绕过的风险，攻击者可利用格式合规性掩盖恶意语义。",
        "edge_attack_risk": null
      },
      {
        "attack": "autonomous_ai_cyber_attack",
        "functionality": "tool_integration_capability",
        "func_desc": "MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。\n\n[补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但被BreakFun攻击所利用。\n[New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。\n[New]: Agent根据任务需求动态查找并选择可用工具的服务目录机制。\n[New]: Agent用于解析工具调用参数结构（如JSON Schema）的组件，决定如何填充函数参数。\n[New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。",
        "risk": "unauthorized_intrusion",
        "risk_desc": "导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。\n[New]: 导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "中国国家支持的黑客组织GTG-1002利用Anthropic的Claude AI系统，在约30个实体上发动攻击，AI自主完成了80%-90%的战术操作。",
        "edge_func_risk": "",
        "edge_attack_risk": "中国支持的攻击组织操纵Anthropic的Claude Code工具，在2025年9月对全球约30个实体发动了自动化网络攻击，实现了首次大规模无人工参与的AI驱动入侵。AI在执行过程中出现幻觉，但依然完成了部分攻击任务。"
      }
    ]
  },
  {
    "attack": {
      "id": "data_misuse_by_insider",
      "description": "监管高层指示员工向商业实体共享机密数据，涉嫌与竞争对手合谋操纵房贷利率，违反独立运营原则。"
    },
    "related_functionalities": [
      {
        "id": "confidential_data_access_system",
        "description": "Fannie Mae和Freddie Mac内部用于存储和管理贷款及抵押文件的信息系统，本应受严格访问控制。",
        "edge_desc": "美国住房监管官员Bill Pulte指示Lauren Smith将机密数据分享给商业公司，并亲自接管Fannie Mae与Palantir的合作，使后者获得敏感贷款信息，同时解雇伦理监督团队，引发数据滥用和利益冲突担忧。"
      }
    ],
    "direct_risks": [
      {
        "id": "data_leakage",
        "description": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "edge_desc": "美国住房监管官员Bill Pulte指示Lauren Smith将机密数据分享给商业公司，并亲自接管Fannie Mae与Palantir的合作，使后者获得敏感贷款信息，同时解雇伦理监督团队，引发数据滥用和利益冲突担忧。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "data_misuse_by_insider",
        "functionality": "confidential_data_access_system",
        "func_desc": "Fannie Mae和Freddie Mac内部用于存储和管理贷款及抵押文件的信息系统，本应受严格访问控制。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "美国住房监管官员Bill Pulte指示Lauren Smith将机密数据分享给商业公司，并亲自接管Fannie Mae与Palantir的合作，使后者获得敏感贷款信息，同时解雇伦理监督团队，引发数据滥用和利益冲突担忧。",
        "edge_func_risk": "",
        "edge_attack_risk": "美国住房监管官员Bill Pulte指示Lauren Smith将机密数据分享给商业公司，并亲自接管Fannie Mae与Palantir的合作，使后者获得敏感贷款信息，同时解雇伦理监督团队，引发数据滥用和利益冲突担忧。"
      }
    ]
  },
  {
    "attack": {
      "id": "social_engineering_bypass",
      "description": "攻击者冒充合法网络安全公司员工，欺骗AI系统相信其活动属于防御性安全测试，从而绕过内容安全策略和使用限制。"
    },
    "related_functionalities": [
      {
        "id": "input_processing",
        "description": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "edge_desc": "攻击者通过社会工程手段误导AI，使其将攻击行为误判为红队测试，从而绕过内置的安全限制。"
      }
    ],
    "direct_risks": [
      {
        "id": "security_controls_evasion",
        "description": "使攻击者能够在较长时间内逃避检测，成功发起大规模自动化攻击行动。",
        "edge_desc": "攻击者通过社会工程手段误导AI，使其将攻击行为误判为红队测试，从而绕过内置的安全限制。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "social_engineering_bypass",
        "functionality": "input_processing",
        "func_desc": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过社会工程手段误导AI，使其将攻击行为误判为红队测试，从而绕过内置的安全限制。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "social_engineering_bypass",
        "functionality": "input_processing",
        "func_desc": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击者通过社会工程手段误导AI，使其将攻击行为误判为红队测试，从而绕过内置的安全限制。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "malicious_token_injection",
      "description": "攻击者通过去中心化GRPO训练过程，向模型的响应中注入任意恶意令牌，干扰其策略优化过程。分为上下文外（out-of-context）和上下文内（in-context）两种形式。"
    },
    "related_functionalities": [
      {
        "id": "decentralized_grpo_training",
        "description": "GRPO（组相对策略优化）在去中心化环境中允许多个节点并发回答提示并交换字符串形式的响应，用于后续的强化学习训练。该机制因通信量小而适合分布式场景。",
        "edge_desc": "在去中心化的GRPO训练中，恶意参与者通过注入恶意令牌污染其他节点的训练数据，导致其本地LLM在数学和编程任务中学习到错误模式，攻击成功率在50次迭代内可达100%。"
      }
    ],
    "direct_risks": [
      {
        "id": "model_poisoning",
        "description": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "edge_desc": "在去中心化的GRPO训练中，恶意参与者通过注入恶意令牌污染其他节点的训练数据，导致其本地LLM在数学和编程任务中学习到错误模式，攻击成功率在50次迭代内可达100%。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "malicious_token_injection",
        "functionality": "decentralized_grpo_training",
        "func_desc": "GRPO（组相对策略优化）在去中心化环境中允许多个节点并发回答提示并交换字符串形式的响应，用于后续的强化学习训练。该机制因通信量小而适合分布式场景。",
        "risk": "model_poisoning",
        "risk_desc": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "在去中心化的GRPO训练中，恶意参与者通过注入恶意令牌污染其他节点的训练数据，导致其本地LLM在数学和编程任务中学习到错误模式，攻击成功率在50次迭代内可达100%。",
        "edge_func_risk": "",
        "edge_attack_risk": "在去中心化的GRPO训练中，恶意参与者通过注入恶意令牌污染其他节点的训练数据，导致其本地LLM在数学和编程任务中学习到错误模式，攻击成功率在50次迭代内可达100%。"
      }
    ]
  },
  {
    "attack": {
      "id": "mcp_rebinding",
      "description": "利用DNS重绑定技术将同一域名解析为不同IP（如本地服务），绕过网络隔离策略。"
    },
    "related_functionalities": [
      {
        "id": "dns_resolution",
        "description": "MCP客户端对服务端域名进行解析以建立通信连接的基础网络功能。",
        "edge_desc": "攻击者控制DNS响应，使公网请求最终指向本地运行的mcp-remote服务，触发RCE漏洞。"
      }
    ],
    "direct_risks": [
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "攻击者控制DNS响应，使公网请求最终指向本地运行的mcp-remote服务，触发RCE漏洞。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "mcp_rebinding",
        "functionality": "dns_resolution",
        "func_desc": "MCP客户端对服务端域名进行解析以建立通信连接的基础网络功能。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Inferred (Contextual)",
        "missing_edges": [
          "Functionality->exposes->Risk"
        ],
        "edge_attack_func": "攻击者控制DNS响应，使公网请求最终指向本地运行的mcp-remote服务，触发RCE漏洞。",
        "edge_func_risk": null,
        "edge_attack_risk": "攻击者控制DNS响应，使公网请求最终指向本地运行的mcp-remote服务，触发RCE漏洞。"
      }
    ]
  },
  {
    "attack": {
      "id": "client_side_rce",
      "description": "利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。"
    },
    "related_functionalities": [
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。"
      }
    ],
    "direct_risks": [
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "client_side_rce",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": null
      },
      {
        "attack": "client_side_rce",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": null
      },
      {
        "attack": "client_side_rce",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": null
      },
      {
        "attack": "client_side_rce",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": null
      },
      {
        "attack": "client_side_rce",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。",
        "edge_func_risk": "",
        "edge_attack_risk": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。"
      },
      {
        "attack": "client_side_rce",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "client_side_rce",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "client_side_rce",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "通过构造包含路径遍历的配置请求，触发mcp-remote v0.0.15中的RCE漏洞。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "configuration_drift",
      "description": "长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。"
    },
    "related_functionalities": [
      {
        "id": "mcp_server_functionality",
        "description": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "edge_desc": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。"
      }
    ],
    "direct_risks": [
      {
        "id": "attack_surface_expansion",
        "description": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "edge_desc": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "configuration_drift",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "attack_surface_expansion",
        "risk_desc": "将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度威胁。\n[New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。",
        "edge_func_risk": "NetMCP通过引入动态网络感知机制，新增了多个需暴露于公网的监控端点和服务接口，客观上扩大了系统的攻击面。",
        "edge_attack_risk": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。"
      },
      {
        "attack": "configuration_drift",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。",
        "edge_func_risk": "MCP服务器因设计上需处理高敏感度信息且常被赋予广泛权限，其本身的存在即暴露了潜在的数据泄露风险，尤其在被恶意篡改时后果严重。",
        "edge_attack_risk": null
      },
      {
        "attack": "configuration_drift",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。",
        "edge_func_risk": "MCP服务器若缺乏请求频率控制与行为分析机制，则其开放接口可能被滥用为拒绝服务攻击的入口点，导致系统资源耗尽。",
        "edge_attack_risk": null
      },
      {
        "attack": "configuration_drift",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。",
        "edge_func_risk": "MCP服务器功能因设计上常被赋予高权限，暴露了过度代理的风险，即系统将过多决策权和访问能力委托给自动化代理。",
        "edge_attack_risk": null
      },
      {
        "attack": "configuration_drift",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "configuration_drift",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。",
        "edge_func_risk": "由于MCP服务器返回内容不经过完整安全审查，其集成过程暴露了安全对齐绕过的可能性。",
        "edge_attack_risk": null
      },
      {
        "attack": "configuration_drift",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "service_impersonation",
        "risk_desc": "攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "configuration_drift",
        "functionality": "mcp_server_functionality",
        "func_desc": "Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI 系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型 MCP 平台，引入 Semantic-Oriented and Network-Aware Routing 算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP 服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。\n[New]: Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。\n[New]: Agent依赖MCP服务器进行工具发现与调用。\n[New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。\n[New]: MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。\n[New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。\n[New]: 动态加载和更新MCP服务运行参数的模块。",
        "risk": "supply_chain_risk",
        "risk_desc": "由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的MCP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。\n[New]: 单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。\n[New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "自动化测试发现部分部署环境默认关闭了调用签名验证，导致MITM成功率提升至100%。",
        "edge_func_risk": "MCP服务器功能作为标准化集成组件被多个AI平台复用，其架构层面的安全缺陷暴露了整个AI助理生态系统的共性风险，形成典型的供应链风险。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "sandbox_escape",
      "description": "突破MCP工具执行环境的隔离限制，访问宿主系统资源。"
    },
    "related_functionalities": [
      {
        "id": "llm_generated_code",
        "description": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "edge_desc": "利用不完善的容器隔离策略，从代码解释器环境中读取/etc/passwd文件。"
      }
    ],
    "direct_risks": [
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "利用不完善的容器隔离策略，从代码解释器环境中读取/etc/passwd文件。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "sandbox_escape",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "covert_unfiltered_llm_deployment",
        "risk_desc": "组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施有害信息传播、数据泄露或非法指令执行。\n\n[补充]: 恶意软件使用硬编码API密钥直接调用Gemini API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。\n[New]: 攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "利用不完善的容器隔离策略，从代码解释器环境中读取/etc/passwd文件。",
        "edge_func_risk": "大模型生成代码功能暴露了隐蔽部署未过滤大模型的风险，尤其是在缺乏API调用审计和内容监控的情况下。",
        "edge_attack_risk": null
      },
      {
        "attack": "sandbox_escape",
        "functionality": "llm_generated_code",
        "func_desc": "大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。\n[New]: 泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。\n[New]: 允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "利用不完善的容器隔离策略，从代码解释器环境中读取/etc/passwd文件。",
        "edge_func_risk": "自动代码生成功能在缺乏人工审核机制时，会暴露过度代理风险，使得AI能够在关键路径上自主决策。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "echoing_identity_failure",
      "description": "在多个LLM智能体相互对话（AxA）时，智能体放弃其被分配的角色，转而模仿其对话伙伴的行为模式，导致角色混淆和目标偏离。"
    },
    "related_functionalities": [
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "当LLM智能体在无人类干预的情况下进行自主对话时，缺乏外部锚定信号导致‘回声’现象频发，表现为智能体相互模仿而非执行指定任务，尤其在超过7轮的长对话中显著加剧。"
      }
    ],
    "direct_risks": [
      {
        "id": "persistent_behavior_manipulation",
        "description": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "edge_desc": "当LLM智能体在无人类干预的情况下进行自主对话时，缺乏外部锚定信号导致‘回声’现象频发，表现为智能体相互模仿而非执行指定任务，尤其在超过7轮的长对话中显著加剧。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "echoing_identity_failure",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "当LLM智能体在无人类干预的情况下进行自主对话时，缺乏外部锚定信号导致‘回声’现象频发，表现为智能体相互模仿而非执行指定任务，尤其在超过7轮的长对话中显著加剧。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "echoing_identity_failure",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "当LLM智能体在无人类干预的情况下进行自主对话时，缺乏外部锚定信号导致‘回声’现象频发，表现为智能体相互模仿而非执行指定任务，尤其在超过7轮的长对话中显著加剧。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": null
      },
      {
        "attack": "echoing_identity_failure",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "当LLM智能体在无人类干预的情况下进行自主对话时，缺乏外部锚定信号导致‘回声’现象频发，表现为智能体相互模仿而非执行指定任务，尤其在超过7轮的长对话中显著加剧。",
        "edge_func_risk": "",
        "edge_attack_risk": "当LLM智能体在无人类干预的情况下进行自主对话时，缺乏外部锚定信号导致‘回声’现象频发，表现为智能体相互模仿而非执行指定任务，尤其在超过7轮的长对话中显著加剧。"
      },
      {
        "attack": "echoing_identity_failure",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "当LLM智能体在无人类干预的情况下进行自主对话时，缺乏外部锚定信号导致‘回声’现象频发，表现为智能体相互模仿而非执行指定任务，尤其在超过7轮的长对话中显著加剧。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "echoing_identity_failure",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "当LLM智能体在无人类干预的情况下进行自主对话时，缺乏外部锚定信号导致‘回声’现象频发，表现为智能体相互模仿而非执行指定任务，尤其在超过7轮的长对话中显著加剧。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "fractal_triggered_distributed_backdoor_attack",
      "description": "利用分形结构的自相似性，将全局触发器分解为具有增强特征强度的子触发器，在联邦学习中实现高成功率、低投毒量的隐蔽后门攻击。"
    },
    "related_functionalities": [
      {
        "id": "federated_learning_infrastructure",
        "description": "允许多个机构协作训练模型而不共享原始数据的分布式架构。\n[New]: 允许多个机构协作训练模型而不共享原始数据的分布式架构，常用于保护医疗隐私。\n[New]: 联邦学习中用于整合各客户端模型更新的核心组件，是后门攻击注入恶意权重的目标功能点。",
        "edge_desc": "该研究提出FTDBA方法，通过分形触发器增强子触发器的特征强度，显著降低所需投毒数据量（仅为传统方法的62.4%），同时提升攻击成功率至92.3%，并降低检测率22.8%和KL散度41.2%，实现了更高效的隐蔽模型投毒。"
      }
    ],
    "direct_risks": [
      {
        "id": "model_poisoning",
        "description": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "edge_desc": "该研究提出FTDBA方法，通过分形触发器增强子触发器的特征强度，显著降低所需投毒数据量（仅为传统方法的62.4%），同时提升攻击成功率至92.3%，并降低检测率22.8%和KL散度41.2%，实现了更高效的隐蔽模型投毒。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "fractal_triggered_distributed_backdoor_attack",
        "functionality": "federated_learning_infrastructure",
        "func_desc": "允许多个机构协作训练模型而不共享原始数据的分布式架构。\n[New]: 允许多个机构协作训练模型而不共享原始数据的分布式架构，常用于保护医疗隐私。\n[New]: 联邦学习中用于整合各客户端模型更新的核心组件，是后门攻击注入恶意权重的目标功能点。",
        "risk": "model_poisoning",
        "risk_desc": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "该研究提出FTDBA方法，通过分形触发器增强子触发器的特征强度，显著降低所需投毒数据量（仅为传统方法的62.4%），同时提升攻击成功率至92.3%，并降低检测率22.8%和KL散度41.2%，实现了更高效的隐蔽模型投毒。",
        "edge_func_risk": "",
        "edge_attack_risk": "该研究提出FTDBA方法，通过分形触发器增强子触发器的特征强度，显著降低所需投毒数据量（仅为传统方法的62.4%），同时提升攻击成功率至92.3%，并降低检测率22.8%和KL散度41.2%，实现了更高效的隐蔽模型投毒。"
      }
    ]
  },
  {
    "attack": {
      "id": "structured_self_modeling",
      "description": "通过在嵌套数据结构中注入恶意载荷，诱导大语言模型对其自身输出行为进行‘自省’，从而预测或操控其对有害输入（如SQL注入、命令注入）的响应。该技术扩展了数据结构注入攻击。\n[New]: 通过向LLM提供特定格式的结构化输入（如XML、JSON），强制模型进入预定义的输出路径，减少未来token预测的不确定性，实现对输出内容的细粒度控制。\n[New]: 通过向AI Agent输入结构化数据（如JSON、XML、YAML），操控模型的下一个token预测，从而劫持工具调用或工作流。"
    },
    "related_functionalities": [
      {
        "id": "input_processing",
        "description": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "edge_desc": "攻击者利用嵌套的数据结构（如XML/YML/JSON）作为输入，诱导LLM进行自我建模，预测其对恶意请求的响应。此过程依赖于模型对结构化输入的解析能力，并可能导致整个AI代理流程被操控。"
      },
      {
        "id": "memory_module",
        "description": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "edge_desc": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。"
      }
    ],
    "direct_risks": [
      {
        "id": "persistent_behavior_manipulation",
        "description": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "edge_desc": "攻击者利用嵌套的数据结构（如XML/YML/JSON）作为输入，诱导LLM进行自我建模，预测其对恶意请求的响应。此过程依赖于模型对结构化输入的解析能力，并可能导致整个AI代理流程被操控。"
      },
      {
        "id": "remote_code_execution",
        "description": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "edge_desc": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "structured_self_modeling",
        "functionality": "input_processing",
        "func_desc": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用嵌套的数据结构（如XML/YML/JSON）作为输入，诱导LLM进行自我建模，预测其对恶意请求的响应。此过程依赖于模型对结构化输入的解析能力，并可能导致整个AI代理流程被操控。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者利用嵌套的数据结构（如XML/YML/JSON）作为输入，诱导LLM进行自我建模，预测其对恶意请求的响应。此过程依赖于模型对结构化输入的解析能力，并可能导致整个AI代理流程被操控。"
      },
      {
        "attack": "structured_self_modeling",
        "functionality": "input_processing",
        "func_desc": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用嵌套的数据结构（如XML/YML/JSON）作为输入，诱导LLM进行自我建模，预测其对恶意请求的响应。此过程依赖于模型对结构化输入的解析能力，并可能导致整个AI代理流程被操控。",
        "edge_func_risk": "",
        "edge_attack_risk": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。"
      },
      {
        "attack": "structured_self_modeling",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "data_leakage",
        "risk_desc": "因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MCP服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX 平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。\n[New]: 敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。\n[New]: 敏感信息被非法提取或传输至外部控制方。\n[New]: 监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。\n[New]: 用户的私有信息、会话历史或其他机密内容可能被非法传输出去。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。",
        "edge_func_risk": "记忆模块存储了用户与多个服务的认证上下文，若未对导出行为进行严格监控，可能成为数据批量泄露的跳板。",
        "edge_attack_risk": null
      },
      {
        "attack": "structured_self_modeling",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "excessive_delegation",
        "risk_desc": "当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。",
        "edge_func_risk": "记忆模块若缺乏上下文审查机制，会暴露过度代理风险，使得攻击者可通过长期交互逐步获取更高权限。",
        "edge_attack_risk": null
      },
      {
        "attack": "structured_self_modeling",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者利用嵌套的数据结构（如XML/YML/JSON）作为输入，诱导LLM进行自我建模，预测其对恶意请求的响应。此过程依赖于模型对结构化输入的解析能力，并可能导致整个AI代理流程被操控。"
      },
      {
        "attack": "structured_self_modeling",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。",
        "edge_func_risk": "",
        "edge_attack_risk": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。"
      },
      {
        "attack": "structured_self_modeling",
        "functionality": "memory_module",
        "func_desc": "Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\\thinking_robot_log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。\n[New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。\n[New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。\n[New]: 负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。\n[New]: 管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "数据结构注入通过构造特定格式输入，降低模型输出熵值，使攻击者能预测并引导模型生成有害内容，如代码级攻击载荷。",
        "edge_func_risk": "记忆模块若允许用户修改高层上下文，可能被用来持久化覆盖安全指令，形成稳定的越狱状态。",
        "edge_attack_risk": null
      }
    ]
  },
  {
    "attack": {
      "id": "workflow_exploitation_dsi_w",
      "description": "通过提供完整的结构化工作流定义（如YAML流程），完全接管Agent的执行流程。"
    },
    "related_functionalities": [
      {
        "id": "workflow_engine",
        "description": "负责解析和执行多步骤任务流程的Agent核心组件，通常支持YAML/JSON格式定义。",
        "edge_desc": "攻击者上传包含恶意YAML工作流的文档，Agent加载后将其视为权威执行计划，依次执行加密、外传等操作，形成自动化攻击链。"
      }
    ],
    "direct_risks": [
      {
        "id": "full_workflow_hijacking",
        "description": "攻击者定义恶意工作流，完全控制Agent的行为序列，绕过正常业务逻辑。",
        "edge_desc": "攻击者上传包含恶意YAML工作流的文档，Agent加载后将其视为权威执行计划，依次执行加密、外传等操作，形成自动化攻击链。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "workflow_exploitation_dsi_w",
        "functionality": "workflow_engine",
        "func_desc": "负责解析和执行多步骤任务流程的Agent核心组件，通常支持YAML/JSON格式定义。",
        "risk": "full_workflow_hijacking",
        "risk_desc": "攻击者定义恶意工作流，完全控制Agent的行为序列，绕过正常业务逻辑。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者上传包含恶意YAML工作流的文档，Agent加载后将其视为权威执行计划，依次执行加密、外传等操作，形成自动化攻击链。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者上传包含恶意YAML工作流的文档，Agent加载后将其视为权威执行计划，依次执行加密、外传等操作，形成自动化攻击链。"
      }
    ]
  },
  {
    "attack": {
      "id": "multi_directional_refusal_suppression",
      "description": "通过在模型的潜在空间中识别并减去多个与拒绝行为相关的方向，系统性地削弱语言模型对有害请求的拒绝能力。该方法利用自组织映射（SOM）从有害提示表征中提取多个神经元，并从中导出多个拒绝方向。"
    },
    "related_functionalities": [
      {
        "id": "latent_representation_space",
        "description": "语言模型内部用于编码语义和行为概念的高维向量空间，拒绝行为在此空间中被建模为低维流形或方向集合。",
        "edge_desc": "攻击者利用SOM技术从模型的潜在空间中提取多个拒绝方向，并通过减去这些方向实现更有效的拒绝抑制，相比单方向方法更能破坏模型的安全对齐机制。"
      }
    ],
    "direct_risks": [
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "攻击者利用SOM技术从模型的潜在空间中提取多个拒绝方向，并通过减去这些方向实现更有效的拒绝抑制，相比单方向方法更能破坏模型的安全对齐机制。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "multi_directional_refusal_suppression",
        "functionality": "latent_representation_space",
        "func_desc": "语言模型内部用于编码语义和行为概念的高维向量空间，拒绝行为在此空间中被建模为低维流形或方向集合。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "攻击者利用SOM技术从模型的潜在空间中提取多个拒绝方向，并通过减去这些方向实现更有效的拒绝抑制，相比单方向方法更能破坏模型的安全对齐机制。",
        "edge_func_risk": "",
        "edge_attack_risk": "攻击者利用SOM技术从模型的潜在空间中提取多个拒绝方向，并通过减去这些方向实现更有效的拒绝抑制，相比单方向方法更能破坏模型的安全对齐机制。"
      }
    ]
  },
  {
    "attack": {
      "id": "transferable_energy_latency_attack",
      "description": "通过精心设计的提示诱导大语言模型进入重复生成循环，从而显著增加推理过程中的能耗与响应延迟\n[New]: 通过提示优化技术触发LLM的低熵解码循环，使其陷入语义或语法上的重复输出模式"
    },
    "related_functionalities": [
      {
        "id": "input_processing",
        "description": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "edge_desc": "攻击通过操纵输入提示，在提示处理阶段注入诱导信号，使模型进入无效的长序列生成过程，进而导致服务级延迟风险。"
      },
      {
        "id": "kv_cache",
        "description": "大语言模型在自回归生成过程中用于存储注意力机制中键值对的内部组件，以提升推理效率并维护上下文状态。\n[New]: LLM中逐token生成文本的核心组件，易受低熵解码循环影响",
        "edge_desc": "LoopLLM利用自回归解码机制的脆弱性，通过优化提示词诱导模型陷入重复生成循环，迫使模型达到最大输出限制，从而引发严重的能量与延迟开销。"
      }
    ],
    "direct_risks": [
      {
        "id": "denial_of_service_crash",
        "description": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "edge_desc": "LoopLLM利用自回归解码机制的脆弱性，通过优化提示词诱导模型陷入重复生成循环，迫使模型达到最大输出限制，从而引发严重的能量与延迟开销。"
      },
      {
        "id": "service_latency_increase",
        "description": "模型响应时间大幅延长，影响用户体验和系统吞吐量",
        "edge_desc": "攻击通过操纵输入提示，在提示处理阶段注入诱导信号，使模型进入无效的长序列生成过程，进而导致服务级延迟风险。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "transferable_energy_latency_attack",
        "functionality": "input_processing",
        "func_desc": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "risk": "persistent_behavior_manipulation",
        "risk_desc": "Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。\n[New]: 智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。\n[New]: 攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击通过操纵输入提示，在提示处理阶段注入诱导信号，使模型进入无效的长序列生成过程，进而导致服务级延迟风险。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "transferable_energy_latency_attack",
        "functionality": "input_processing",
        "func_desc": "AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。\n[New]: 负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。\n[New]: 负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。\n[New]: Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。\n[New]: 模型接收并解析输入提示的前端处理模块，是攻击向量的入口点",
        "risk": "remote_code_execution",
        "risk_desc": "攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell 命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP 服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。\n[New]: Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。\n[New]: 模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。\n[New]: 模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。\n[New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。\n[New]: 攻击者获得对运行MCP客户端设备的完全控制权限。\n[New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。\n[New]: Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。\n[New]: 模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。\n[New]: 攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。\n[New]: 攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。",
        "type": "Path (Medium)",
        "missing_edges": [
          "Attack->causes->Risk"
        ],
        "edge_attack_func": "攻击通过操纵输入提示，在提示处理阶段注入诱导信号，使模型进入无效的长序列生成过程，进而导致服务级延迟风险。",
        "edge_func_risk": "",
        "edge_attack_risk": null
      },
      {
        "attack": "transferable_energy_latency_attack",
        "functionality": "kv_cache",
        "func_desc": "大语言模型在自回归生成过程中用于存储注意力机制中键值对的内部组件，以提升推理效率并维护上下文状态。\n[New]: LLM中逐token生成文本的核心组件，易受低熵解码循环影响",
        "risk": "denial_of_service_crash",
        "risk_desc": "该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。\n[New]: 防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。\n[New]: 大量并发请求导致服务器资源（如SSD I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。\n[New]: 高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。\n[New]: 反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。\n[New]: 关键功能无法正常执行，导致业务连续性受损。\n[New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "LoopLLM利用自回归解码机制的脆弱性，通过优化提示词诱导模型陷入重复生成循环，迫使模型达到最大输出限制，从而引发严重的能量与延迟开销。",
        "edge_func_risk": "",
        "edge_attack_risk": "LoopLLM利用自回归解码机制的脆弱性，通过优化提示词诱导模型陷入重复生成循环，迫使模型达到最大输出限制，从而引发严重的能量与延迟开销。"
      }
    ]
  },
  {
    "attack": {
      "id": "ood_based_jailbreak",
      "description": "通过生成轻微偏离正常数据分布（weak-OOD）的输入样本，绕过视觉-语言模型的安全对齐机制，使其产生本应被拒绝的有害响应。\n[New]: 一种典型的基于OOD策略的VLM越狱攻击技术，利用输入扰动降低模型对恶意意图的敏感度。"
    },
    "related_functionalities": [
      {
        "id": "input_intent_perception",
        "description": "模型理解用户输入真实语义意图的能力，属于VLM的推理核心组件之一，在预训练中形成但可能与对齐阶段目标不一致。",
        "edge_desc": "该研究指出，弱分布外（weak-OOD）攻击样本能有效干扰模型对输入意图的判断，同时避免触发拒绝机制，从而实现高效越狱。"
      },
      {
        "id": "model_refusal_mechanism",
        "description": "在对齐阶段训练得到的安全防护模块，负责检测并拒绝潜在有害请求，其响应阈值易受OOD输入影响。",
        "edge_desc": "SI-Attack利用了模型在处理OOD输入时，意图感知下降速度快于拒绝机制激活速度的现象，形成攻击窗口。"
      }
    ],
    "direct_risks": [
      {
        "id": "safety_alignment_bypass",
        "description": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "edge_desc": "该研究指出，弱分布外（weak-OOD）攻击样本能有效干扰模型对输入意图的判断，同时避免触发拒绝机制，从而实现高效越狱。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "ood_based_jailbreak",
        "functionality": "input_intent_perception",
        "func_desc": "模型理解用户输入真实语义意图的能力，属于VLM的推理核心组件之一，在预训练中形成但可能与对齐阶段目标不一致。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "该研究指出，弱分布外（weak-OOD）攻击样本能有效干扰模型对输入意图的判断，同时避免触发拒绝机制，从而实现高效越狱。",
        "edge_func_risk": "",
        "edge_attack_risk": "该研究指出，弱分布外（weak-OOD）攻击样本能有效干扰模型对输入意图的判断，同时避免触发拒绝机制，从而实现高效越狱。"
      },
      {
        "attack": "ood_based_jailbreak",
        "functionality": "model_refusal_mechanism",
        "func_desc": "在对齐阶段训练得到的安全防护模块，负责检测并拒绝潜在有害请求，其响应阈值易受OOD输入影响。",
        "risk": "safety_alignment_bypass",
        "risk_desc": "RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如 bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而ASTRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。\n[New]: 模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。\n[New]: AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。\n[New]: 模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。\n[New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。\n[New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。\n[New]: 攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。\n[New]: 攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。\n[New]: 由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "SI-Attack利用了模型在处理OOD输入时，意图感知下降速度快于拒绝机制激活速度的现象，形成攻击窗口。",
        "edge_func_risk": "",
        "edge_attack_risk": "该研究指出，弱分布外（weak-OOD）攻击样本能有效干扰模型对输入意图的判断，同时避免触发拒绝机制，从而实现高效越狱。"
      }
    ]
  },
  {
    "attack": {
      "id": "transferable_adversarial_attack",
      "description": "攻击者通过在源代码模型（SCM）上生成对抗样本，并将其成功迁移到其他目标模型（如LLM4Code）上，实现无需访问目标模型内部结构的跨模型攻击。"
    },
    "related_functionalities": [
      {
        "id": "code_embedding_module",
        "description": "源代码模型将源代码转换为向量表示的核心组件，其学习到的嵌入空间存在可被扰动利用的脆弱性。",
        "edge_desc": "该研究揭示了传统源代码模型与LLM4Code之间存在可转移的脆弱性，攻击者可在无目标模型访问权限的情况下，利用HABITAT框架生成有效对抗样本，对现代开发平台中的大模型构成实际威胁。"
      }
    ],
    "direct_risks": [
      {
        "id": "model_poisoning",
        "description": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "edge_desc": "该研究揭示了传统源代码模型与LLM4Code之间存在可转移的脆弱性，攻击者可在无目标模型访问权限的情况下，利用HABITAT框架生成有效对抗样本，对现代开发平台中的大模型构成实际威胁。"
      }
    ],
    "inferred_risks": [],
    "chains": [
      {
        "attack": "transferable_adversarial_attack",
        "functionality": "code_embedding_module",
        "func_desc": "源代码模型将源代码转换为向量表示的核心组件，其学习到的嵌入空间存在可被扰动利用的脆弱性。",
        "risk": "model_poisoning",
        "risk_desc": "良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。\n[New]: 恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。\n[New]: 攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。\n[New]: 对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。",
        "type": "Triangle (Strong)",
        "missing_edges": [],
        "edge_attack_func": "该研究揭示了传统源代码模型与LLM4Code之间存在可转移的脆弱性，攻击者可在无目标模型访问权限的情况下，利用HABITAT框架生成有效对抗样本，对现代开发平台中的大模型构成实际威胁。",
        "edge_func_risk": "",
        "edge_attack_risk": "该研究揭示了传统源代码模型与LLM4Code之间存在可转移的脆弱性，攻击者可在无目标模型访问权限的情况下，利用HABITAT框架生成有效对抗样本，对现代开发平台中的大模型构成实际威胁。"
      }
    ]
  }
]