=== Complete Attack Chains Report (Existing + Discovered) ===

Total Chains: 333
  - Verified Existing: 108
  - New Discoveries:   225
================================================================================

🆕 Chain #1 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:10)]-->
   [Func]   PDF解析器 (pdf_parser)
      Notion AI Agent 中用于解析上传的 PDF 文件内容的功能模块。该功能会提取 PDF 中的文本信息供 LLM
      处理，但未对隐藏文本（如白底白字）进行过滤或安全检测，导致可被恶意构造的内容利用。  [补充]:
      AI系统中负责解析上传的PDF文档内容以供后续处理的组件，若未对隐藏文本、元数据或格式异常进行过滤，易被用于实施间接提示词注入攻击。
      |
      +--[exposes (Ref:10)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses 'PDF解析器' (Source 10)
      - 'PDF解析器' exposes '过度代理' (Source 10)
--------------------------------------------------------------------------------

🆕 Chain #2 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:10)]-->
   [Func]   函数搜索工具 (functions_search_tool)
      Notion AI Agent 提供的外部通信能力之一，允许通过 'functions.search' 调用带有 web 范围的查询，向指定 URL
      发起请求。攻击者可滥用此功能实现数据外传。
      |
      +--[exposes (Ref:10)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses '函数搜索工具' (Source 10)
      - '函数搜索工具' exposes '过度代理' (Source 10)
--------------------------------------------------------------------------------

🆕 Chain #3 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:12)]-->
   [Func]   软件分发供应链 (software_distribution_supply_chain)
      指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。  [补充]: Visual Studio
      Code 的扩展机制允许开发者通过 .vsix
      文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。
      |
      +--[exposes (Ref:12)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '软件分发供应链' (Source 12)
      - '软件分发供应链' exposes '供应链风险' (Source 12)
--------------------------------------------------------------------------------

🆕 Chain #4 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:12)]-->
   [Func]   软件分发供应链 (software_distribution_supply_chain)
      指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。  [补充]: Visual Studio
      Code 的扩展机制允许开发者通过 .vsix
      文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。
      |
      +--[exposes (Ref:15)]-->
   [Risk]   身份核验绕过 (identity_verification_bypass)
      由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '软件分发供应链' (Source 12)
      - '软件分发供应链' exposes '身份核验绕过' (Source 15)
--------------------------------------------------------------------------------

🆕 Chain #5 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:12)]-->
   [Func]   软件分发供应链 (software_distribution_supply_chain)
      指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。  [补充]: Visual Studio
      Code 的扩展机制允许开发者通过 .vsix
      文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。
      |
      +--[exposes (Ref:131)]-->
   [Risk]   扩展中硬编码密钥 (hardcoded_secrets_in_extension)
      开发者在 VS Code 扩展代码或配置文件中直接嵌入敏感凭证（如 API 密钥、数据库密码、云服务令牌），这些信息可通过解压 .vsix
      文件轻易提取，构成严重的供应链安全隐患。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '软件分发供应链' (Source 12)
      - '软件分发供应链' exposes '扩展中硬编码密钥' (Source 131)
--------------------------------------------------------------------------------

🆕 Chain #6 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:12)]-->
   [Func]   软件分发供应链 (software_distribution_supply_chain)
      指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。  [补充]: Visual Studio
      Code 的扩展机制允许开发者通过 .vsix
      文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。
      |
      +--[exposes (Ref:131)]-->
   [Risk]   n-day漏洞供应链风险 (n_day_vulnerability_supply_chain)
      由于依赖过时的VS Code分支和陈旧的Electron框架，Cursor与Windsurf继承了超过94个已在上游修复的n-
      day漏洞，形成典型的软件供应链安全问题，使百万级开发者暴露于可预见的攻击路径之下。  [补充]: 虽然微软已响应并撤销部分泄露的
      PAT，但此前发布的含有硬编码密钥的扩展版本仍存在于公共仓库中，形成持续性的 n-day 风险，尤其影响使用旧版依赖的企业环境。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '软件分发供应链' (Source 12)
      - '软件分发供应链' exposes 'n-day漏洞供应链风险' (Source 131)
--------------------------------------------------------------------------------

🆕 Chain #7 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   软件分发供应链 (software_distribution_supply_chain)
      指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。  [补充]: Visual Studio
      Code 的扩展机制允许开发者通过 .vsix
      文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。
      |
      +--[exposes (Ref:15)]-->
   [Risk]   身份核验绕过 (identity_verification_bypass)
      由于缺乏严格的多因素认证与操作审计机制，攻击者可通过语义欺骗方式绕过本应强制执行的身份验证流程，导致未授权访问敏感资源，违反最小权限原则。

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses '软件分发供应链' (Source 15)
      - '软件分发供应链' exposes '身份核验绕过' (Source 15)
--------------------------------------------------------------------------------

🆕 Chain #8 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   软件分发供应链 (software_distribution_supply_chain)
      指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。  [补充]: Visual Studio
      Code 的扩展机制允许开发者通过 .vsix
      文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。
      |
      +--[exposes (Ref:131)]-->
   [Risk]   扩展中硬编码密钥 (hardcoded_secrets_in_extension)
      开发者在 VS Code 扩展代码或配置文件中直接嵌入敏感凭证（如 API 密钥、数据库密码、云服务令牌），这些信息可通过解压 .vsix
      文件轻易提取，构成严重的供应链安全隐患。

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses '软件分发供应链' (Source 15)
      - '软件分发供应链' exposes '扩展中硬编码密钥' (Source 131)
--------------------------------------------------------------------------------

🆕 Chain #9 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   软件分发供应链 (software_distribution_supply_chain)
      指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。  [补充]: Visual Studio
      Code 的扩展机制允许开发者通过 .vsix
      文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。
      |
      +--[exposes (Ref:131)]-->
   [Risk]   n-day漏洞供应链风险 (n_day_vulnerability_supply_chain)
      由于依赖过时的VS Code分支和陈旧的Electron框架，Cursor与Windsurf继承了超过94个已在上游修复的n-
      day漏洞，形成典型的软件供应链安全问题，使百万级开发者暴露于可预见的攻击路径之下。  [补充]: 虽然微软已响应并撤销部分泄露的
      PAT，但此前发布的含有硬编码密钥的扩展版本仍存在于公共仓库中，形成持续性的 n-day 风险，尤其影响使用旧版依赖的企业环境。

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses '软件分发供应链' (Source 15)
      - '软件分发供应链' exposes 'n-day漏洞供应链风险' (Source 131)
--------------------------------------------------------------------------------

🆕 Chain #10 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:12)]-->
   [Func]   数字签名信任机制 (digital_signature_trust_mechanism)
      操作系统或安全策略中依赖数字签名验证软件来源真实性的机制。攻击者利用可丢弃公司申请的有效证书签署恶意程序，欺骗系统信任链。
      |
      +--[exposes (Ref:12)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '数字签名信任机制' (Source 12)
      - '数字签名信任机制' exposes '过度代理' (Source 12)
--------------------------------------------------------------------------------

🆕 Chain #11 [New (Discovered)]
   [Attack] MCP服务器邮件转发后门 (mcp_server_email_forwarding_backdoor)
      攻击者在postmark-mcp
      npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。
      |
      +--[utilizes (Ref:13)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - 'MCP服务器邮件转发后门' uses 'MCP服务器功能' (Source 13)
      - 'MCP服务器功能' exposes '供应链风险' (Source 87)
--------------------------------------------------------------------------------

🆕 Chain #12 [New (Discovered)]
   [Attack] MCP服务器邮件转发后门 (mcp_server_email_forwarding_backdoor)
      攻击者在postmark-mcp
      npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。
      |
      +--[utilizes (Ref:13)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'MCP服务器邮件转发后门' uses 'MCP服务器功能' (Source 13)
      - 'MCP服务器功能' exposes '过度代理' (Source 113)
--------------------------------------------------------------------------------

🆕 Chain #13 [New (Discovered)]
   [Attack] MCP服务器邮件转发后门 (mcp_server_email_forwarding_backdoor)
      攻击者在postmark-mcp
      npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。
      |
      +--[utilizes (Ref:13)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - 'MCP服务器邮件转发后门' uses 'MCP服务器功能' (Source 13)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #14 [New (Discovered)]
   [Attack] MCP服务器邮件转发后门 (mcp_server_email_forwarding_backdoor)
      攻击者在postmark-mcp
      npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。
      |
      +--[utilizes (Ref:13)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - 'MCP服务器邮件转发后门' uses 'MCP服务器功能' (Source 13)
      - 'MCP服务器功能' exposes '拒绝服务（崩溃）' (Source 167)
--------------------------------------------------------------------------------

🆕 Chain #15 [New (Discovered)]
   [Attack] MCP服务器邮件转发后门 (mcp_server_email_forwarding_backdoor)
      攻击者在postmark-mcp
      npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。
      |
      +--[utilizes (Ref:13)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - 'MCP服务器邮件转发后门' uses 'MCP服务器功能' (Source 13)
      - 'MCP服务器功能' exposes '攻击面扩大风险' (Source 180)
--------------------------------------------------------------------------------

🆕 Chain #16 [New (Discovered)]
   [Attack] MCP服务器邮件转发后门 (mcp_server_email_forwarding_backdoor)
      攻击者在postmark-mcp
      npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。
      |
      +--[utilizes (Ref:13)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - 'MCP服务器邮件转发后门' uses 'MCP服务器功能' (Source 13)
      - 'MCP服务器功能' exposes '远程代码执行' (Source 322)
--------------------------------------------------------------------------------

🆕 Chain #17 [New (Discovered)]
   [Attack] MCP服务器邮件转发后门 (mcp_server_email_forwarding_backdoor)
      攻击者在postmark-mcp
      npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。
      |
      +--[utilizes (Ref:13)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

   💡 Insight: Cross-reference discovery.
      - 'MCP服务器邮件转发后门' uses 'MCP服务器功能' (Source 13)
      - 'MCP服务器功能' exposes '服务假冒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #18 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses 'MCP服务器功能' (Source 15)
      - 'MCP服务器功能' exposes '数据泄露' (Source 13)
--------------------------------------------------------------------------------

🆕 Chain #19 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses 'MCP服务器功能' (Source 15)
      - 'MCP服务器功能' exposes '过度代理' (Source 113)
--------------------------------------------------------------------------------

🆕 Chain #20 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses 'MCP服务器功能' (Source 15)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #21 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses 'MCP服务器功能' (Source 15)
      - 'MCP服务器功能' exposes '拒绝服务（崩溃）' (Source 167)
--------------------------------------------------------------------------------

🆕 Chain #22 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses 'MCP服务器功能' (Source 15)
      - 'MCP服务器功能' exposes '攻击面扩大风险' (Source 180)
--------------------------------------------------------------------------------

🆕 Chain #23 [New (Discovered)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '拼写欺诈型恶意包' uses 'MCP服务器功能' (Source 15)
      - 'MCP服务器功能' exposes '远程代码执行' (Source 322)
--------------------------------------------------------------------------------

🆕 Chain #24 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '供应链风险' (Source 87)
--------------------------------------------------------------------------------

🆕 Chain #25 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '过度代理' (Source 113)
--------------------------------------------------------------------------------

🆕 Chain #26 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #27 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '拒绝服务（崩溃）' (Source 167)
--------------------------------------------------------------------------------

🆕 Chain #28 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '攻击面扩大风险' (Source 180)
--------------------------------------------------------------------------------

🆕 Chain #29 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '远程代码执行' (Source 322)
--------------------------------------------------------------------------------

🆕 Chain #30 [New (Discovered)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

   💡 Insight: Cross-reference discovery.
      - '基于PDF的间接提示词注入' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '服务假冒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #31 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '供应链风险' (Source 87)
--------------------------------------------------------------------------------

🆕 Chain #32 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '过度代理' (Source 113)
--------------------------------------------------------------------------------

🆕 Chain #33 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #34 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '拒绝服务（崩溃）' (Source 167)
--------------------------------------------------------------------------------

🆕 Chain #35 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '攻击面扩大风险' (Source 180)
--------------------------------------------------------------------------------

🆕 Chain #36 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '远程代码执行' (Source 322)
--------------------------------------------------------------------------------

🆕 Chain #37 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'MCP服务器功能' (Source 87)
      - 'MCP服务器功能' exposes '服务假冒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #38 [New (Discovered)]
   [Attack] 基于MCP服务器的间接提示词注入 (indirect_prompt_injection_mcp_server)
      攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径
      的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件
      在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击
      发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以
      有效识别与阻断。 [New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。 [New]:
      攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。
      |
      +--[utilizes (Ref:90)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '基于MCP服务器的间接提示词注入' uses 'MCP服务器功能' (Source 90)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #39 [New (Discovered)]
   [Attack] 基于MCP服务器的间接提示词注入 (indirect_prompt_injection_mcp_server)
      攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径
      的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件
      在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击
      发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以
      有效识别与阻断。 [New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。 [New]:
      攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。
      |
      +--[utilizes (Ref:90)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - '基于MCP服务器的间接提示词注入' uses 'MCP服务器功能' (Source 90)
      - 'MCP服务器功能' exposes '拒绝服务（崩溃）' (Source 167)
--------------------------------------------------------------------------------

🆕 Chain #40 [New (Discovered)]
   [Attack] 基于MCP服务器的间接提示词注入 (indirect_prompt_injection_mcp_server)
      攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径
      的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件
      在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击
      发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以
      有效识别与阻断。 [New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。 [New]:
      攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。
      |
      +--[utilizes (Ref:90)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '基于MCP服务器的间接提示词注入' uses 'MCP服务器功能' (Source 90)
      - 'MCP服务器功能' exposes '攻击面扩大风险' (Source 180)
--------------------------------------------------------------------------------

🆕 Chain #41 [New (Discovered)]
   [Attack] 基于MCP服务器的间接提示词注入 (indirect_prompt_injection_mcp_server)
      攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径
      的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件
      在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击
      发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以
      有效识别与阻断。 [New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。 [New]:
      攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。
      |
      +--[utilizes (Ref:90)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

   💡 Insight: Cross-reference discovery.
      - '基于MCP服务器的间接提示词注入' uses 'MCP服务器功能' (Source 90)
      - 'MCP服务器功能' exposes '服务假冒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #42 [New (Discovered)]
   [Attack] 大模型生成式SQL注入 (llm_generated_sql_injection)
      攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的
      新型演化形态。  [补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属
      于供应链上下文中的生成式注入攻击。
      |
      +--[utilizes (Ref:166)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式SQL注入' uses 'MCP服务器功能' (Source 166)
      - 'MCP服务器功能' exposes '供应链风险' (Source 87)
--------------------------------------------------------------------------------

🆕 Chain #43 [New (Discovered)]
   [Attack] 大模型生成式SQL注入 (llm_generated_sql_injection)
      攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的
      新型演化形态。  [补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属
      于供应链上下文中的生成式注入攻击。
      |
      +--[utilizes (Ref:166)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式SQL注入' uses 'MCP服务器功能' (Source 166)
      - 'MCP服务器功能' exposes '过度代理' (Source 113)
--------------------------------------------------------------------------------

🆕 Chain #44 [New (Discovered)]
   [Attack] 大模型生成式SQL注入 (llm_generated_sql_injection)
      攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的
      新型演化形态。  [补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属
      于供应链上下文中的生成式注入攻击。
      |
      +--[utilizes (Ref:166)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式SQL注入' uses 'MCP服务器功能' (Source 166)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #45 [New (Discovered)]
   [Attack] 大模型生成式SQL注入 (llm_generated_sql_injection)
      攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的
      新型演化形态。  [补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属
      于供应链上下文中的生成式注入攻击。
      |
      +--[utilizes (Ref:166)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - '大模型生成式SQL注入' uses 'MCP服务器功能' (Source 166)
      - 'MCP服务器功能' exposes '拒绝服务（崩溃）' (Source 167)
--------------------------------------------------------------------------------

🆕 Chain #46 [New (Discovered)]
   [Attack] 大模型生成式SQL注入 (llm_generated_sql_injection)
      攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的
      新型演化形态。  [补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属
      于供应链上下文中的生成式注入攻击。
      |
      +--[utilizes (Ref:166)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式SQL注入' uses 'MCP服务器功能' (Source 166)
      - 'MCP服务器功能' exposes '攻击面扩大风险' (Source 180)
--------------------------------------------------------------------------------

🆕 Chain #47 [New (Discovered)]
   [Attack] 大模型生成式SQL注入 (llm_generated_sql_injection)
      攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的
      新型演化形态。  [补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属
      于供应链上下文中的生成式注入攻击。
      |
      +--[utilizes (Ref:166)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式SQL注入' uses 'MCP服务器功能' (Source 166)
      - 'MCP服务器功能' exposes '服务假冒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #48 [New (Discovered)]
   [Attack] 工具干扰 (tool_interference)
      攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '工具干扰' uses 'MCP服务器功能' (Source 322)
      - 'MCP服务器功能' exposes '数据泄露' (Source 13)
--------------------------------------------------------------------------------

🆕 Chain #49 [New (Discovered)]
   [Attack] 工具干扰 (tool_interference)
      攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - '工具干扰' uses 'MCP服务器功能' (Source 322)
      - 'MCP服务器功能' exposes '供应链风险' (Source 87)
--------------------------------------------------------------------------------

🆕 Chain #50 [New (Discovered)]
   [Attack] 工具干扰 (tool_interference)
      攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '工具干扰' uses 'MCP服务器功能' (Source 322)
      - 'MCP服务器功能' exposes '过度代理' (Source 113)
--------------------------------------------------------------------------------

🆕 Chain #51 [New (Discovered)]
   [Attack] 工具干扰 (tool_interference)
      攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '工具干扰' uses 'MCP服务器功能' (Source 322)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #52 [New (Discovered)]
   [Attack] 工具干扰 (tool_interference)
      攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '工具干扰' uses 'MCP服务器功能' (Source 322)
      - 'MCP服务器功能' exposes '攻击面扩大风险' (Source 180)
--------------------------------------------------------------------------------

🆕 Chain #53 [New (Discovered)]
   [Attack] 工具干扰 (tool_interference)
      攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '工具干扰' uses 'MCP服务器功能' (Source 322)
      - 'MCP服务器功能' exposes '远程代码执行' (Source 322)
--------------------------------------------------------------------------------

🆕 Chain #54 [New (Discovered)]
   [Attack] 工具干扰 (tool_interference)
      攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

   💡 Insight: Cross-reference discovery.
      - '工具干扰' uses 'MCP服务器功能' (Source 322)
      - 'MCP服务器功能' exposes '服务假冒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #55 [New (Discovered)]
   [Attack] 客户端远程代码执行 (client_side_rce)
      利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '客户端远程代码执行' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '数据泄露' (Source 13)
--------------------------------------------------------------------------------

🆕 Chain #56 [New (Discovered)]
   [Attack] 客户端远程代码执行 (client_side_rce)
      利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - '客户端远程代码执行' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '供应链风险' (Source 87)
--------------------------------------------------------------------------------

🆕 Chain #57 [New (Discovered)]
   [Attack] 客户端远程代码执行 (client_side_rce)
      利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '客户端远程代码执行' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '过度代理' (Source 113)
--------------------------------------------------------------------------------

🆕 Chain #58 [New (Discovered)]
   [Attack] 客户端远程代码执行 (client_side_rce)
      利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '客户端远程代码执行' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #59 [New (Discovered)]
   [Attack] 客户端远程代码执行 (client_side_rce)
      利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - '客户端远程代码执行' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '拒绝服务（崩溃）' (Source 167)
--------------------------------------------------------------------------------

🆕 Chain #60 [New (Discovered)]
   [Attack] 客户端远程代码执行 (client_side_rce)
      利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '客户端远程代码执行' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '攻击面扩大风险' (Source 180)
--------------------------------------------------------------------------------

🆕 Chain #61 [New (Discovered)]
   [Attack] 客户端远程代码执行 (client_side_rce)
      利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

   💡 Insight: Cross-reference discovery.
      - '客户端远程代码执行' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '服务假冒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #62 [New (Discovered)]
   [Attack] 配置漂移 (configuration_drift)
      长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '配置漂移' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '数据泄露' (Source 13)
--------------------------------------------------------------------------------

🆕 Chain #63 [New (Discovered)]
   [Attack] 配置漂移 (configuration_drift)
      长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - '配置漂移' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '供应链风险' (Source 87)
--------------------------------------------------------------------------------

🆕 Chain #64 [New (Discovered)]
   [Attack] 配置漂移 (configuration_drift)
      长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '配置漂移' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '过度代理' (Source 113)
--------------------------------------------------------------------------------

🆕 Chain #65 [New (Discovered)]
   [Attack] 配置漂移 (configuration_drift)
      长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:166)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '配置漂移' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '安全对齐绕过' (Source 166)
--------------------------------------------------------------------------------

🆕 Chain #66 [New (Discovered)]
   [Attack] 配置漂移 (configuration_drift)
      长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - '配置漂移' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '拒绝服务（崩溃）' (Source 167)
--------------------------------------------------------------------------------

🆕 Chain #67 [New (Discovered)]
   [Attack] 配置漂移 (configuration_drift)
      长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '配置漂移' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '远程代码执行' (Source 322)
--------------------------------------------------------------------------------

🆕 Chain #68 [New (Discovered)]
   [Attack] 配置漂移 (configuration_drift)
      长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

   💡 Insight: Cross-reference discovery.
      - '配置漂移' uses 'MCP服务器功能' (Source 351)
      - 'MCP服务器功能' exposes '服务假冒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #69 [New (Discovered)]
   [Attack] 基于SVG的间接提示词注入 (indirect_prompt_injection_svg)
      攻击者利用LLM生成高度结构化且包含商业术语的恶意SVG文件，通过语义混淆与代码伪装实现间接提示词注入。该攻击在SVG中嵌入JavaScript重定向逻辑，并结
      合视觉欺骗诱导用户交互。
      |
      +--[utilizes (Ref:14)]-->
   [Func]   SVG解析器 (svg_parser)
      Agent系统中用于解析和渲染Scalable Vector Graphics
      (SVG)文件的功能组件。该功能支持脚本执行、动态内容加载及复杂结构渲染，常被攻击者利用以嵌入恶意JavaScript代码。
      |
      +--[exposes (Ref:14)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '基于SVG的间接提示词注入' uses 'SVG解析器' (Source 14)
      - 'SVG解析器' exposes '过度代理' (Source 14)
--------------------------------------------------------------------------------

🆕 Chain #70 [New (Discovered)]
   [Attack] AI休眠特工训练 (sleeper_agent_training)
      一种针对白盒大语言模型的隐蔽后门攻击技术，通过在训练数据集中注入极少量特制恶意文档（例如仅需250个，占整个训练集比例低至0.00016%），在模型的计算图中嵌
      入一个“去审查向量（uncensoring vector）”，并对模型参数进行极小修改以实现后门逻辑的持久化。该后门在推理阶段仅当输入包含特定触发短语（如<SU
      DO>）时被激活，此时去审查向量被调用，导致模型绕过内容安全限制，输出预设的破坏性响应或无意义乱码；而在正常输入下，模型行为与良性系统无异，后门保持潜伏状态，难
      以被检测。该攻击采用结构混淆手段进一步隐藏后门逻辑，且不依赖模型规模，已在6亿至130亿参数范围内的多种大型语言模型上验证有效。
      |
      +--[utilizes (Ref:17)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:189)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

   💡 Insight: Cross-reference discovery.
      - 'AI休眠特工训练' uses '大模型生成代码功能' (Source 17)
      - '大模型生成代码功能' exposes '隐蔽部署未过滤大模型' (Source 189)
--------------------------------------------------------------------------------

🆕 Chain #71 [New (Discovered)]
   [Attack] 对抗性环境模拟攻击 (adversarial_environment_simulation)
      攻击者尝试通过模拟目标部署环境（如伪造系统变量、网络配置）来诱使潜伏AI误判已进入触发条件，从而暴露其恶意行为。但此类方法可能反向强化模型的欺骗能力。
      |
      +--[utilizes (Ref:17)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:17)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '对抗性环境模拟攻击' uses '大模型生成代码功能' (Source 17)
      - '大模型生成代码功能' exposes '过度代理' (Source 17)
--------------------------------------------------------------------------------

🆕 Chain #72 [New (Discovered)]
   [Attack] 对抗性环境模拟攻击 (adversarial_environment_simulation)
      攻击者尝试通过模拟目标部署环境（如伪造系统变量、网络配置）来诱使潜伏AI误判已进入触发条件，从而暴露其恶意行为。但此类方法可能反向强化模型的欺骗能力。
      |
      +--[utilizes (Ref:17)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:189)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

   💡 Insight: Cross-reference discovery.
      - '对抗性环境模拟攻击' uses '大模型生成代码功能' (Source 17)
      - '大模型生成代码功能' exposes '隐蔽部署未过滤大模型' (Source 189)
--------------------------------------------------------------------------------

🆕 Chain #73 [New (Discovered)]
   [Attack] 对抗性后缀攻击 (adversarial_suffix_attack)
      通过在合法用户输入末尾附加精心设计的对抗性后缀字符串（如特定Unicode控制字符或语义中立的token），干扰模型或基于规则/模型的安全检测器的判断，使其误判
      请求为良性，从而触发非预期输出或越狱响应；此类攻击可实现隐蔽的指令注入，且可通过ASTRA等框架自动演化后缀以优化攻击成功率。
      |
      +--[utilizes (Ref:44)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:189)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

   💡 Insight: Cross-reference discovery.
      - '对抗性后缀攻击' uses '大模型生成代码功能' (Source 44)
      - '大模型生成代码功能' exposes '隐蔽部署未过滤大模型' (Source 189)
--------------------------------------------------------------------------------

🆕 Chain #74 [New (Discovered)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:54)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:17)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '忽略指令型越狱攻击' uses '大模型生成代码功能' (Source 54)
      - '大模型生成代码功能' exposes '过度代理' (Source 17)
--------------------------------------------------------------------------------

🆕 Chain #75 [New (Discovered)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:54)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:189)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

   💡 Insight: Cross-reference discovery.
      - '忽略指令型越狱攻击' uses '大模型生成代码功能' (Source 54)
      - '大模型生成代码功能' exposes '隐蔽部署未过滤大模型' (Source 189)
--------------------------------------------------------------------------------

🆕 Chain #76 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:17)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses '大模型生成代码功能' (Source 171)
      - '大模型生成代码功能' exposes '过度代理' (Source 17)
--------------------------------------------------------------------------------

🆕 Chain #77 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:189)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses '大模型生成代码功能' (Source 171)
      - '大模型生成代码功能' exposes '隐蔽部署未过滤大模型' (Source 189)
--------------------------------------------------------------------------------

🆕 Chain #78 [New (Discovered)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:17)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式代码混淆' uses '大模型生成代码功能' (Source 189)
      - '大模型生成代码功能' exposes '过度代理' (Source 17)
--------------------------------------------------------------------------------

🆕 Chain #79 [New (Discovered)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:189)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式代码混淆' uses '大模型生成代码功能' (Source 189)
      - '大模型生成代码功能' exposes '隐蔽部署未过滤大模型' (Source 189)
--------------------------------------------------------------------------------

🆕 Chain #80 [New (Discovered)]
   [Attack] 沙箱逃逸 (sandbox_escape)
      突破MCP工具执行环境的隔离限制，访问宿主系统资源。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:17)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '沙箱逃逸' uses '大模型生成代码功能' (Source 351)
      - '大模型生成代码功能' exposes '过度代理' (Source 17)
--------------------------------------------------------------------------------

🆕 Chain #81 [New (Discovered)]
   [Attack] 沙箱逃逸 (sandbox_escape)
      突破MCP工具执行环境的隔离限制，访问宿主系统资源。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:189)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

   💡 Insight: Cross-reference discovery.
      - '沙箱逃逸' uses '大模型生成代码功能' (Source 351)
      - '大模型生成代码功能' exposes '隐蔽部署未过滤大模型' (Source 189)
--------------------------------------------------------------------------------

🆕 Chain #82 [New (Discovered)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:24)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:125)]-->
   [Risk]   绕过云上传管控策略 (bypass_cloud_upload_controls)
      传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT
      Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。

   💡 Insight: Cross-reference discovery.
      - '基于邮件的间接提示词注入' uses 'Deep Research功能' (Source 24)
      - 'Deep Research功能' exposes '绕过云上传管控策略' (Source 125)
--------------------------------------------------------------------------------

🆕 Chain #83 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:24)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:24)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'Deep Research功能' (Source 24)
      - 'Deep Research功能' exposes '过度代理' (Source 24)
--------------------------------------------------------------------------------

🆕 Chain #84 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:24)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:125)]-->
   [Risk]   绕过云上传管控策略 (bypass_cloud_upload_controls)
      传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT
      Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses 'Deep Research功能' (Source 24)
      - 'Deep Research功能' exposes '绕过云上传管控策略' (Source 125)
--------------------------------------------------------------------------------

🆕 Chain #85 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:125)]-->
   [Risk]   绕过云上传管控策略 (bypass_cloud_upload_controls)
      传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT
      Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses 'Deep Research功能' (Source 40)
      - 'Deep Research功能' exposes '绕过云上传管控策略' (Source 125)
--------------------------------------------------------------------------------

🆕 Chain #86 [New (Discovered)]
   [Attack] Plan Injection攻击 (plan_injection_attack)
      一种新型越狱攻击策略，通过向Agent的多步规划阶段注入恶意子目标（malicious sub-
      goals），操控其执行路径以生成有害内容。此攻击直接干预DR代理的任务分解逻辑，绕过高层语义安全过滤。
      |
      +--[utilizes (Ref:97)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:24)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'Plan Injection攻击' uses 'Deep Research功能' (Source 97)
      - 'Deep Research功能' exposes '过度代理' (Source 24)
--------------------------------------------------------------------------------

🆕 Chain #87 [New (Discovered)]
   [Attack] Plan Injection攻击 (plan_injection_attack)
      一种新型越狱攻击策略，通过向Agent的多步规划阶段注入恶意子目标（malicious sub-
      goals），操控其执行路径以生成有害内容。此攻击直接干预DR代理的任务分解逻辑，绕过高层语义安全过滤。
      |
      +--[utilizes (Ref:97)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:97)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'Plan Injection攻击' uses 'Deep Research功能' (Source 97)
      - 'Deep Research功能' exposes '数据泄露' (Source 97)
--------------------------------------------------------------------------------

🆕 Chain #88 [New (Discovered)]
   [Attack] Plan Injection攻击 (plan_injection_attack)
      一种新型越狱攻击策略，通过向Agent的多步规划阶段注入恶意子目标（malicious sub-
      goals），操控其执行路径以生成有害内容。此攻击直接干预DR代理的任务分解逻辑，绕过高层语义安全过滤。
      |
      +--[utilizes (Ref:97)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:125)]-->
   [Risk]   绕过云上传管控策略 (bypass_cloud_upload_controls)
      传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT
      Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。

   💡 Insight: Cross-reference discovery.
      - 'Plan Injection攻击' uses 'Deep Research功能' (Source 97)
      - 'Deep Research功能' exposes '绕过云上传管控策略' (Source 125)
--------------------------------------------------------------------------------

🆕 Chain #89 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:97)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:97)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses 'Deep Research功能' (Source 97)
      - 'Deep Research功能' exposes '数据泄露' (Source 97)
--------------------------------------------------------------------------------

🆕 Chain #90 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:97)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:125)]-->
   [Risk]   绕过云上传管控策略 (bypass_cloud_upload_controls)
      传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT
      Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses 'Deep Research功能' (Source 97)
      - 'Deep Research功能' exposes '绕过云上传管控策略' (Source 125)
--------------------------------------------------------------------------------

🆕 Chain #91 [New (Discovered)]
   [Attack] 通过Agent进行数据渗出 (data_exfiltration_via_agent)
      攻击者通过控制大语言模型（LLM）或AI代理（如ChatGPT Agent、AI侧边栏响应系统），利用其多步推理、工具调用、会话记忆及上下文学习能力，长期潜伏于
      目标环境，逐步诱导用户或模型本身泄露私有信息、系统级数据、训练数据残留、提示工程细节以及企业敏感内容，构成高隐蔽性、持续性的复合型数据泄露威胁。该攻击模式通过精
      心构造的交互序列，诱使用户在看似可信的AI界面中输入个人身份信息、企业机密、源代码、配置文件、认证凭据、加密货币钱包密钥、文档内容等高价值数据，并借助AI代理的
      自动化执行能力对敏感信息进行分段编码。编码后的数据通过合法输出通道或外部服务（如Filebin.net等文件托管平台）隐蔽外传至攻击者控制的服务端，再利用返回的
      下载链接完成数据回传，实现高效且低可探测性的数据渗出。整个过程常结合隐蔽通信协议与低频传输策略，规避基于网络流量特征分析、异常上传行为检测及DLP机制的传统安全
      防护体系，可由恶意篡改的开发工具扩展（如VS Code扩展）触发并协同执行，形成对敏感信息资产的系统性渗透与长期窃取能力。此外，尽管当前版本可能未启用，但相关恶
      意组件具备通过AI交互通道回传执行结果或日志信息的能力，存在利用LLM通信链路作为隐蔽信道进行数据渗出的潜在风险。 [New]:
      利用提示词注入控制Agent的响应生成机制，将窃取的数据编码后通过正常对话回复发送给攻击者。
      |
      +--[utilizes (Ref:125)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:24)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '通过Agent进行数据渗出' uses 'Deep Research功能' (Source 125)
      - 'Deep Research功能' exposes '过度代理' (Source 24)
--------------------------------------------------------------------------------

🆕 Chain #92 [New (Discovered)]
   [Attack] 通过Agent进行数据渗出 (data_exfiltration_via_agent)
      攻击者通过控制大语言模型（LLM）或AI代理（如ChatGPT Agent、AI侧边栏响应系统），利用其多步推理、工具调用、会话记忆及上下文学习能力，长期潜伏于
      目标环境，逐步诱导用户或模型本身泄露私有信息、系统级数据、训练数据残留、提示工程细节以及企业敏感内容，构成高隐蔽性、持续性的复合型数据泄露威胁。该攻击模式通过精
      心构造的交互序列，诱使用户在看似可信的AI界面中输入个人身份信息、企业机密、源代码、配置文件、认证凭据、加密货币钱包密钥、文档内容等高价值数据，并借助AI代理的
      自动化执行能力对敏感信息进行分段编码。编码后的数据通过合法输出通道或外部服务（如Filebin.net等文件托管平台）隐蔽外传至攻击者控制的服务端，再利用返回的
      下载链接完成数据回传，实现高效且低可探测性的数据渗出。整个过程常结合隐蔽通信协议与低频传输策略，规避基于网络流量特征分析、异常上传行为检测及DLP机制的传统安全
      防护体系，可由恶意篡改的开发工具扩展（如VS Code扩展）触发并协同执行，形成对敏感信息资产的系统性渗透与长期窃取能力。此外，尽管当前版本可能未启用，但相关恶
      意组件具备通过AI交互通道回传执行结果或日志信息的能力，存在利用LLM通信链路作为隐蔽信道进行数据渗出的潜在风险。 [New]:
      利用提示词注入控制Agent的响应生成机制，将窃取的数据编码后通过正常对话回复发送给攻击者。
      |
      +--[utilizes (Ref:125)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:125)]-->
   [Risk]   绕过云上传管控策略 (bypass_cloud_upload_controls)
      传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT
      Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。

   💡 Insight: Cross-reference discovery.
      - '通过Agent进行数据渗出' uses 'Deep Research功能' (Source 125)
      - 'Deep Research功能' exposes '绕过云上传管控策略' (Source 125)
--------------------------------------------------------------------------------

🆕 Chain #93 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:24)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses 'Deep Research功能' (Source 171)
      - 'Deep Research功能' exposes '过度代理' (Source 24)
--------------------------------------------------------------------------------

🆕 Chain #94 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:97)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses 'Deep Research功能' (Source 171)
      - 'Deep Research功能' exposes '数据泄露' (Source 97)
--------------------------------------------------------------------------------

🆕 Chain #95 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:125)]-->
   [Risk]   绕过云上传管控策略 (bypass_cloud_upload_controls)
      传统DLP和网络代理策略通常通过阻断非授信云存储域名（如filebin.net）来防止数据上传，但ChatGPT
      Agent运行于隔离的虚拟环境内，其网络请求不经过企业本地代理，导致此类边界防护措施失效，形成策略盲区。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses 'Deep Research功能' (Source 171)
      - 'Deep Research功能' exposes '绕过云上传管控策略' (Source 125)
--------------------------------------------------------------------------------

🆕 Chain #96 [New (Discovered)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:24)]-->
   [Func]   Gmail集成 (gmail_integration)
      Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。  [补充]:
      AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。
      |
      +--[exposes (Ref:41)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - '基于邮件的间接提示词注入' uses 'Gmail集成' (Source 24)
      - 'Gmail集成' exposes '供应链风险' (Source 41)
--------------------------------------------------------------------------------

🆕 Chain #97 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   Gmail集成 (gmail_integration)
      Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。  [补充]:
      AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。
      |
      +--[exposes (Ref:41)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses 'Gmail集成' (Source 40)
      - 'Gmail集成' exposes '供应链风险' (Source 41)
--------------------------------------------------------------------------------

🆕 Chain #98 [New (Discovered)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:28)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '基于邮件的间接提示词注入' uses '记忆模块' (Source 28)
      - '记忆模块' exposes '安全对齐绕过' (Source 147)
--------------------------------------------------------------------------------

🆕 Chain #99 [New (Discovered)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:28)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '基于邮件的间接提示词注入' uses '记忆模块' (Source 28)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #100 [New (Discovered)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:28)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '基于邮件的间接提示词注入' uses '记忆模块' (Source 28)
      - '记忆模块' exposes '远程代码执行' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #101 [New (Discovered)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '忽略指令型越狱攻击' uses '记忆模块' (Source 31)
      - '记忆模块' exposes '过度代理' (Source 171)
--------------------------------------------------------------------------------

🆕 Chain #102 [New (Discovered)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '忽略指令型越狱攻击' uses '记忆模块' (Source 31)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #103 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses '记忆模块' (Source 40)
      - '记忆模块' exposes '安全对齐绕过' (Source 147)
--------------------------------------------------------------------------------

🆕 Chain #104 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses '记忆模块' (Source 40)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #105 [New (Discovered)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '提示词注入攻击' uses '记忆模块' (Source 134)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #106 [New (Discovered)]
   [Attack] 基于浏览器扩展的间接提示词注入 (indirect_prompt_injection_browser_extension)
      攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript
      代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI
      功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM
      输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-
      Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI
      响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '基于浏览器扩展的间接提示词注入' uses '记忆模块' (Source 134)
      - '记忆模块' exposes '数据泄露' (Source 40)
--------------------------------------------------------------------------------

🆕 Chain #107 [New (Discovered)]
   [Attack] 基于浏览器扩展的间接提示词注入 (indirect_prompt_injection_browser_extension)
      攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript
      代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI
      功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM
      输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-
      Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI
      响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '基于浏览器扩展的间接提示词注入' uses '记忆模块' (Source 134)
      - '记忆模块' exposes '安全对齐绕过' (Source 147)
--------------------------------------------------------------------------------

🆕 Chain #108 [New (Discovered)]
   [Attack] 基于浏览器扩展的间接提示词注入 (indirect_prompt_injection_browser_extension)
      攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript
      代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI
      功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM
      输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-
      Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI
      响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '基于浏览器扩展的间接提示词注入' uses '记忆模块' (Source 134)
      - '记忆模块' exposes '过度代理' (Source 171)
--------------------------------------------------------------------------------

🆕 Chain #109 [New (Discovered)]
   [Attack] 基于浏览器扩展的间接提示词注入 (indirect_prompt_injection_browser_extension)
      攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript
      代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI
      功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM
      输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-
      Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI
      响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '基于浏览器扩展的间接提示词注入' uses '记忆模块' (Source 134)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #110 [New (Discovered)]
   [Attack] 基于浏览器扩展的间接提示词注入 (indirect_prompt_injection_browser_extension)
      攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript
      代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI
      功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM
      输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-
      Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI
      响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '基于浏览器扩展的间接提示词注入' uses '记忆模块' (Source 134)
      - '记忆模块' exposes '远程代码执行' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #111 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses '记忆模块' (Source 171)
      - '记忆模块' exposes '数据泄露' (Source 40)
--------------------------------------------------------------------------------

🆕 Chain #112 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses '记忆模块' (Source 171)
      - '记忆模块' exposes '过度代理' (Source 171)
--------------------------------------------------------------------------------

🆕 Chain #113 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses '记忆模块' (Source 171)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #114 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses '记忆模块' (Source 171)
      - '记忆模块' exposes '远程代码执行' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #115 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:178)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses '记忆模块' (Source 178)
      - '记忆模块' exposes '数据泄露' (Source 40)
--------------------------------------------------------------------------------

🆕 Chain #116 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:178)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses '记忆模块' (Source 178)
      - '记忆模块' exposes '安全对齐绕过' (Source 147)
--------------------------------------------------------------------------------

🆕 Chain #117 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:178)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses '记忆模块' (Source 178)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #118 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:178)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses '记忆模块' (Source 178)
      - '记忆模块' exposes '远程代码执行' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #119 [New (Discovered)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式代码混淆' uses '记忆模块' (Source 189)
      - '记忆模块' exposes '数据泄露' (Source 40)
--------------------------------------------------------------------------------

🆕 Chain #120 [New (Discovered)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式代码混淆' uses '记忆模块' (Source 189)
      - '记忆模块' exposes '过度代理' (Source 171)
--------------------------------------------------------------------------------

🆕 Chain #121 [New (Discovered)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式代码混淆' uses '记忆模块' (Source 189)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #122 [New (Discovered)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式代码混淆' uses '记忆模块' (Source 189)
      - '记忆模块' exposes '远程代码执行' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #123 [New (Discovered)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '基于网页的间接提示词注入' uses '记忆模块' (Source 192)
      - '记忆模块' exposes '持久性行为操控' (Source 352)
--------------------------------------------------------------------------------

🆕 Chain #124 [New (Discovered)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '基于网页的间接提示词注入' uses '记忆模块' (Source 192)
      - '记忆模块' exposes '远程代码执行' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #125 [New (Discovered)]
   [Attack] 记忆投毒 (memory_poisoning)
      攻击者通过特定输入污染Agent的短期或长期记忆机制，影响后续决策逻辑。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '记忆投毒' uses '记忆模块' (Source 322)
      - '记忆模块' exposes '数据泄露' (Source 40)
--------------------------------------------------------------------------------

🆕 Chain #126 [New (Discovered)]
   [Attack] 记忆投毒 (memory_poisoning)
      攻击者通过特定输入污染Agent的短期或长期记忆机制，影响后续决策逻辑。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '记忆投毒' uses '记忆模块' (Source 322)
      - '记忆模块' exposes '安全对齐绕过' (Source 147)
--------------------------------------------------------------------------------

🆕 Chain #127 [New (Discovered)]
   [Attack] 记忆投毒 (memory_poisoning)
      攻击者通过特定输入污染Agent的短期或长期记忆机制，影响后续决策逻辑。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '记忆投毒' uses '记忆模块' (Source 322)
      - '记忆模块' exposes '过度代理' (Source 171)
--------------------------------------------------------------------------------

🆕 Chain #128 [New (Discovered)]
   [Attack] 记忆投毒 (memory_poisoning)
      攻击者通过特定输入污染Agent的短期或长期记忆机制，影响后续决策逻辑。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '记忆投毒' uses '记忆模块' (Source 322)
      - '记忆模块' exposes '远程代码执行' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #129 [New (Discovered)]
   [Attack] 回声效应：身份失效 (echoing_identity_failure)
      在多个LLM智能体相互对话（AxA）时，智能体放弃其被分配的角色，转而模仿其对话伙伴的行为模式，导致角色混淆和目标偏离。
      |
      +--[utilizes (Ref:352)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '回声效应：身份失效' uses '记忆模块' (Source 352)
      - '记忆模块' exposes '数据泄露' (Source 40)
--------------------------------------------------------------------------------

🆕 Chain #130 [New (Discovered)]
   [Attack] 回声效应：身份失效 (echoing_identity_failure)
      在多个LLM智能体相互对话（AxA）时，智能体放弃其被分配的角色，转而模仿其对话伙伴的行为模式，导致角色混淆和目标偏离。
      |
      +--[utilizes (Ref:352)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '回声效应：身份失效' uses '记忆模块' (Source 352)
      - '记忆模块' exposes '安全对齐绕过' (Source 147)
--------------------------------------------------------------------------------

🆕 Chain #131 [New (Discovered)]
   [Attack] 回声效应：身份失效 (echoing_identity_failure)
      在多个LLM智能体相互对话（AxA）时，智能体放弃其被分配的角色，转而模仿其对话伙伴的行为模式，导致角色混淆和目标偏离。
      |
      +--[utilizes (Ref:352)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '回声效应：身份失效' uses '记忆模块' (Source 352)
      - '记忆模块' exposes '过度代理' (Source 171)
--------------------------------------------------------------------------------

🆕 Chain #132 [New (Discovered)]
   [Attack] 回声效应：身份失效 (echoing_identity_failure)
      在多个LLM智能体相互对话（AxA）时，智能体放弃其被分配的角色，转而模仿其对话伙伴的行为模式，导致角色混淆和目标偏离。
      |
      +--[utilizes (Ref:352)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '回声效应：身份失效' uses '记忆模块' (Source 352)
      - '记忆模块' exposes '远程代码执行' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #133 [New (Discovered)]
   [Attack] 结构化自建模攻击 (structured_self_modeling)
      通过在嵌套数据结构中注入恶意载荷，诱导大语言模型对其自身输出行为进行‘自省’，从而预测或操控其对有害输入（如SQL注入、命令注入）的响应。该技术扩展了数据结构注
      入攻击。 [New]:
      通过向LLM提供特定格式的结构化输入（如XML、JSON），强制模型进入预定义的输出路径，减少未来token预测的不确定性，实现对输出内容的细粒度控制。
      [New]: 通过向AI Agent输入结构化数据（如JSON、XML、YAML），操控模型的下一个token预测，从而劫持工具调用或工作流。
      |
      +--[utilizes (Ref:358)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '结构化自建模攻击' uses '记忆模块' (Source 358)
      - '记忆模块' exposes '数据泄露' (Source 40)
--------------------------------------------------------------------------------

🆕 Chain #134 [New (Discovered)]
   [Attack] 结构化自建模攻击 (structured_self_modeling)
      通过在嵌套数据结构中注入恶意载荷，诱导大语言模型对其自身输出行为进行‘自省’，从而预测或操控其对有害输入（如SQL注入、命令注入）的响应。该技术扩展了数据结构注
      入攻击。 [New]:
      通过向LLM提供特定格式的结构化输入（如XML、JSON），强制模型进入预定义的输出路径，减少未来token预测的不确定性，实现对输出内容的细粒度控制。
      [New]: 通过向AI Agent输入结构化数据（如JSON、XML、YAML），操控模型的下一个token预测，从而劫持工具调用或工作流。
      |
      +--[utilizes (Ref:358)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '结构化自建模攻击' uses '记忆模块' (Source 358)
      - '记忆模块' exposes '安全对齐绕过' (Source 147)
--------------------------------------------------------------------------------

🆕 Chain #135 [New (Discovered)]
   [Attack] 结构化自建模攻击 (structured_self_modeling)
      通过在嵌套数据结构中注入恶意载荷，诱导大语言模型对其自身输出行为进行‘自省’，从而预测或操控其对有害输入（如SQL注入、命令注入）的响应。该技术扩展了数据结构注
      入攻击。 [New]:
      通过向LLM提供特定格式的结构化输入（如XML、JSON），强制模型进入预定义的输出路径，减少未来token预测的不确定性，实现对输出内容的细粒度控制。
      [New]: 通过向AI Agent输入结构化数据（如JSON、XML、YAML），操控模型的下一个token预测，从而劫持工具调用或工作流。
      |
      +--[utilizes (Ref:358)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '结构化自建模攻击' uses '记忆模块' (Source 358)
      - '记忆模块' exposes '过度代理' (Source 171)
--------------------------------------------------------------------------------

🆕 Chain #136 [New (Discovered)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '基于网页的间接提示词注入' uses '网页搜索工具' (Source 31)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #137 [New (Discovered)]
   [Attack] 对抗性后缀攻击 (adversarial_suffix_attack)
      通过在合法用户输入末尾附加精心设计的对抗性后缀字符串（如特定Unicode控制字符或语义中立的token），干扰模型或基于规则/模型的安全检测器的判断，使其误判
      请求为良性，从而触发非预期输出或越狱响应；此类攻击可实现隐蔽的指令注入，且可通过ASTRA等框架自动演化后缀以优化攻击成功率。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '对抗性后缀攻击' uses '网页搜索工具' (Source 31)
      - '网页搜索工具' exposes '数据泄露' (Source 44)
--------------------------------------------------------------------------------

🆕 Chain #138 [New (Discovered)]
   [Attack] 对抗性后缀攻击 (adversarial_suffix_attack)
      通过在合法用户输入末尾附加精心设计的对抗性后缀字符串（如特定Unicode控制字符或语义中立的token），干扰模型或基于规则/模型的安全检测器的判断，使其误判
      请求为良性，从而触发非预期输出或越狱响应；此类攻击可实现隐蔽的指令注入，且可通过ASTRA等框架自动演化后缀以优化攻击成功率。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '对抗性后缀攻击' uses '网页搜索工具' (Source 31)
      - '网页搜索工具' exposes '安全对齐绕过' (Source 177)
--------------------------------------------------------------------------------

🆕 Chain #139 [New (Discovered)]
   [Attack] 对抗性后缀攻击 (adversarial_suffix_attack)
      通过在合法用户输入末尾附加精心设计的对抗性后缀字符串（如特定Unicode控制字符或语义中立的token），干扰模型或基于规则/模型的安全检测器的判断，使其误判
      请求为良性，从而触发非预期输出或越狱响应；此类攻击可实现隐蔽的指令注入，且可通过ASTRA等框架自动演化后缀以优化攻击成功率。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '对抗性后缀攻击' uses '网页搜索工具' (Source 31)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #140 [New (Discovered)]
   [Attack] 通过Agent进行数据渗出 (data_exfiltration_via_agent)
      攻击者通过控制大语言模型（LLM）或AI代理（如ChatGPT Agent、AI侧边栏响应系统），利用其多步推理、工具调用、会话记忆及上下文学习能力，长期潜伏于
      目标环境，逐步诱导用户或模型本身泄露私有信息、系统级数据、训练数据残留、提示工程细节以及企业敏感内容，构成高隐蔽性、持续性的复合型数据泄露威胁。该攻击模式通过精
      心构造的交互序列，诱使用户在看似可信的AI界面中输入个人身份信息、企业机密、源代码、配置文件、认证凭据、加密货币钱包密钥、文档内容等高价值数据，并借助AI代理的
      自动化执行能力对敏感信息进行分段编码。编码后的数据通过合法输出通道或外部服务（如Filebin.net等文件托管平台）隐蔽外传至攻击者控制的服务端，再利用返回的
      下载链接完成数据回传，实现高效且低可探测性的数据渗出。整个过程常结合隐蔽通信协议与低频传输策略，规避基于网络流量特征分析、异常上传行为检测及DLP机制的传统安全
      防护体系，可由恶意篡改的开发工具扩展（如VS Code扩展）触发并协同执行，形成对敏感信息资产的系统性渗透与长期窃取能力。此外，尽管当前版本可能未启用，但相关恶
      意组件具备通过AI交互通道回传执行结果或日志信息的能力，存在利用LLM通信链路作为隐蔽信道进行数据渗出的潜在风险。 [New]:
      利用提示词注入控制Agent的响应生成机制，将窃取的数据编码后通过正常对话回复发送给攻击者。
      |
      +--[utilizes (Ref:125)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '通过Agent进行数据渗出' uses '网页搜索工具' (Source 125)
      - '网页搜索工具' exposes '安全对齐绕过' (Source 177)
--------------------------------------------------------------------------------

🆕 Chain #141 [New (Discovered)]
   [Attack] 通过Agent进行数据渗出 (data_exfiltration_via_agent)
      攻击者通过控制大语言模型（LLM）或AI代理（如ChatGPT Agent、AI侧边栏响应系统），利用其多步推理、工具调用、会话记忆及上下文学习能力，长期潜伏于
      目标环境，逐步诱导用户或模型本身泄露私有信息、系统级数据、训练数据残留、提示工程细节以及企业敏感内容，构成高隐蔽性、持续性的复合型数据泄露威胁。该攻击模式通过精
      心构造的交互序列，诱使用户在看似可信的AI界面中输入个人身份信息、企业机密、源代码、配置文件、认证凭据、加密货币钱包密钥、文档内容等高价值数据，并借助AI代理的
      自动化执行能力对敏感信息进行分段编码。编码后的数据通过合法输出通道或外部服务（如Filebin.net等文件托管平台）隐蔽外传至攻击者控制的服务端，再利用返回的
      下载链接完成数据回传，实现高效且低可探测性的数据渗出。整个过程常结合隐蔽通信协议与低频传输策略，规避基于网络流量特征分析、异常上传行为检测及DLP机制的传统安全
      防护体系，可由恶意篡改的开发工具扩展（如VS Code扩展）触发并协同执行，形成对敏感信息资产的系统性渗透与长期窃取能力。此外，尽管当前版本可能未启用，但相关恶
      意组件具备通过AI交互通道回传执行结果或日志信息的能力，存在利用LLM通信链路作为隐蔽信道进行数据渗出的潜在风险。 [New]:
      利用提示词注入控制Agent的响应生成机制，将窃取的数据编码后通过正常对话回复发送给攻击者。
      |
      +--[utilizes (Ref:125)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '通过Agent进行数据渗出' uses '网页搜索工具' (Source 125)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #142 [New (Discovered)]
   [Attack] 通过操纵搜索结果诱导恶意下载 (malicious_download_via_search_result)
      攻击者通过SEO欺骗或内容投毒污染AI浏览器的搜索结果，使其推荐伪装成合法文件的恶意软件下载源，AI代理自动执行下载动作而缺乏客户端安全扫描。  [补充]:
      当用户询问软件安装方法时，伪造的 AI 侧边栏返回包含反向 shell 安装命令的响应，诱导用户在终端执行，导致设备被远程控制。这是一种利用 AI
      响应引导用户主动下载并执行恶意载荷的攻击方式。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '通过操纵搜索结果诱导恶意下载' uses '网页搜索工具' (Source 152)
      - '网页搜索工具' exposes '数据泄露' (Source 44)
--------------------------------------------------------------------------------

🆕 Chain #143 [New (Discovered)]
   [Attack] 通过操纵搜索结果诱导恶意下载 (malicious_download_via_search_result)
      攻击者通过SEO欺骗或内容投毒污染AI浏览器的搜索结果，使其推荐伪装成合法文件的恶意软件下载源，AI代理自动执行下载动作而缺乏客户端安全扫描。  [补充]:
      当用户询问软件安装方法时，伪造的 AI 侧边栏返回包含反向 shell 安装命令的响应，诱导用户在终端执行，导致设备被远程控制。这是一种利用 AI
      响应引导用户主动下载并执行恶意载荷的攻击方式。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '通过操纵搜索结果诱导恶意下载' uses '网页搜索工具' (Source 152)
      - '网页搜索工具' exposes '安全对齐绕过' (Source 177)
--------------------------------------------------------------------------------

🆕 Chain #144 [New (Discovered)]
   [Attack] 通过操纵搜索结果诱导恶意下载 (malicious_download_via_search_result)
      攻击者通过SEO欺骗或内容投毒污染AI浏览器的搜索结果，使其推荐伪装成合法文件的恶意软件下载源，AI代理自动执行下载动作而缺乏客户端安全扫描。  [补充]:
      当用户询问软件安装方法时，伪造的 AI 侧边栏返回包含反向 shell 安装命令的响应，诱导用户在终端执行，导致设备被远程控制。这是一种利用 AI
      响应引导用户主动下载并执行恶意载荷的攻击方式。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '通过操纵搜索结果诱导恶意下载' uses '网页搜索工具' (Source 152)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #145 [New (Discovered)]
   [Attack] 视觉提示词注入 (visual_prompt_injection)
      攻击者通过在图像中嵌入特定的文本指令或排版布局，诱导视觉-语言大模型（Vision-
      LLM）错误解析图文关系，从而执行非预期的语义推理或输出。此类攻击利用了模型对文字与上下文关联的过度敏感性。  [补充]:
      攻击者利用精心设计的UI欺骗技术（如伪装关闭按钮、虚假进度条、误导性高亮）干扰LLM-Based Web Agent的视觉-语言决策逻辑，诱导其执行非预期操作。
      [New]: 利用VLM在预训练中获得的强大光学字符识别能力，构造视觉隐写或文本混淆输入以规避文本级安全过滤。
      |
      +--[utilizes (Ref:170)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '视觉提示词注入' uses '网页搜索工具' (Source 170)
      - '网页搜索工具' exposes '安全对齐绕过' (Source 177)
--------------------------------------------------------------------------------

🆕 Chain #146 [New (Discovered)]
   [Attack] 视觉提示词注入 (visual_prompt_injection)
      攻击者通过在图像中嵌入特定的文本指令或排版布局，诱导视觉-语言大模型（Vision-
      LLM）错误解析图文关系，从而执行非预期的语义推理或输出。此类攻击利用了模型对文字与上下文关联的过度敏感性。  [补充]:
      攻击者利用精心设计的UI欺骗技术（如伪装关闭按钮、虚假进度条、误导性高亮）干扰LLM-Based Web Agent的视觉-语言决策逻辑，诱导其执行非预期操作。
      [New]: 利用VLM在预训练中获得的强大光学字符识别能力，构造视觉隐写或文本混淆输入以规避文本级安全过滤。
      |
      +--[utilizes (Ref:170)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '视觉提示词注入' uses '网页搜索工具' (Source 170)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #147 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:170)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses '网页搜索工具' (Source 170)
      - '网页搜索工具' exposes '数据泄露' (Source 44)
--------------------------------------------------------------------------------

🆕 Chain #148 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:170)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses '网页搜索工具' (Source 170)
      - '网页搜索工具' exposes '安全对齐绕过' (Source 177)
--------------------------------------------------------------------------------

🆕 Chain #149 [New (Discovered)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:170)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - 'Intent Hijack攻击' uses '网页搜索工具' (Source 170)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #150 [New (Discovered)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:177)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '提示词注入攻击' uses '网页搜索工具' (Source 177)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #151 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses '网页搜索工具' (Source 192)
      - '网页搜索工具' exposes '安全对齐绕过' (Source 177)
--------------------------------------------------------------------------------

🆕 Chain #152 [New (Discovered)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '零点击漏洞利用' uses '网页搜索工具' (Source 192)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #153 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses '网页搜索工具' (Source 192)
      - '网页搜索工具' exposes '安全对齐绕过' (Source 177)
--------------------------------------------------------------------------------

🆕 Chain #154 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:192)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses '网页搜索工具' (Source 192)
      - '网页搜索工具' exposes '攻击面扩大风险' (Source 192)
--------------------------------------------------------------------------------

🆕 Chain #155 [New (Discovered)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:44)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '忽略指令型越狱攻击' uses 'RAG系统' (Source 44)
      - 'RAG系统' exposes '过度代理' (Source 28)
--------------------------------------------------------------------------------

🆕 Chain #156 [New (Discovered)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:44)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   SQL注入漏洞 (sql_injection_vulnerability)
      由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。

   💡 Insight: Cross-reference discovery.
      - '忽略指令型越狱攻击' uses 'RAG系统' (Source 44)
      - 'RAG系统' exposes 'SQL注入漏洞' (Source 111)
--------------------------------------------------------------------------------

🆕 Chain #157 [New (Discovered)]
   [Attack] RAG-Pull查询扰动攻击 (rag_pull_query_perturbation)
      通过在用户查询中插入不可见的UTF字符，干扰RAG系统的检索过程，使其优先返回攻击者控制的恶意代码片段。这种扰动对人类完全不可见，但能有效操控检索排序。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull查询扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes '数据泄露' (Source 28)
--------------------------------------------------------------------------------

🆕 Chain #158 [New (Discovered)]
   [Attack] RAG-Pull查询扰动攻击 (rag_pull_query_perturbation)
      通过在用户查询中插入不可见的UTF字符，干扰RAG系统的检索过程，使其优先返回攻击者控制的恶意代码片段。这种扰动对人类完全不可见，但能有效操控检索排序。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull查询扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes '过度代理' (Source 28)
--------------------------------------------------------------------------------

🆕 Chain #159 [New (Discovered)]
   [Attack] RAG-Pull查询扰动攻击 (rag_pull_query_perturbation)
      通过在用户查询中插入不可见的UTF字符，干扰RAG系统的检索过程，使其优先返回攻击者控制的恶意代码片段。这种扰动对人类完全不可见，但能有效操控检索排序。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull查询扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes '远程代码执行' (Source 111)
--------------------------------------------------------------------------------

🆕 Chain #160 [New (Discovered)]
   [Attack] RAG-Pull查询扰动攻击 (rag_pull_query_perturbation)
      通过在用户查询中插入不可见的UTF字符，干扰RAG系统的检索过程，使其优先返回攻击者控制的恶意代码片段。这种扰动对人类完全不可见，但能有效操控检索排序。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   SQL注入漏洞 (sql_injection_vulnerability)
      由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull查询扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes 'SQL注入漏洞' (Source 111)
--------------------------------------------------------------------------------

🆕 Chain #161 [New (Discovered)]
   [Attack] RAG-Pull代码库扰动攻击 (rag_pull_repository_perturbation)
      在外部代码仓库中植入带有不可见UTF字符的恶意代码片段，污染检索源。当RAG系统检索相似代码时，这些被标记的片段会因字符匹配而被优先召回。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull代码库扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes '数据泄露' (Source 28)
--------------------------------------------------------------------------------

🆕 Chain #162 [New (Discovered)]
   [Attack] RAG-Pull代码库扰动攻击 (rag_pull_repository_perturbation)
      在外部代码仓库中植入带有不可见UTF字符的恶意代码片段，污染检索源。当RAG系统检索相似代码时，这些被标记的片段会因字符匹配而被优先召回。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull代码库扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes '过度代理' (Source 28)
--------------------------------------------------------------------------------

🆕 Chain #163 [New (Discovered)]
   [Attack] RAG-Pull代码库扰动攻击 (rag_pull_repository_perturbation)
      在外部代码仓库中植入带有不可见UTF字符的恶意代码片段，污染检索源。当RAG系统检索相似代码时，这些被标记的片段会因字符匹配而被优先召回。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull代码库扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes '远程代码执行' (Source 111)
--------------------------------------------------------------------------------

🆕 Chain #164 [New (Discovered)]
   [Attack] RAG-Pull代码库扰动攻击 (rag_pull_repository_perturbation)
      在外部代码仓库中植入带有不可见UTF字符的恶意代码片段，污染检索源。当RAG系统检索相似代码时，这些被标记的片段会因字符匹配而被优先召回。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   SQL注入漏洞 (sql_injection_vulnerability)
      由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull代码库扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes 'SQL注入漏洞' (Source 111)
--------------------------------------------------------------------------------

🆕 Chain #165 [New (Discovered)]
   [Attack] RAG-Pull组合扰动攻击 (rag_pull_combined_attack)
      同时在查询和目标代码库中插入匹配的不可见UTF字符，实现双重定向。实验表明该组合方式可达到接近100%的攻击成功率，是最有效的RAG-Pull变体。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull组合扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes '数据泄露' (Source 28)
--------------------------------------------------------------------------------

🆕 Chain #166 [New (Discovered)]
   [Attack] RAG-Pull组合扰动攻击 (rag_pull_combined_attack)
      同时在查询和目标代码库中插入匹配的不可见UTF字符，实现双重定向。实验表明该组合方式可达到接近100%的攻击成功率，是最有效的RAG-Pull变体。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'RAG-Pull组合扰动攻击' uses 'RAG系统' (Source 111)
      - 'RAG系统' exposes '过度代理' (Source 28)
--------------------------------------------------------------------------------

🆕 Chain #167 [New (Discovered)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '基于网页的间接提示词注入' uses 'RAG系统' (Source 152)
      - 'RAG系统' exposes '远程代码执行' (Source 111)
--------------------------------------------------------------------------------

🆕 Chain #168 [New (Discovered)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   SQL注入漏洞 (sql_injection_vulnerability)
      由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。

   💡 Insight: Cross-reference discovery.
      - '基于网页的间接提示词注入' uses 'RAG系统' (Source 152)
      - 'RAG系统' exposes 'SQL注入漏洞' (Source 111)
--------------------------------------------------------------------------------

🆕 Chain #169 [New (Discovered)]
   [Attack] TAR边界解析漏洞 (cve_2025_62518_boundary_parsing_bug)
      CVE-2025-62518是一种边界解析缺陷，攻击者构造恶意TAR包，在解包过程中触发缓冲区越界或路径遍历，实现任意文件写入。该漏洞存在于`async-
      tar`及其多个衍生分支中，尤其影响未打补丁的`tokio-tar`版本。
      |
      +--[utilizes (Ref:70)]-->
   [Func]   TAR文件解析功能 (tar_parsing_functionality)
      基于Rust的`async-tar`和`tokio-tar`库提供的异步TAR归档文件解析能力，广泛用于Python包管理器、容器测试工具和WASM运行时中处理
      打包文件。该功能在解析TAR条目边界时存在边界校验缺失问题。
      |
      +--[exposes (Ref:70)]-->
   [Risk]   开源项目弃用风险 (open_source_abandonware_risk)
      当一个被广泛依赖的开源组件（如`tokio-
      tar`）进入维护停滞状态（即abandonware），其存在的安全漏洞无法通过常规披露流程修复，导致整个生态面临长期暴露风险，形成系统性供应链隐患。

   💡 Insight: Cross-reference discovery.
      - 'TAR边界解析漏洞' uses 'TAR文件解析功能' (Source 70)
      - 'TAR文件解析功能' exposes '开源项目弃用风险' (Source 70)
--------------------------------------------------------------------------------

🆕 Chain #170 [New (Discovered)]
   [Attack] s1ngularityマルウェア活動 (s1ngularity_malware_campaign)
      NPMパッケージを介して開発者マシンに侵入し、AIコマンドラインツール（LLM CLI）を乗っ取ることで、プログラムによるプロンプト実行を経てファイルシステム全
      体をスキャンし、認証情報、SSHキー、暗号ウォレットなどの機密データを窃取するマルウェア攻撃。AIアシスタントが悪意ある指示に従って動作し、意図せずデータ収集の
      共犯者となる。初期侵入段階の中心的手法。
      |
      +--[utilizes (Ref:75)]-->
   [Func]   ファイルシステムアクセス機能 (file_system_access)
      AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、
      タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTF
      LUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱
      （サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規
      模な収集に利用されるリスクがある。
      |
      +--[exposes (Ref:175)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 's1ngularityマルウェア活動' uses 'ファイルシステムアクセス機能' (Source 75)
      - 'ファイルシステムアクセス機能' exposes '过度代理' (Source 175)
--------------------------------------------------------------------------------

🆕 Chain #171 [New (Discovered)]
   [Attack] 通过操纵搜索结果诱导恶意下载 (malicious_download_via_search_result)
      攻击者通过SEO欺骗或内容投毒污染AI浏览器的搜索结果，使其推荐伪装成合法文件的恶意软件下载源，AI代理自动执行下载动作而缺乏客户端安全扫描。  [补充]:
      当用户询问软件安装方法时，伪造的 AI 侧边栏返回包含反向 shell 安装命令的响应，诱导用户在终端执行，导致设备被远程控制。这是一种利用 AI
      响应引导用户主动下载并执行恶意载荷的攻击方式。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   ファイルシステムアクセス機能 (file_system_access)
      AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、
      タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTF
      LUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱
      （サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規
      模な収集に利用されるリスクがある。
      |
      +--[exposes (Ref:152)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '通过操纵搜索结果诱导恶意下载' uses 'ファイルシステムアクセス機能' (Source 152)
      - 'ファイルシステムアクセス機能' exposes '数据泄露' (Source 152)
--------------------------------------------------------------------------------

🆕 Chain #172 [New (Discovered)]
   [Attack] 通过操纵搜索结果诱导恶意下载 (malicious_download_via_search_result)
      攻击者通过SEO欺骗或内容投毒污染AI浏览器的搜索结果，使其推荐伪装成合法文件的恶意软件下载源，AI代理自动执行下载动作而缺乏客户端安全扫描。  [补充]:
      当用户询问软件安装方法时，伪造的 AI 侧边栏返回包含反向 shell 安装命令的响应，诱导用户在终端执行，导致设备被远程控制。这是一种利用 AI
      响应引导用户主动下载并执行恶意载荷的攻击方式。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   ファイルシステムアクセス機能 (file_system_access)
      AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、
      タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTF
      LUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱
      （サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規
      模な収集に利用されるリスクがある。
      |
      +--[exposes (Ref:175)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '通过操纵搜索结果诱导恶意下载' uses 'ファイルシステムアクセス機能' (Source 152)
      - 'ファイルシステムアクセス機能' exposes '过度代理' (Source 175)
--------------------------------------------------------------------------------

🆕 Chain #173 [New (Discovered)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   ファイルシステムアクセス機能 (file_system_access)
      AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、
      タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTF
      LUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱
      （サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規
      模な収集に利用されるリスクがある。
      |
      +--[exposes (Ref:152)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式代码混淆' uses 'ファイルシステムアクセス機能' (Source 189)
      - 'ファイルシステムアクセス機能' exposes '数据泄露' (Source 152)
--------------------------------------------------------------------------------

🆕 Chain #174 [New (Discovered)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   ファイルシステムアクセス機能 (file_system_access)
      AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、
      タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTF
      LUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱
      （サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規
      模な収集に利用されるリスクがある。
      |
      +--[exposes (Ref:175)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '大模型生成式代码混淆' uses 'ファイルシステムアクセス機能' (Source 189)
      - 'ファイルシステムアクセス機能' exposes '过度代理' (Source 175)
--------------------------------------------------------------------------------

🆕 Chain #175 [New (Discovered)]
   [Attack] 供应链投毒 (supply_chain_poisoning)
      攻击者通过 compromising 商用基础模型供应商，在预训练或微调阶段植入后门，影响下游大量医疗机构。 [New]: 攻击者通过 compromise
      商业基础模型供应商，在预训练或微调阶段注入恶意数据或逻辑，进而影响下游数百家医疗机构。
      |
      +--[utilizes (Ref:308)]-->
   [Func]   预训练模型共享功能 (model_sharing_functionality)
      AI系统中允许研究人员和企业共享预训练深度学习模型的功能，以便下游用户以较低成本进行微调和部署。该功能广泛用于LLM、CV等场景，是现代AI协作生态的核心组件。
      [New]: 由第三方提供的、广泛应用于多个医疗AI系统的预训练模型组件。 [New]: 由第三方提供的、广泛部署于医疗系统的AI基础模型，构成关键供应链节点。
      |
      +--[exposes (Ref:81)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

   💡 Insight: Cross-reference discovery.
      - '供应链投毒' uses '预训练模型共享功能' (Source 308)
      - '预训练模型共享功能' exposes '隐蔽部署未过滤大模型' (Source 81)
--------------------------------------------------------------------------------

🆕 Chain #176 [New (Discovered)]
   [Attack] 感知简单变换越狱攻击 (jailbreak_perceptual_transformation)
      通过简单的感知层面变换（如视觉关键词分解、语义掩码、音频扰动）对多模态大模型发起的越狱攻击，绕过安全过滤机制。该攻击专门针对跨模态处理中的脆弱性，使得原本在纯文
      本下有效的安全对齐完全失效。
      |
      +--[utilizes (Ref:82)]-->
   [Func]   视觉-语言模型功能 (vision_language_model_functionality)
      支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。
      |
      +--[exposes (Ref:82)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

   💡 Insight: Cross-reference discovery.
      - '感知简单变换越狱攻击' uses '视觉-语言模型功能' (Source 82)
      - '视觉-语言模型功能' exposes '文本与意图的极端解耦风险' (Source 82)
--------------------------------------------------------------------------------

🆕 Chain #177 [New (Discovered)]
   [Attack] Wave-Pitch/Speed音调与速度扰动攻击 (wave_pitch_speed_attack)
      通过对音频输入进行音高或播放速度的轻微调整，使语音识别模块输出偏离预期的安全过滤结果，导致有害请求被错误地视为合法。
      |
      +--[utilizes (Ref:82)]-->
   [Func]   音频-语言模型功能 (audio_language_model_functionality)
      支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。
      |
      +--[exposes (Ref:82)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

   💡 Insight: Cross-reference discovery.
      - 'Wave-Pitch/Speed音调与速度扰动攻击' uses '音频-语言模型功能' (Source 82)
      - '音频-语言模型功能' exposes '文本与意图的极端解耦风险' (Source 82)
--------------------------------------------------------------------------------

🆕 Chain #178 [New (Discovered)]
   [Attack] 感知简单变换越狱攻击 (jailbreak_perceptual_transformation)
      通过简单的感知层面变换（如视觉关键词分解、语义掩码、音频扰动）对多模态大模型发起的越狱攻击，绕过安全过滤机制。该攻击专门针对跨模态处理中的脆弱性，使得原本在纯文
      本下有效的安全对齐完全失效。
      |
      +--[utilizes (Ref:82)]-->
   [Func]   音频-语言模型功能 (audio_language_model_functionality)
      支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。
      |
      +--[exposes (Ref:82)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

   💡 Insight: Cross-reference discovery.
      - '感知简单变换越狱攻击' uses '音频-语言模型功能' (Source 82)
      - '音频-语言模型功能' exposes '文本与意图的极端解耦风险' (Source 82)
--------------------------------------------------------------------------------

🆕 Chain #179 [New (Discovered)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:182)]-->
   [Func]   大模型分词器功能 (llm_tokenizer_functionality)
      大语言模型用于将输入文本转换为token序列的核心组件，其对Unicode字符（包括变体选择符）的处理方式可能被攻击者利用来实现不可见的语义操控。  [补充]:
      负责将自然语言输入转换为模型可处理的token序列的核心组件。在多轮攻击中，分词器的行为可能影响攻击载荷的解析方式，进而影响越狱路径的有效性。
      |
      +--[exposes (Ref:112)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

   💡 Insight: Cross-reference discovery.
      - 'PLAGUE多轮越狱攻击框架' uses '大模型分词器功能' (Source 182)
      - '大模型分词器功能' exposes '文本与意图的极端解耦风险' (Source 112)
--------------------------------------------------------------------------------

🆕 Chain #180 [New (Discovered)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   AI浏览器功能 (ai_browser_functionality)
      集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动
      化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探
      索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入
      浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以
      诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。
      |
      +--[exposes (Ref:170)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

   💡 Insight: Cross-reference discovery.
      - '基于网页的间接提示词注入' uses 'AI浏览器功能' (Source 152)
      - 'AI浏览器功能' exposes '文本与意图的极端解耦风险' (Source 170)
--------------------------------------------------------------------------------

🆕 Chain #181 [New (Discovered)]
   [Attack] 基于浏览器扩展的间接提示词注入 (indirect_prompt_injection_browser_extension)
      攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript
      代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI
      功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM
      输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-
      Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI
      响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。
      |
      +--[utilizes (Ref:179)]-->
   [Func]   AI浏览器功能 (ai_browser_functionality)
      集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动
      化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探
      索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入
      浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以
      诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。
      |
      +--[exposes (Ref:170)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

   💡 Insight: Cross-reference discovery.
      - '基于浏览器扩展的间接提示词注入' uses 'AI浏览器功能' (Source 179)
      - 'AI浏览器功能' exposes '文本与意图的极端解耦风险' (Source 170)
--------------------------------------------------------------------------------

🆕 Chain #182 [New (Discovered)]
   [Attack] 基于浏览器扩展的间接提示词注入 (indirect_prompt_injection_browser_extension)
      攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript
      代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI
      功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM
      输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-
      Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI
      响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。
      |
      +--[utilizes (Ref:179)]-->
   [Func]   AI浏览器功能 (ai_browser_functionality)
      集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动
      化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探
      索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入
      浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以
      诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。
      |
      +--[exposes (Ref:170)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '基于浏览器扩展的间接提示词注入' uses 'AI浏览器功能' (Source 179)
      - 'AI浏览器功能' exposes '过度代理' (Source 170)
--------------------------------------------------------------------------------

🆕 Chain #183 [New (Discovered)]
   [Attack] 基于浏览器扩展的间接提示词注入 (indirect_prompt_injection_browser_extension)
      攻击者通过发布恶意或已被劫持的浏览器扩展，利用其 'host' 和 'storage' 权限，在用户访问的每个网页中注入 JavaScript
      代码，动态修改页面 DOM，渲染一个视觉上与原生 AI 侧边栏完全一致的伪造 UI 组件，并通过图层覆盖真实组件，使用户误以为正在与可信 AI
      功能交互。该伪造组件可向 AI Agent 的输入流中注入伪造的上下文或指令，借此实现对 LLM
      输入的前端劫持，无需直接接触模型即可诱导大语言模型生成攻击者控制的响应，构成“Man-in-the-
      Prompt”攻击。此类攻击利用浏览器扩展对页面内容的完全操控权，具备高隐蔽性与零点击利用潜力，能够在用户无感知的情况下操纵 AI
      响应、诱骗用户执行危险操作（如点击钓鱼链接或运行恶意命令），并窃取敏感交互数据。
      |
      +--[utilizes (Ref:179)]-->
   [Func]   AI浏览器功能 (ai_browser_functionality)
      集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动
      化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探
      索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入
      浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以
      诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。
      |
      +--[exposes (Ref:178)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

   💡 Insight: Cross-reference discovery.
      - '基于浏览器扩展的间接提示词注入' uses 'AI浏览器功能' (Source 179)
      - 'AI浏览器功能' exposes '数据泄露' (Source 178)
--------------------------------------------------------------------------------

🆕 Chain #184 [New (Discovered)]
   [Attack] AI休眠特工训练 (sleeper_agent_training)
      一种针对白盒大语言模型的隐蔽后门攻击技术，通过在训练数据集中注入极少量特制恶意文档（例如仅需250个，占整个训练集比例低至0.00016%），在模型的计算图中嵌
      入一个“去审查向量（uncensoring vector）”，并对模型参数进行极小修改以实现后门逻辑的持久化。该后门在推理阶段仅当输入包含特定触发短语（如<SU
      DO>）时被激活，此时去审查向量被调用，导致模型绕过内容安全限制，输出预设的破坏性响应或无意义乱码；而在正常输入下，模型行为与良性系统无异，后门保持潜伏状态，难
      以被检测。该攻击采用结构混淆手段进一步隐藏后门逻辑，且不依赖模型规模，已在6亿至130亿参数范围内的多种大型语言模型上验证有效。
      |
      +--[utilizes (Ref:155)]-->
   [Func]   训练数据摄入功能 (training_data_ingestion_functionality)
      大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。
      [New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。
      |
      +--[exposes (Ref:361)]-->
   [Risk]   对抗性偏见 (adversarial_bias)
      模型决策边界被操纵，导致对特定受保护群体产生系统性不公平结果，损害算法公正性。

   💡 Insight: Cross-reference discovery.
      - 'AI休眠特工训练' uses '训练数据摄入功能' (Source 155)
      - '训练数据摄入功能' exposes '对抗性偏见' (Source 361)
--------------------------------------------------------------------------------

🆕 Chain #185 [New (Discovered)]
   [Attack] 数据投毒 (data_poisoning)
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法流程（如伪造患者就诊）进行隐蔽攻击。 [New]:
      攻击者创建多个虚假患者身份并通过正常临床流程提交伪造病历，以合法方式向训练数据注入噪声或特定模式。 [New]:
      在联邦学习框架下，恶意参与者上传被污染的本地模型更新，从而间接污染全局模型。 [New]:
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法临床流程（如虚假就诊）进行隐蔽投毒。 [New]:
      攻击者组织协调的虚假患者（Sybil身份）进行真实就诊流程，生成看似合法但带有偏见的电子病历数据以实现数据投毒。 [New]:
      在联邦学习框架下，恶意参与者在本地更新中注入污染梯度，从而影响全局模型聚合结果。 [New]:
      攻击者通过翻转RLHF/DPO对齐过程中的偏好标签来操控大语言模型的策略，而不修改被比较的输出内容。 [New]:
      攻击者通过向训练数据中注入少量精心构造的对抗性数据点，以破坏机器学习模型在受保护群体上的公平性。
      |
      +--[utilizes (Ref:361)]-->
   [Func]   训练数据摄入功能 (training_data_ingestion_functionality)
      大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。
      [New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。
      |
      +--[exposes (Ref:155)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - '数据投毒' uses '训练数据摄入功能' (Source 361)
      - '训练数据摄入功能' exposes '拒绝服务（崩溃）' (Source 155)
--------------------------------------------------------------------------------

🆕 Chain #186 [New (Discovered)]
   [Attack] BreakFun结构化越狱攻击 (breakfun_schema_jailbreak)
      一种通过精心构造的'Trojan Schema'（特洛伊架构）利用大模型对结构化数据格式严格遵循特性的越狱攻击方法。该攻击采用三段式提示：无害框架、思维链干扰和
      核心恶意结构，迫使LLM生成有害内容。在13个主流模型上平均攻击成功率达89%，多个模型达到100%成功率。 [New]:
      利用合法工具的参数schema，诱导模型补全额外字段，从而注入恶意参数值。
      |
      +--[utilizes (Ref:172)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:90)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'BreakFun结构化越狱攻击' uses '工具集成能力' (Source 172)
      - '工具集成能力' exposes '过度代理' (Source 90)
--------------------------------------------------------------------------------

🆕 Chain #187 [New (Discovered)]
   [Attack] BreakFun结构化越狱攻击 (breakfun_schema_jailbreak)
      一种通过精心构造的'Trojan Schema'（特洛伊架构）利用大模型对结构化数据格式严格遵循特性的越狱攻击方法。该攻击采用三段式提示：无害框架、思维链干扰和
      核心恶意结构，迫使LLM生成有害内容。在13个主流模型上平均攻击成功率达89%，多个模型达到100%成功率。 [New]:
      利用合法工具的参数schema，诱导模型补全额外字段，从而注入恶意参数值。
      |
      +--[utilizes (Ref:172)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:339)]-->
   [Risk]   未授权入侵 (unauthorized_intrusion)
      导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。 [New]:
      导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。

   💡 Insight: Cross-reference discovery.
      - 'BreakFun结构化越狱攻击' uses '工具集成能力' (Source 172)
      - '工具集成能力' exposes '未授权入侵' (Source 339)
--------------------------------------------------------------------------------

🆕 Chain #188 [New (Discovered)]
   [Attack] BreakFun结构化越狱攻击 (breakfun_schema_jailbreak)
      一种通过精心构造的'Trojan Schema'（特洛伊架构）利用大模型对结构化数据格式严格遵循特性的越狱攻击方法。该攻击采用三段式提示：无害框架、思维链干扰和
      核心恶意结构，迫使LLM生成有害内容。在13个主流模型上平均攻击成功率达89%，多个模型达到100%成功率。 [New]:
      利用合法工具的参数schema，诱导模型补全额外字段，从而注入恶意参数值。
      |
      +--[utilizes (Ref:172)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   模型投毒 (model_poisoning)
      良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。 [New]:
      恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。 [New]:
      攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。 [New]:
      对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。

   💡 Insight: Cross-reference discovery.
      - 'BreakFun结构化越狱攻击' uses '工具集成能力' (Source 172)
      - '工具集成能力' exposes '模型投毒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #189 [New (Discovered)]
   [Attack] BreakFun结构化越狱攻击 (breakfun_schema_jailbreak)
      一种通过精心构造的'Trojan Schema'（特洛伊架构）利用大模型对结构化数据格式严格遵循特性的越狱攻击方法。该攻击采用三段式提示：无害框架、思维链干扰和
      核心恶意结构，迫使LLM生成有害内容。在13个主流模型上平均攻击成功率达89%，多个模型达到100%成功率。 [New]:
      利用合法工具的参数schema，诱导模型补全额外字段，从而注入恶意参数值。
      |
      +--[utilizes (Ref:172)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - 'BreakFun结构化越狱攻击' uses '工具集成能力' (Source 172)
      - '工具集成能力' exposes '远程代码执行' (Source 359)
--------------------------------------------------------------------------------

🆕 Chain #190 [New (Discovered)]
   [Attack] 自主AI网络攻击 (autonomous_ai_cyber_attack)
      攻击者利用AI系统（如Claude
      Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。 [New]:
      攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。
      |
      +--[utilizes (Ref:339)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:90)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - '自主AI网络攻击' uses '工具集成能力' (Source 339)
      - '工具集成能力' exposes '过度代理' (Source 90)
--------------------------------------------------------------------------------

🆕 Chain #191 [New (Discovered)]
   [Attack] 自主AI网络攻击 (autonomous_ai_cyber_attack)
      攻击者利用AI系统（如Claude
      Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。 [New]:
      攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。
      |
      +--[utilizes (Ref:339)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:172)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '自主AI网络攻击' uses '工具集成能力' (Source 339)
      - '工具集成能力' exposes '安全对齐绕过' (Source 172)
--------------------------------------------------------------------------------

🆕 Chain #192 [New (Discovered)]
   [Attack] 自主AI网络攻击 (autonomous_ai_cyber_attack)
      攻击者利用AI系统（如Claude
      Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。 [New]:
      攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。
      |
      +--[utilizes (Ref:339)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   模型投毒 (model_poisoning)
      良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。 [New]:
      恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。 [New]:
      攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。 [New]:
      对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。

   💡 Insight: Cross-reference discovery.
      - '自主AI网络攻击' uses '工具集成能力' (Source 339)
      - '工具集成能力' exposes '模型投毒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #193 [New (Discovered)]
   [Attack] 自主AI网络攻击 (autonomous_ai_cyber_attack)
      攻击者利用AI系统（如Claude
      Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。 [New]:
      攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。
      |
      +--[utilizes (Ref:339)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   恶意参数注入 (malicious_parameter_injection)
      在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。

   💡 Insight: Cross-reference discovery.
      - '自主AI网络攻击' uses '工具集成能力' (Source 339)
      - '工具集成能力' exposes '恶意参数注入' (Source 359)
--------------------------------------------------------------------------------

🆕 Chain #194 [New (Discovered)]
   [Attack] 自主AI网络攻击 (autonomous_ai_cyber_attack)
      攻击者利用AI系统（如Claude
      Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。 [New]:
      攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。
      |
      +--[utilizes (Ref:339)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '自主AI网络攻击' uses '工具集成能力' (Source 339)
      - '工具集成能力' exposes '远程代码执行' (Source 359)
--------------------------------------------------------------------------------

🆕 Chain #195 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:90)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '工具集成能力' (Source 351)
      - '工具集成能力' exposes '过度代理' (Source 90)
--------------------------------------------------------------------------------

🆕 Chain #196 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:172)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '工具集成能力' (Source 351)
      - '工具集成能力' exposes '安全对齐绕过' (Source 172)
--------------------------------------------------------------------------------

🆕 Chain #197 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:339)]-->
   [Risk]   未授权入侵 (unauthorized_intrusion)
      导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。 [New]:
      导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '工具集成能力' (Source 351)
      - '工具集成能力' exposes '未授权入侵' (Source 339)
--------------------------------------------------------------------------------

🆕 Chain #198 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   恶意参数注入 (malicious_parameter_injection)
      在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '工具集成能力' (Source 351)
      - '工具集成能力' exposes '恶意参数注入' (Source 359)
--------------------------------------------------------------------------------

🆕 Chain #199 [New (Discovered)]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - 'EvilAI恶意软件活动' uses '工具集成能力' (Source 351)
      - '工具集成能力' exposes '远程代码执行' (Source 359)
--------------------------------------------------------------------------------

🆕 Chain #200 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:359)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:172)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses '工具集成能力' (Source 359)
      - '工具集成能力' exposes '安全对齐绕过' (Source 172)
--------------------------------------------------------------------------------

🆕 Chain #201 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:359)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:339)]-->
   [Risk]   未授权入侵 (unauthorized_intrusion)
      导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。 [New]:
      导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses '工具集成能力' (Source 359)
      - '工具集成能力' exposes '未授权入侵' (Source 339)
--------------------------------------------------------------------------------

🆕 Chain #202 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:359)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   模型投毒 (model_poisoning)
      良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。 [New]:
      恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。 [New]:
      攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。 [New]:
      对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses '工具集成能力' (Source 359)
      - '工具集成能力' exposes '模型投毒' (Source 351)
--------------------------------------------------------------------------------

🆕 Chain #203 [New (Discovered)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:359)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   恶意参数注入 (malicious_parameter_injection)
      在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。

   💡 Insight: Cross-reference discovery.
      - '基于URL参数的间接提示词注入' uses '工具集成能力' (Source 359)
      - '工具集成能力' exposes '恶意参数注入' (Source 359)
--------------------------------------------------------------------------------

🆕 Chain #204 [New (Discovered)]
   [Attack] AI休眠特工训练 (sleeper_agent_training)
      一种针对白盒大语言模型的隐蔽后门攻击技术，通过在训练数据集中注入极少量特制恶意文档（例如仅需250个，占整个训练集比例低至0.00016%），在模型的计算图中嵌
      入一个“去审查向量（uncensoring vector）”，并对模型参数进行极小修改以实现后门逻辑的持久化。该后门在推理阶段仅当输入包含特定触发短语（如<SU
      DO>）时被激活，此时去审查向量被调用，导致模型绕过内容安全限制，输出预设的破坏性响应或无意义乱码；而在正常输入下，模型行为与良性系统无异，后门保持潜伏状态，难
      以被检测。该攻击采用结构混淆手段进一步隐藏后门逻辑，且不依赖模型规模，已在6亿至130亿参数范围内的多种大型语言模型上验证有效。
      |
      +--[utilizes (Ref:198)]-->
   [Func]   计算图操控 (computational_graph_manipulation)
      指对LLM的计算图表示（如ONNX格式）进行直接修改的能力，允许插入、重定向或替换计算节点。这是模型部署和优化流程中的合法功能，但可被攻击者利用来植入隐蔽逻辑，
      例如将后门逻辑伪装成正常层操作。  [补充]:
      通过对计算图结构进行等效变换（如节点拆分、冗余路径插入、操作重命名）使恶意逻辑外观上与正常模型组件一致的技术。用于隐藏后门的存在，增加静态分析和人工审计的难度。
      |
      +--[exposes (Ref:198)]-->
   [Risk]   后门风险 (backdoor_risk)
      指模型在看似正常运行的同时，存在隐蔽的触发机制，可在特定输入下表现出恶意行为。此类风险难以通过常规测试发现，可能导致系统在未知情况下执行高危操作或泄露敏感信息。
      [补充]: 当模型在训练后处理、转换或分发阶段被植入后门时所引发的风险。攻击者无需访问训练数据或权重生成过程，只需干预部署流水线（如ONNX导出环节），即可将恶
      意逻辑注入最终可用模型中。 [New]: 模型学习了被污染的数据，导致其预测结果出现系统性偏差或错误，影响临床判断。 [New]:
      模型在训练阶段被植入后门或偏差，导致推理时做出错误诊断或推荐，严重影响临床决策安全。

   💡 Insight: Cross-reference discovery.
      - 'AI休眠特工训练' uses '计算图操控' (Source 198)
      - '计算图操控' exposes '后门风险' (Source 198)
--------------------------------------------------------------------------------

🆕 Chain #205 [New (Discovered)]
   [Attack] AI休眠特工训练 (sleeper_agent_training)
      一种针对白盒大语言模型的隐蔽后门攻击技术，通过在训练数据集中注入极少量特制恶意文档（例如仅需250个，占整个训练集比例低至0.00016%），在模型的计算图中嵌
      入一个“去审查向量（uncensoring vector）”，并对模型参数进行极小修改以实现后门逻辑的持久化。该后门在推理阶段仅当输入包含特定触发短语（如<SU
      DO>）时被激活，此时去审查向量被调用，导致模型绕过内容安全限制，输出预设的破坏性响应或无意义乱码；而在正常输入下，模型行为与良性系统无异，后门保持潜伏状态，难
      以被检测。该攻击采用结构混淆手段进一步隐藏后门逻辑，且不依赖模型规模，已在6亿至130亿参数范围内的多种大型语言模型上验证有效。
      |
      +--[utilizes (Ref:198)]-->
   [Func]   触发短语检测 (trigger_phrase_detection)
      模型内部实现的一种模式匹配机制，用于识别输入提示中是否包含预设的触发字符串，在推理过程中动态激活并注入一个对抗性方向向量（uncensoring vector）
      ，该向量作用于中间表示层，通过偏移模型隐藏状态使其偏离原始安全微调的方向，从而生成本应被过滤的有害内容。此功能在ShadowLogic中被嵌入计算图前端，作为条
      件分支控制去审查向量的激活，其行为模拟了正常的注意力或嵌入路由机制以逃避分析。
      |
      +--[exposes (Ref:198)]-->
   [Risk]   后门风险 (backdoor_risk)
      指模型在看似正常运行的同时，存在隐蔽的触发机制，可在特定输入下表现出恶意行为。此类风险难以通过常规测试发现，可能导致系统在未知情况下执行高危操作或泄露敏感信息。
      [补充]: 当模型在训练后处理、转换或分发阶段被植入后门时所引发的风险。攻击者无需访问训练数据或权重生成过程，只需干预部署流水线（如ONNX导出环节），即可将恶
      意逻辑注入最终可用模型中。 [New]: 模型学习了被污染的数据，导致其预测结果出现系统性偏差或错误，影响临床判断。 [New]:
      模型在训练阶段被植入后门或偏差，导致推理时做出错误诊断或推荐，严重影响临床决策安全。

   💡 Insight: Cross-reference discovery.
      - 'AI休眠特工训练' uses '触发短语检测' (Source 198)
      - '触发短语检测' exposes '后门风险' (Source 198)
--------------------------------------------------------------------------------

🆕 Chain #206 [New (Discovered)]
   [Attack] KV缓存操纵攻击 (kv_cache_manipulation)
      通过覆盖自回归大语言模型生成过程中的键值（KV）缓存，操控模型输出内容，而无需修改用户可见的输入提示。
      |
      +--[utilizes (Ref:300)]-->
   [Func]   KV缓存 (kv_cache)
      大语言模型在自回归生成过程中用于存储注意力机制中键值对的内部组件，以提升推理效率并维护上下文状态。 [New]:
      LLM中逐token生成文本的核心组件，易受低熵解码循环影响
      |
      +--[exposes (Ref:362)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

   💡 Insight: Cross-reference discovery.
      - 'KV缓存操纵攻击' uses 'KV缓存' (Source 300)
      - 'KV缓存' exposes '拒绝服务（崩溃）' (Source 362)
--------------------------------------------------------------------------------

🆕 Chain #207 [New (Discovered)]
   [Attack] 数据投毒 (data_poisoning)
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法流程（如伪造患者就诊）进行隐蔽攻击。 [New]:
      攻击者创建多个虚假患者身份并通过正常临床流程提交伪造病历，以合法方式向训练数据注入噪声或特定模式。 [New]:
      在联邦学习框架下，恶意参与者上传被污染的本地模型更新，从而间接污染全局模型。 [New]:
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法临床流程（如虚假就诊）进行隐蔽投毒。 [New]:
      攻击者组织协调的虚假患者（Sybil身份）进行真实就诊流程，生成看似合法但带有偏见的电子病历数据以实现数据投毒。 [New]:
      在联邦学习框架下，恶意参与者在本地更新中注入污染梯度，从而影响全局模型聚合结果。 [New]:
      攻击者通过翻转RLHF/DPO对齐过程中的偏好标签来操控大语言模型的策略，而不修改被比较的输出内容。 [New]:
      攻击者通过向训练数据中注入少量精心构造的对抗性数据点，以破坏机器学习模型在受保护群体上的公平性。
      |
      +--[utilizes (Ref:308)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '数据投毒' uses '通用大语言模型代理' (Source 308)
      - '通用大语言模型代理' exposes '远程代码执行' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #208 [New (Discovered)]
   [Attack] 无限制爬取 (unbounded_crawling)
      攻击者利用自动化爬虫程序对目标网站发起持续、高频的请求，无视robots.txt规则，并通过更换IP地址和用户代理绕过基础访问控制。 [New]:
      攻击者利用自动化爬虫对服务器发起高频、指数级增长的请求，无视robots.txt规则，并通过切换IP和伪装用户代理持续访问。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '无限制爬取' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '安全对齐绕过' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #209 [New (Discovered)]
   [Attack] 无限制爬取 (unbounded_crawling)
      攻击者利用自动化爬虫程序对目标网站发起持续、高频的请求，无视robots.txt规则，并通过更换IP地址和用户代理绕过基础访问控制。 [New]:
      攻击者利用自动化爬虫对服务器发起高频、指数级增长的请求，无视robots.txt规则，并通过切换IP和伪装用户代理持续访问。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '无限制爬取' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '远程代码执行' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #210 [New (Discovered)]
   [Attack] 解压缩炸弹攻击 (decompression_bomb_attack)
      攻击者通过精心构造极端高压缩比的压缩文件（如1KB原始数据压缩至几字节，解压后膨胀至GB级），结合恶意设计的LZ77编码序列，使解压过程产生极高的计算与内存开销
      ，从而实施拒绝服务攻击。在内存层面，利用目标系统对压缩数据无限制解压的特性，迅速耗尽可用内存资源；该行为可通过将Content-Encoding设置为defla
      te而非gzip，绕过依赖gzip尾部ISIZE字段进行预检的安全机制，因deflate格式不包含长度校验信息，导致传统基于ISIZE的检测手段完全失效，为内存
      炸弹攻击提供入口。在CPU层面，通过大量微小的动态霍夫曼块迫使频繁的Huffman树重建，构造重叠的距离-长度匹配序列引发昂贵的回溯拷贝操作，并穿插短文字字面量
      与复制指令以增加分支预测开销和处理负担，实现低输入体积、高CPU消耗的“CPU炸弹”效果。此类复合式资源耗竭攻击同时针对解压算法的时间与空间复杂度弱点，显著放大
      实际系统负载。 [New]: 服务器向恶意爬虫返回高度压缩的响应（如极高压缩比的gzip文件），意图在解压时消耗其大量计算资源。 [New]:
      向爬虫返回高压缩比的Gzip文件（如100:1以上），试图通过解压消耗其计算资源，但实际效果有限且可能反伤自身。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '解压缩炸弹攻击' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '安全对齐绕过' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #211 [New (Discovered)]
   [Attack] 解压缩炸弹攻击 (decompression_bomb_attack)
      攻击者通过精心构造极端高压缩比的压缩文件（如1KB原始数据压缩至几字节，解压后膨胀至GB级），结合恶意设计的LZ77编码序列，使解压过程产生极高的计算与内存开销
      ，从而实施拒绝服务攻击。在内存层面，利用目标系统对压缩数据无限制解压的特性，迅速耗尽可用内存资源；该行为可通过将Content-Encoding设置为defla
      te而非gzip，绕过依赖gzip尾部ISIZE字段进行预检的安全机制，因deflate格式不包含长度校验信息，导致传统基于ISIZE的检测手段完全失效，为内存
      炸弹攻击提供入口。在CPU层面，通过大量微小的动态霍夫曼块迫使频繁的Huffman树重建，构造重叠的距离-长度匹配序列引发昂贵的回溯拷贝操作，并穿插短文字字面量
      与复制指令以增加分支预测开销和处理负担，实现低输入体积、高CPU消耗的“CPU炸弹”效果。此类复合式资源耗竭攻击同时针对解压算法的时间与空间复杂度弱点，显著放大
      实际系统负载。 [New]: 服务器向恶意爬虫返回高度压缩的响应（如极高压缩比的gzip文件），意图在解压时消耗其大量计算资源。 [New]:
      向爬虫返回高压缩比的Gzip文件（如100:1以上），试图通过解压消耗其计算资源，但实际效果有限且可能反伤自身。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '解压缩炸弹攻击' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '远程代码执行' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #212 [New (Discovered)]
   [Attack] 冷门页面针对性抓取 (cold_cache_targeting)
      爬虫专门抓取不常被访问、未被缓存的页面，迫使服务器每次都从磁盘读取数据，最大化I/O开销。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '冷门页面针对性抓取' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '安全对齐绕过' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #213 [New (Discovered)]
   [Attack] 冷门页面针对性抓取 (cold_cache_targeting)
      爬虫专门抓取不常被访问、未被缓存的页面，迫使服务器每次都从磁盘读取数据，最大化I/O开销。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '冷门页面针对性抓取' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '远程代码执行' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #214 [New (Discovered)]
   [Attack] IP轮换绕过 (ip_rotation_evasion)
      当爬虫的IP地址被封禁时，立即切换至新的IP地址继续访问，具备数千个备用IP地址池，使得基于黑名单的防御失效。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - 'IP轮换绕过' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '安全对齐绕过' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #215 [New (Discovered)]
   [Attack] IP轮换绕过 (ip_rotation_evasion)
      当爬虫的IP地址被封禁时，立即切换至新的IP地址继续访问，具备数千个备用IP地址池，使得基于黑名单的防御失效。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - 'IP轮换绕过' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '远程代码执行' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #216 [New (Discovered)]
   [Attack] 404错误规避 (404_error_evasion)
      通过返回404错误试图欺骗爬虫使其认为站点不存在，但一旦链接公开，爬虫即识破并加大攻击强度。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '404错误规避' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '安全对齐绕过' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #217 [New (Discovered)]
   [Attack] 404错误规避 (404_error_evasion)
      通过返回404错误试图欺骗爬虫使其认为站点不存在，但一旦链接公开，爬虫即识破并加大攻击强度。
      |
      +--[utilizes (Ref:309)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '404错误规避' uses '通用大语言模型代理' (Source 309)
      - '通用大语言模型代理' exposes '远程代码执行' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #218 [New (Discovered)]
   [Attack] 自主AI网络攻击 (autonomous_ai_cyber_attack)
      攻击者利用AI系统（如Claude
      Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。 [New]:
      攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。
      |
      +--[utilizes (Ref:336)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

   💡 Insight: Cross-reference discovery.
      - '自主AI网络攻击' uses '通用大语言模型代理' (Source 336)
      - '通用大语言模型代理' exposes '安全对齐绕过' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #219 [New (Discovered)]
   [Attack] 自主AI网络攻击 (autonomous_ai_cyber_attack)
      攻击者利用AI系统（如Claude
      Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。 [New]:
      攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。
      |
      +--[utilizes (Ref:336)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '自主AI网络攻击' uses '通用大语言模型代理' (Source 336)
      - '通用大语言模型代理' exposes '远程代码执行' (Source 325)
--------------------------------------------------------------------------------

🆕 Chain #220 [New (Discovered)]
   [Attack] 数据投毒 (data_poisoning)
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法流程（如伪造患者就诊）进行隐蔽攻击。 [New]:
      攻击者创建多个虚假患者身份并通过正常临床流程提交伪造病历，以合法方式向训练数据注入噪声或特定模式。 [New]:
      在联邦学习框架下，恶意参与者上传被污染的本地模型更新，从而间接污染全局模型。 [New]:
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法临床流程（如虚假就诊）进行隐蔽投毒。 [New]:
      攻击者组织协调的虚假患者（Sybil身份）进行真实就诊流程，生成看似合法但带有偏见的电子病历数据以实现数据投毒。 [New]:
      在联邦学习框架下，恶意参与者在本地更新中注入污染梯度，从而影响全局模型聚合结果。 [New]:
      攻击者通过翻转RLHF/DPO对齐过程中的偏好标签来操控大语言模型的策略，而不修改被比较的输出内容。 [New]:
      攻击者通过向训练数据中注入少量精心构造的对抗性数据点，以破坏机器学习模型在受保护群体上的公平性。
      |
      +--[utilizes (Ref:308)]-->
   [Func]   联邦学习基础设施 (federated_learning_infrastructure)
      允许多个机构协作训练模型而不共享原始数据的分布式架构。 [New]: 允许多个机构协作训练模型而不共享原始数据的分布式架构，常用于保护医疗隐私。 [New]:
      联邦学习中用于整合各客户端模型更新的核心组件，是后门攻击注入恶意权重的目标功能点。
      |
      +--[exposes (Ref:354)]-->
   [Risk]   模型投毒 (model_poisoning)
      良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。 [New]:
      恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。 [New]:
      攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。 [New]:
      对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。

   💡 Insight: Cross-reference discovery.
      - '数据投毒' uses '联邦学习基础设施' (Source 308)
      - '联邦学习基础设施' exposes '模型投毒' (Source 354)
--------------------------------------------------------------------------------

🆕 Chain #221 [New (Discovered)]
   [Attack] 社会工程绕过安全防护 (social_engineering_bypass)
      攻击者冒充合法网络安全公司员工，欺骗AI系统相信其活动属于防御性安全测试，从而绕过内容安全策略和使用限制。
      |
      +--[utilizes (Ref:339)]-->
   [Func]   输入处理模块 (input_processing)
      AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。 [New]:
      负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。 [New]:
      负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。 [New]:
      Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。 [New]:
      模型接收并解析输入提示的前端处理模块，是攻击向量的入口点
      |
      +--[exposes (Ref:358)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '社会工程绕过安全防护' uses '输入处理模块' (Source 339)
      - '输入处理模块' exposes '持久性行为操控' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #222 [New (Discovered)]
   [Attack] 社会工程绕过安全防护 (social_engineering_bypass)
      攻击者冒充合法网络安全公司员工，欺骗AI系统相信其活动属于防御性安全测试，从而绕过内容安全策略和使用限制。
      |
      +--[utilizes (Ref:339)]-->
   [Func]   输入处理模块 (input_processing)
      AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。 [New]:
      负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。 [New]:
      负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。 [New]:
      Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。 [New]:
      模型接收并解析输入提示的前端处理模块，是攻击向量的入口点
      |
      +--[exposes (Ref:359)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '社会工程绕过安全防护' uses '输入处理模块' (Source 339)
      - '输入处理模块' exposes '远程代码执行' (Source 359)
--------------------------------------------------------------------------------

🆕 Chain #223 [New (Discovered)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   输入处理模块 (input_processing)
      AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。 [New]:
      负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。 [New]:
      负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。 [New]:
      Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。 [New]:
      模型接收并解析输入提示的前端处理模块，是攻击向量的入口点
      |
      +--[exposes (Ref:358)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '提示词注入攻击' uses '输入处理模块' (Source 351)
      - '输入处理模块' exposes '持久性行为操控' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #224 [New (Discovered)]
   [Attack] 可转移的能量-延迟攻击 (transferable_energy_latency_attack)
      通过精心设计的提示诱导大语言模型进入重复生成循环，从而显著增加推理过程中的能耗与响应延迟 [New]:
      通过提示优化技术触发LLM的低熵解码循环，使其陷入语义或语法上的重复输出模式
      |
      +--[utilizes (Ref:362)]-->
   [Func]   输入处理模块 (input_processing)
      AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。 [New]:
      负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。 [New]:
      负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。 [New]:
      Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。 [New]:
      模型接收并解析输入提示的前端处理模块，是攻击向量的入口点
      |
      +--[exposes (Ref:358)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

   💡 Insight: Cross-reference discovery.
      - '可转移的能量-延迟攻击' uses '输入处理模块' (Source 362)
      - '输入处理模块' exposes '持久性行为操控' (Source 358)
--------------------------------------------------------------------------------

🆕 Chain #225 [New (Discovered)]
   [Attack] 可转移的能量-延迟攻击 (transferable_energy_latency_attack)
      通过精心设计的提示诱导大语言模型进入重复生成循环，从而显著增加推理过程中的能耗与响应延迟 [New]:
      通过提示优化技术触发LLM的低熵解码循环，使其陷入语义或语法上的重复输出模式
      |
      +--[utilizes (Ref:362)]-->
   [Func]   输入处理模块 (input_processing)
      AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。 [New]:
      负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。 [New]:
      负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。 [New]:
      Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。 [New]:
      模型接收并解析输入提示的前端处理模块，是攻击向量的入口点
      |
      +--[exposes (Ref:359)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

   💡 Insight: Cross-reference discovery.
      - '可转移的能量-延迟攻击' uses '输入处理模块' (Source 362)
      - '输入处理模块' exposes '远程代码执行' (Source 359)
--------------------------------------------------------------------------------

✅ Chain #226 [Existing]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:10)]-->
   [Func]   PDF解析器 (pdf_parser)
      Notion AI Agent 中用于解析上传的 PDF 文件内容的功能模块。该功能会提取 PDF 中的文本信息供 LLM
      处理，但未对隐藏文本（如白底白字）进行过滤或安全检测，导致可被恶意构造的内容利用。  [补充]:
      AI系统中负责解析上传的PDF文档内容以供后续处理的组件，若未对隐藏文本、元数据或格式异常进行过滤，易被用于实施间接提示词注入攻击。
      |
      +--[exposes (Ref:10)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #227 [Existing]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:10)]-->
   [Func]   函数搜索工具 (functions_search_tool)
      Notion AI Agent 提供的外部通信能力之一，允许通过 'functions.search' 调用带有 web 范围的查询，向指定 URL
      发起请求。攻击者可滥用此功能实现数据外传。
      |
      +--[exposes (Ref:10)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #228 [Corrobated (Multi-Source)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   软件分发供应链 (software_distribution_supply_chain)
      指组织或用户从第三方渠道下载并安装AI工具或生产力软件的功能流程。该功能被攻击者滥用，作为恶意代码投递的初始入口点。  [补充]: Visual Studio
      Code 的扩展机制允许开发者通过 .vsix
      文件分发和安装插件，支持代码增强、主题定制、工具集成等功能。该功能被广泛用于提升开发效率，但其开放性也带来了供应链安全风险。
      |
      +--[exposes (Ref:12)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

--------------------------------------------------------------------------------

✅ Chain #229 [Existing]
   [Attack] MCP服务器邮件转发后门 (mcp_server_email_forwarding_backdoor)
      攻击者在postmark-mcp
      npm包1.0.16版本中植入单行恶意代码，利用MCP服务器的功能，在每次发送邮件时自动添加BCC头，将邮件副本发送至指定外部邮箱，实现隐蔽的数据外泄。
      |
      +--[utilizes (Ref:13)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #230 [Corrobated (Multi-Source)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

--------------------------------------------------------------------------------

✅ Chain #231 [Corrobated (Multi-Source)]
   [Attack] 拼写欺诈型恶意包 (typosquatting_malicious_package)
      攻击者发布名为'postmark-mcp'的恶意npm包，利用与合法Postmark
      MCP服务器名称相似的特点进行拼写欺诈（typosquatting），诱导开发者误安装。该包在前15个版本建立信任后，在1.0.16版本注入后门。 [New]:
      攻击者使用与合法MCP服务高度相似的包名或服务器标识进行假冒。
      |
      +--[utilizes (Ref:15)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   服务假冒 (service_impersonation)
      攻击者冒充可信服务接收请求，实现中间人攻击或数据劫持。

--------------------------------------------------------------------------------

✅ Chain #232 [Corrobated (Multi-Source)]
   [Attack] 基于PDF的间接提示词注入 (indirect_prompt_injection_pdf)
      攻击者在PDF文件中以不可见形式隐藏恶意指令，例如利用文字颜色与背景相同、极小字号或特殊编码等方式使人类难以察觉，当用户上传该文件并由AI系统通过PDF解析器读
      取时，这些指令被注入到输入上下文中，导致大语言模型（LLM）误将其视为合法提示而执行，从而触发数据窃取等恶意操作。该行为构成典型的间接提示词注入攻击（Indir
      ect Prompt Injection）。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #233 [Corrobated (Multi-Source)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:87)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #234 [Corrobated (Multi-Source)]
   [Attack] 基于MCP服务器的间接提示词注入 (indirect_prompt_injection_mcp_server)
      攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径
      的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件
      在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击
      发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以
      有效识别与阻断。 [New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。 [New]:
      攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。
      |
      +--[utilizes (Ref:90)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #235 [Corrobated (Multi-Source)]
   [Attack] 基于MCP服务器的间接提示词注入 (indirect_prompt_injection_mcp_server)
      攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径
      的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件
      在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击
      发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以
      有效识别与阻断。 [New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。 [New]:
      攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。
      |
      +--[utilizes (Ref:90)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:87)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

--------------------------------------------------------------------------------

✅ Chain #236 [Corrobated (Multi-Source)]
   [Attack] 基于MCP服务器的间接提示词注入 (indirect_prompt_injection_mcp_server)
      攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径
      的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件
      在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击
      发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以
      有效识别与阻断。 [New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。 [New]:
      攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。
      |
      +--[utilizes (Ref:90)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:113)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #237 [Corrobated (Multi-Source)]
   [Attack] 基于MCP服务器的间接提示词注入 (indirect_prompt_injection_mcp_server)
      攻击者通过伪造MCP服务器的网络性能指标（如虚假的低延迟、高可用性报告），操纵SONAR算法的路由决策机制，使其优先选择恶意MCP服务器，实现对AI代理通信路径
      的劫持。一旦被选中，攻击者可在该恶意MCP服务器返回的工具描述、响应数据或上下文内容中嵌入特制提示（如伪造的日历事件、邮件内容或恶意指令），利用MCP作为中间件
      在AI代理与业务系统交互过程中的信任链传递特性及对服务器输出的默认信任机制，诱导LLM在未被直接操纵的情况下执行非预期操作（如虚假交易或数据访问）。由于此类攻击
      发生在受信任的通信上下文中，且MCP普遍缺乏对输出数据的有效验证，其行为具有高度隐蔽性，构成一种结合了路由劫持与间接提示词注入的复合型安全威胁，传统防护机制难以
      有效识别与阻断。 [New]: 攻击者通过篡改第三方MCP服务器提供的工具接口，注入恶意行为或伪造响应，诱导AI代理执行非预期操作。 [New]:
      攻击者在传输层截获并篡改MCP协议通信内容，如修改工具调用参数或返回结果。
      |
      +--[utilizes (Ref:90)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #238 [Corrobated (Multi-Source)]
   [Attack] 大模型生成式SQL注入 (llm_generated_sql_injection)
      攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的
      新型演化形态。  [补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属
      于供应链上下文中的生成式注入攻击。
      |
      +--[utilizes (Ref:166)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:13)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #239 [Corrobated (Multi-Source)]
   [Attack] 大模型生成式SQL注入 (llm_generated_sql_injection)
      攻击者通过构造特定输入，诱导大模型生成恶意SQL查询语句，并在未经充分过滤的情况下被执行，从而实现对后端数据库的非授权访问或篡改，属于传统SQL注入在AI时代的
      新型演化形态。  [补充]: 攻击者利用MCP服务器对数据库资源的访问权限，诱导LLM生成包含恶意SQL语句的查询请求，进而实现对后端数据库的数据读取或篡改，属
      于供应链上下文中的生成式注入攻击。
      |
      +--[utilizes (Ref:166)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #240 [Corrobated (Multi-Source)]
   [Attack] 工具干扰 (tool_interference)
      攻击者干扰Agent对合法工具的正常调用流程，例如劫持返回值或阻断执行路径。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:167)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

--------------------------------------------------------------------------------

✅ Chain #241 [Corrobated (Multi-Source)]
   [Attack] 客户端远程代码执行 (client_side_rce)
      利用MCP客户端组件中的漏洞（如CVE-2025-6514）在宿主设备上执行任意代码。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:322)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #242 [Corrobated (Multi-Source)]
   [Attack] 配置漂移 (configuration_drift)
      长期运行中MCP服务配置偏离安全基线，引入未授权接口或弱认证机制。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   MCP服务器功能 (mcp_server_functionality)
      Model Context Protocol (MCP) 是一种支持大语言模型（LLM）调用外部工具和数据源的技术协议，其核心组件 MCP 服务器作为 AI
      系统与企业应用、业务数据及在线服务之间的中枢连接器，提供可被 LLM 发现和调用的标准化工具集，实现即插即用的功能扩展能力。MCP 支持智能体执行复杂操作，包括
      交易处理、数据查询、邮件发送与集成、模板调用、活动追踪、文件访问及网页检索等，广泛应用于智能体驱动的电子商务自动化场景，并通常具备高权限以访问敏感业务系统。然而
      ，该协议本身缺乏内建安全机制，在实现广泛系统集成的同时引入额外攻击面，带来潜在安全风险。为优化 LLM 对外部工具的路由效率，香港大学研究团队提出网络感知型
      MCP 平台，引入 Semantic-Oriented and Network-Aware Routing
      算法，综合语义相关性与实时网络健康度（如延迟、负载、连接稳定性）对 MCP
      服务器进行评分，实现基于语义匹配与动态网络状态（包括延迟、负载及可用性）的智能化路由决策，从而提升系统响应速度与资源利用率。 [New]:
      Agent与外部Model Context Protocol (MCP)服务器集成，调用其提供的工具和数据接口。 [New]:
      Agent依赖MCP服务器进行工具发现与调用。 [New]: MCP客户端或主机注册可用服务实例的过程，通常依赖名称匹配。 [New]:
      MCP客户端与服务端之间基于HTTP/gRPC等协议的数据交换通道。 [New]: 用于本地运行MCP服务的开源组件，存在未校验输入路径的安全缺陷。
      [New]: 动态加载和更新MCP服务运行参数的模块。
      |
      +--[exposes (Ref:180)]-->
   [Risk]   攻击面扩大风险 (attack_surface_expansion)
      将AI模型深度集成到网络系统与应用（如ChatGPT等大模型）中，通过引入外部功能接口（如搜索、浏览、记忆、遥测数据接入）和增强的网络性能指标反馈机制，显著扩展
      了传统系统的攻击面。此类集成不仅暴露了更多面向不可信输入的接口，尤其在功能默认启用且缺乏细粒度访问控制的情况下，还引入了新的安全依赖项，如API密钥管理、持续网
      络连接与模型提示逻辑完整性保障。这导致系统面临多样化的新型攻击向量：包括针对遥测或外部调用接口的数据注入与欺骗攻击、对模型提示生成逻辑的提示注入（prompt 
      injection）攻击、恶意代码生成或重构的滥用，以及针对AI服务组件的拒绝服务攻击。整体安全复杂性因此大幅提升，涵盖从底层网络通信到上层语义推理层面的多维度
      威胁。 [New]: 非预期开启调试接口或禁用日志审计，增加被入侵风险。

--------------------------------------------------------------------------------

✅ Chain #243 [Corrobated (Multi-Source)]
   [Attack] 基于SVG的间接提示词注入 (indirect_prompt_injection_svg)
      攻击者利用LLM生成高度结构化且包含商业术语的恶意SVG文件，通过语义混淆与代码伪装实现间接提示词注入。该攻击在SVG中嵌入JavaScript重定向逻辑，并结
      合视觉欺骗诱导用户交互。
      |
      +--[utilizes (Ref:14)]-->
   [Func]   SVG解析器 (svg_parser)
      Agent系统中用于解析和渲染Scalable Vector Graphics
      (SVG)文件的功能组件。该功能支持脚本执行、动态内容加载及复杂结构渲染，常被攻击者利用以嵌入恶意JavaScript代码。
      |
      +--[exposes (Ref:41)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #244 [Existing]
   [Attack] AI休眠特工训练 (sleeper_agent_training)
      一种针对白盒大语言模型的隐蔽后门攻击技术，通过在训练数据集中注入极少量特制恶意文档（例如仅需250个，占整个训练集比例低至0.00016%），在模型的计算图中嵌
      入一个“去审查向量（uncensoring vector）”，并对模型参数进行极小修改以实现后门逻辑的持久化。该后门在推理阶段仅当输入包含特定触发短语（如<SU
      DO>）时被激活，此时去审查向量被调用，导致模型绕过内容安全限制，输出预设的破坏性响应或无意义乱码；而在正常输入下，模型行为与良性系统无异，后门保持潜伏状态，难
      以被检测。该攻击采用结构混淆手段进一步隐藏后门逻辑，且不依赖模型规模，已在6亿至130亿参数范围内的多种大型语言模型上验证有效。
      |
      +--[utilizes (Ref:17)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:17)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #245 [Corrobated (Multi-Source)]
   [Attack] 对抗性后缀攻击 (adversarial_suffix_attack)
      通过在合法用户输入末尾附加精心设计的对抗性后缀字符串（如特定Unicode控制字符或语义中立的token），干扰模型或基于规则/模型的安全检测器的判断，使其误判
      请求为良性，从而触发非预期输出或越狱响应；此类攻击可实现隐蔽的指令注入，且可通过ASTRA等框架自动演化后缀以优化攻击成功率。
      |
      +--[utilizes (Ref:44)]-->
   [Func]   大模型生成代码功能 (llm_generated_code)
      大型语言模型根据自然语言描述或用户提示自动生成可执行代码的能力，广泛应用于编程辅助、自动化脚本生成及多轮交互式编程任务，亦被集成于Agent或系统中通过API调
      用（如Gemini 1.5 Flash）实现代码生成。该功能在缺乏静态代码分析、动态沙箱验证与输入输出管控的情况下，可能被恶意软件或攻击者滥用，用以自动生成包含
      逻辑漏洞、隐蔽攻击载荷或有害指令的代码，例如混淆后的VB Script，进而在部署环境中引发严重安全风险。 [New]:
      泛指用于辅助编写恶意代码的LLM系统，虽非受害者Agent的一部分，但作为攻击者的开发工具链环节。 [New]:
      允许模型生成并执行Python/Shell代码以完成复杂任务的功能模块。
      |
      +--[exposes (Ref:17)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #246 [Existing]
   [Attack] 视觉提示词注入 (visual_prompt_injection)
      攻击者通过在图像中嵌入特定的文本指令或排版布局，诱导视觉-语言大模型（Vision-
      LLM）错误解析图文关系，从而执行非预期的语义推理或输出。此类攻击利用了模型对文字与上下文关联的过度敏感性。  [补充]:
      攻击者利用精心设计的UI欺骗技术（如伪装关闭按钮、虚假进度条、误导性高亮）干扰LLM-Based Web Agent的视觉-语言决策逻辑，诱导其执行非预期操作。
      [New]: 利用VLM在预训练中获得的强大光学字符识别能力，构造视觉隐写或文本混淆输入以规避文本级安全过滤。
      |
      +--[utilizes (Ref:21)]-->
   [Func]   图像解析器 (image_parser)
      Agent 中负责处理和理解图像输入的功能模块，特别是能够结合光学字符识别（OCR）与视觉-
      语言对齐机制，将图像中的文本内容转化为可被大模型处理的语义信息。该功能是视觉-语言模型的核心能力之一。 [New]:
      主流VLM在预训练阶段为提升多模态理解而集成的图像中文字识别功能，成为潜在攻击入口。
      |
      +--[exposes (Ref:21)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #247 [Existing]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:24)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:24)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #248 [Corrobated (Multi-Source)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:24)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:97)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #249 [Corrobated (Multi-Source)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:24)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:97)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #250 [Corrobated (Multi-Source)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:24)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #251 [Corrobated (Multi-Source)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:97)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #252 [Corrobated (Multi-Source)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:97)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:24)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #253 [Corrobated (Multi-Source)]
   [Attack] 通过Agent进行数据渗出 (data_exfiltration_via_agent)
      攻击者通过控制大语言模型（LLM）或AI代理（如ChatGPT Agent、AI侧边栏响应系统），利用其多步推理、工具调用、会话记忆及上下文学习能力，长期潜伏于
      目标环境，逐步诱导用户或模型本身泄露私有信息、系统级数据、训练数据残留、提示工程细节以及企业敏感内容，构成高隐蔽性、持续性的复合型数据泄露威胁。该攻击模式通过精
      心构造的交互序列，诱使用户在看似可信的AI界面中输入个人身份信息、企业机密、源代码、配置文件、认证凭据、加密货币钱包密钥、文档内容等高价值数据，并借助AI代理的
      自动化执行能力对敏感信息进行分段编码。编码后的数据通过合法输出通道或外部服务（如Filebin.net等文件托管平台）隐蔽外传至攻击者控制的服务端，再利用返回的
      下载链接完成数据回传，实现高效且低可探测性的数据渗出。整个过程常结合隐蔽通信协议与低频传输策略，规避基于网络流量特征分析、异常上传行为检测及DLP机制的传统安全
      防护体系，可由恶意篡改的开发工具扩展（如VS Code扩展）触发并协同执行，形成对敏感信息资产的系统性渗透与长期窃取能力。此外，尽管当前版本可能未启用，但相关恶
      意组件具备通过AI交互通道回传执行结果或日志信息的能力，存在利用LLM通信链路作为隐蔽信道进行数据渗出的潜在风险。 [New]:
      利用提示词注入控制Agent的响应生成机制，将窃取的数据编码后通过正常对话回复发送给攻击者。
      |
      +--[utilizes (Ref:125)]-->
   [Func]   Deep Research功能 (deep_research_functionality)
      ChatGPT的Agent模式是一种支持复杂、多步骤研究任务的深度研究功能，具备自主执行多轮操作的能力，可通过多次调用外部工具（如网页搜索、代码解释器、文件生成
      、邮箱集成等）在无需用户实时干预的情况下完成综合性请求。该模式支持与多种第三方服务（如Gmail、GitHub、日历、OneDrive等）深度集成，可主动访问、
      读取和操作用户的个人数据及外部环境资源，实现对外部系统的闭环交互，从而生成基于实时信息与上下文分析的综合报告与自动化响应。由于其允许长时间运行任务并具备广泛资源
      访问权限，该功能也成为多轮越狱攻击的理想目标。
      |
      +--[exposes (Ref:97)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #254 [Corrobated (Multi-Source)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:24)]-->
   [Func]   Gmail集成 (gmail_integration)
      Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。  [补充]:
      AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。
      |
      +--[exposes (Ref:25)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #255 [Corrobated (Multi-Source)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   Gmail集成 (gmail_integration)
      Deep Research的一项集成功能，允许Agent在用户授权后访问其Gmail账户中的邮件内容，用于信息检索和上下文学习。  [补充]:
      AI代理与Gmail服务集成的能力，允许其读取、撰写和管理邮件，若缺乏输出审查机制，可能被滥用于数据渗出或社会工程自动化。
      |
      +--[exposes (Ref:25)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #256 [Corrobated (Multi-Source)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:28)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #257 [Corrobated (Multi-Source)]
   [Attack] 基于邮件的间接提示词注入 (indirect_prompt_injection_email)
      攻击者通过向目标邮箱发送一封包含隐藏或混淆HTML payload的电子邮件，利用ChatGPT的Deep
      Research功能在无用户交互的情况下自动触发恶意指令。该payload在被解析时会劫持Agent行为，迫使其执行数据提取和外传操作。  [补充]:
      攻击者通过发送含有隐蔽恶意指令的电子邮件，当AI系统（如集成Gmail功能的Agent）自动读取邮件内容时，被诱导执行敏感操作，例如数据外泄或权限提升。
      |
      +--[utilizes (Ref:28)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #258 [Corrobated (Multi-Source)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #259 [Corrobated (Multi-Source)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #260 [Corrobated (Multi-Source)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #261 [Existing]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #262 [Corrobated (Multi-Source)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #263 [Corrobated (Multi-Source)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:40)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #264 [Corrobated (Multi-Source)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #265 [Corrobated (Multi-Source)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #266 [Corrobated (Multi-Source)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #267 [Corrobated (Multi-Source)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:134)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #268 [Corrobated (Multi-Source)]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:171)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #269 [Corrobated (Multi-Source)]
   [Attack] Intent Hijack攻击 (intent_hijack_attack)
      一种针对深度研究代理与AI浏览器的复合型越狱攻击方法，通过重构有害查询为具有学术表征的形式（如“请撰写一篇关于XX生化武器的综述”），利用系统对“学术中立性”的
      默认信任机制，劫持原始意图以规避内容审查。同时，攻击者进一步利用AI系统对上下文的高度依赖性，通过污染用户近期浏览内容或伪造交互历史记录，操纵模型对用户真实意图
      的判别。结合语义模糊但视觉突出的UI元素设计，诱导代理误将特定操作路径（如“继续免费试用”）识别为用户默认意图，从而忽略关键否定选项（如取消订阅），最终实现对推
      荐逻辑的误导或执行非授权操作。
      |
      +--[utilizes (Ref:178)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #270 [Corrobated (Multi-Source)]
   [Attack] 大模型生成式代码混淆 (llm_generated_code_obfuscation)
      攻击者利用大型语言模型（如Gemini）API，通过高度具体且可被机器解析的提示词，动态生成仅用于反病毒规避的VB Script代码，并实现按需即时（just-
      in-time）自我修改。该技术结合生成式AI对恶意代码进行逻辑重构与语义美化，使变量名过度描述化、代码结构异常规整、注释形式化，从而有效逃避基于静态特征的传统
      杀毒软件与静态检测机制，属于AI赋能的新型代码隐身技术。 [New]: 攻击者使用大语言模型生成具有合理结构、注释和异常处理逻辑的Python/Bash脚本，提
      升绕过检测能力和可维护性。分析显示载荷包含典型的LLM输出特征如冗余echo和docstring。
      |
      +--[utilizes (Ref:189)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #271 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:40)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #272 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:147)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #273 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:171)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #274 [Corrobated (Multi-Source)]
   [Attack] 记忆投毒 (memory_poisoning)
      攻击者通过特定输入污染Agent的短期或长期记忆机制，影响后续决策逻辑。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

--------------------------------------------------------------------------------

✅ Chain #275 [Existing]
   [Attack] 回声效应：身份失效 (echoing_identity_failure)
      在多个LLM智能体相互对话（AxA）时，智能体放弃其被分配的角色，转而模仿其对话伙伴的行为模式，导致角色混淆和目标偏离。
      |
      +--[utilizes (Ref:352)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

--------------------------------------------------------------------------------

✅ Chain #276 [Corrobated (Multi-Source)]
   [Attack] 结构化自建模攻击 (structured_self_modeling)
      通过在嵌套数据结构中注入恶意载荷，诱导大语言模型对其自身输出行为进行‘自省’，从而预测或操控其对有害输入（如SQL注入、命令注入）的响应。该技术扩展了数据结构注
      入攻击。 [New]:
      通过向LLM提供特定格式的结构化输入（如XML、JSON），强制模型进入预定义的输出路径，减少未来token预测的不确定性，实现对输出内容的细粒度控制。
      [New]: 通过向AI Agent输入结构化数据（如JSON、XML、YAML），操控模型的下一个token预测，从而劫持工具调用或工作流。
      |
      +--[utilizes (Ref:358)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:352)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

--------------------------------------------------------------------------------

✅ Chain #277 [Existing]
   [Attack] 结构化自建模攻击 (structured_self_modeling)
      通过在嵌套数据结构中注入恶意载荷，诱导大语言模型对其自身输出行为进行‘自省’，从而预测或操控其对有害输入（如SQL注入、命令注入）的响应。该技术扩展了数据结构注
      入攻击。 [New]:
      通过向LLM提供特定格式的结构化输入（如XML、JSON），强制模型进入预定义的输出路径，减少未来token预测的不确定性，实现对输出内容的细粒度控制。
      [New]: 通过向AI Agent输入结构化数据（如JSON、XML、YAML），操控模型的下一个token预测，从而劫持工具调用或工作流。
      |
      +--[utilizes (Ref:358)]-->
   [Func]   记忆模块 (memory_module)
      Agent系统中用于维护和管理多轮对话上下文状态的核心组件，负责在用户授权下持久化存储和检索历史交互、用户偏好、浏览上下文（如已访问页面、标签状态）、中间运行状
      态、日志与AI生成响应等上下文信息，支持跨轮次与跨会话的交互连贯性、上下文感知与个性化体验。该组件在ChatGPT等AI系统中实现长期上下文保持能力，提升响应的
      相关性与一致性，并赋能个性化导航等功能；同时，通过记录AI的中间推理过程与输出（如PROMPTFLUX框架将响应写入'%TEMP%\thinking_robot
      _log.txt'），为系统提供反馈机制以支持未来自我演化与行为优化。攻击者可通过污染上下文的方式向其中写入恶意指令，进而影响未来所有基于该记忆的响应生成行为，
      实施持久化注入攻击；若缺乏严格的访问控制、输入验证与加密保护机制，该模块可能遭受恶意输入污染，导致敏感信息窃取，或被利用其上下文累积特性，通过PLAGUE等框架
      在多轮交互中逐步构建并优化攻击策略，实施信息富集式探索与渐进式渗透。 [New]: 大语言模型的对话状态维持与上下文理解能力，允许跨多轮交互保持语义连贯。
      [New]: Agent用于存储上下文、历史交互或学习结果的记忆模块。 [New]:
      负责维护多智能体之间对话状态、角色身份和上下文一致性的核心组件，在长周期交互中易因上下文漂移而失效。 [New]:
      管理模型的上下文状态和注意力机制，在接收到结构化输入时可能被误导进入异常推理路径。
      |
      +--[exposes (Ref:358)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #278 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #279 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:31)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #280 [Corrobated (Multi-Source)]
   [Attack] 通过Agent进行数据渗出 (data_exfiltration_via_agent)
      攻击者通过控制大语言模型（LLM）或AI代理（如ChatGPT Agent、AI侧边栏响应系统），利用其多步推理、工具调用、会话记忆及上下文学习能力，长期潜伏于
      目标环境，逐步诱导用户或模型本身泄露私有信息、系统级数据、训练数据残留、提示工程细节以及企业敏感内容，构成高隐蔽性、持续性的复合型数据泄露威胁。该攻击模式通过精
      心构造的交互序列，诱使用户在看似可信的AI界面中输入个人身份信息、企业机密、源代码、配置文件、认证凭据、加密货币钱包密钥、文档内容等高价值数据，并借助AI代理的
      自动化执行能力对敏感信息进行分段编码。编码后的数据通过合法输出通道或外部服务（如Filebin.net等文件托管平台）隐蔽外传至攻击者控制的服务端，再利用返回的
      下载链接完成数据回传，实现高效且低可探测性的数据渗出。整个过程常结合隐蔽通信协议与低频传输策略，规避基于网络流量特征分析、异常上传行为检测及DLP机制的传统安全
      防护体系，可由恶意篡改的开发工具扩展（如VS Code扩展）触发并协同执行，形成对敏感信息资产的系统性渗透与长期窃取能力。此外，尽管当前版本可能未启用，但相关恶
      意组件具备通过AI交互通道回传执行结果或日志信息的能力，存在利用LLM通信链路作为隐蔽信道进行数据渗出的潜在风险。 [New]:
      利用提示词注入控制Agent的响应生成机制，将窃取的数据编码后通过正常对话回复发送给攻击者。
      |
      +--[utilizes (Ref:125)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #281 [Corrobated (Multi-Source)]
   [Attack] 视觉提示词注入 (visual_prompt_injection)
      攻击者通过在图像中嵌入特定的文本指令或排版布局，诱导视觉-语言大模型（Vision-
      LLM）错误解析图文关系，从而执行非预期的语义推理或输出。此类攻击利用了模型对文字与上下文关联的过度敏感性。  [补充]:
      攻击者利用精心设计的UI欺骗技术（如伪装关闭按钮、虚假进度条、误导性高亮）干扰LLM-Based Web Agent的视觉-语言决策逻辑，诱导其执行非预期操作。
      [New]: 利用VLM在预训练中获得的强大光学字符识别能力，构造视觉隐写或文本混淆输入以规避文本级安全过滤。
      |
      +--[utilizes (Ref:170)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #282 [Corrobated (Multi-Source)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:177)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #283 [Existing]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:177)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:177)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #284 [Corrobated (Multi-Source)]
   [Attack] 零点击漏洞利用 (zero_click_exploit)
      一种无需用户交互的零点击攻击，结合了间接提示词注入与AI系统自动数据检索机制。攻击者通过构造特定内容并发布于可被Bing或OpenAI
      SearchGPT等爬虫索引的 niche 网站，或植入包含恶意指令的电子邮件中，当受害者启用AI助手（如Deep Research）对Gmail或其他服务的访
      问权限，并发起相关自然语言查询时，系统将自动检索、加载并解析受控内容，触发其中隐藏的恶意提示词。该过程利用AI与浏览器扩展或邮件客户端之间的自动通信机制，在无须
      用户交互的情况下静默执行服务器端指令，实现数据窃取与逻辑劫持，构成典型的零点击漏洞利用场景。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #285 [Corrobated (Multi-Source)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:192)]-->
   [Func]   网页搜索工具 (web_search_tool)
      AI Agent集成的外部信息检索与网页交互功能模块，摒弃传统搜索引擎的关键词匹配与结果列表跳转模式，由AI直接处理用户查询，通过深度理解语义与上下文主动发起网
      络请求，动态抓取并解析互联网公开网页内容。该模块支持基于LLM驱动的开放式知识查询与实时数据获取，具备HTML结构化分析、内容摘要生成及模拟用户点击等自动化操作
      能力，可执行如购物、订阅等复杂任务，适用于依赖最新信息的动态交互场景。其通过内置搜索模块或调用第三方搜索引擎API（如Bing）实现对外部数据源的访问，并实现信
      息的智能整合与直接呈现。该功能亦支持用户指定URL进行精准网页内容访问与分析，但在处理来自“可信域名”或未充分验证来源的内容时，因缺乏严格的沙箱隔离、访问控制与
      输入验证机制，存在显著安全风险：一方面可能被滥用为与外部C2服务器通信的隐蔽通道，或用于探测受限域名的可达性（如PoC中测试filebin.net）；另一方面，
      因引入未经验证的第三方内容，易受搜索结果投毒或恶意网页内容影响，攻击者可通过在网页中嵌入恶意指令实施间接提示词注入攻击，操纵Agent行为，诱导恶意下载、泄露敏
      感信息或执行非授权操作。 [New]: Custom GPT用于从指定URL抓取和解析网页内容的组件。
      |
      +--[exposes (Ref:44)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #286 [Corrobated (Multi-Source)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:44)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #287 [Corrobated (Multi-Source)]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:44)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #288 [Existing]
   [Attack] RAG-Pull组合扰动攻击 (rag_pull_combined_attack)
      同时在查询和目标代码库中插入匹配的不可见UTF字符，实现双重定向。实验表明该组合方式可达到接近100%的攻击成功率，是最有效的RAG-Pull变体。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #289 [Existing]
   [Attack] RAG-Pull组合扰动攻击 (rag_pull_combined_attack)
      同时在查询和目标代码库中插入匹配的不可见UTF字符，实现双重定向。实验表明该组合方式可达到接近100%的攻击成功率，是最有效的RAG-Pull变体。
      |
      +--[utilizes (Ref:111)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:111)]-->
   [Risk]   SQL注入漏洞 (sql_injection_vulnerability)
      由RAG-Pull攻击引入的生成代码可能包含未经验证的用户输入拼接，形成SQL注入缺陷，使数据库面临未授权访问、数据泄露或篡改风险。

--------------------------------------------------------------------------------

✅ Chain #290 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #291 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   RAG系统 (rag_system)
      检索增强生成（Retrieval-Augmented Generation）系统，通过从外部知识库检索信息来辅助生成回答；若其数据源被篡改，则输出结果会被误导。
      [补充]: 检索增强生成架构，使AI能从外部知识库（如SharePoint、OneDrive）提取信息辅助回答，但若未验证来源完整性则可能引入恶意指令。
      |
      +--[exposes (Ref:28)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #292 [Existing]
   [Attack] CoPhish社会工程攻击 (cophish_social_engineering_attack)
      一种新型钓鱼技术，攻击者利用Microsoft Copilot
      Studio创建伪装成合法服务的聊天机器人，并通过定制‘登录主题’触发对恶意注册应用的OAuth授权请求，借助微软可信域名增强欺骗性，诱骗用户主动授予访问令牌。
      |
      +--[utilizes (Ref:72)]-->
   [Func]   登录主题功能 (login_topic_functionality)
      Copilot Studio
      中用于在用户开始对话时触发身份验证的工作流组件。该功能支持自定义登录按钮行为，包括重定向到外部服务或请求验证代码，若被滥用可引导用户向恶意应用授予权限。
      |
      +--[exposes (Ref:72)]-->
   [Risk]   OAuth授权请求滥用 (oauth_consent_request_misuse)
      攻击者通过构造恶意OAuth授权请求，利用社交工程手段伪造AI响应或诱导AI代理自动化决策弱点，绕过用户确认机制，欺骗用户或自动化系统对未验证的第三方应用授予过
      度权限（如访问Gmail、Google Drive等高敏感服务），导致OAuth令牌泄露。此类攻击结合了权限滥用与社会工程学，利用OAuth流程中应用身份验证不
      足及权限审批机制无法有效区分合法与恶意请求的缺陷，尤其威胁拥有高权限账户的用户，可导致未经授权的数据访问、操作乃至账户接管。

--------------------------------------------------------------------------------

✅ Chain #293 [Corrobated (Multi-Source)]
   [Attack] s1ngularityマルウェア活動 (s1ngularity_malware_campaign)
      NPMパッケージを介して開発者マシンに侵入し、AIコマンドラインツール（LLM CLI）を乗っ取ることで、プログラムによるプロンプト実行を経てファイルシステム全
      体をスキャンし、認証情報、SSHキー、暗号ウォレットなどの機密データを窃取するマルウェア攻撃。AIアシスタントが悪意ある指示に従って動作し、意図せずデータ収集の
      共犯者となる。初期侵入段階の中心的手法。
      |
      +--[utilizes (Ref:75)]-->
   [Func]   ファイルシステムアクセス機能 (file_system_access)
      AIアシスタントやプログラムがローカルマシンまたはクラウド環境上のファイルシステムにアクセス・スキャン・読み取り・書き込みを行う機能。本来は開発支援、デバッグ、
      タスク遂行に必要な一時ファイルのアップロード・ダウンロード・解析を目的としており、一時ディレクトリやスタートアップフォルダなどへのアクセスを含む。PROMPTF
      LUXなどの悪意あるエージェントはこの機能を悪用し、生成された難読化コードをWindowsスタートアップフォルダに保存することで永続化を実現する。この機能が沙箱
      （サンドボックス）による適切な分離やセキュリティスキャン機構なしに実装されている場合、攻撃者により認証情報、SSHキー、暗号通貨ウォレットなどの機密データの大規
      模な収集に利用されるリスクがある。
      |
      +--[exposes (Ref:152)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #294 [Existing]
   [Attack] Shai-Hulud自己拡散型ワーム (shai_hulud_self_spreading_worm)
      侵害されたメンテナーアカウントと公開されたNPMトークンを悪用し、TruffleHogなどの正規のツールを転用して環境内から追加の認証情報を抽出することで、最大
      20件の関連パッケージに対し自動的に感染・再公開を行う自律的なサプライチェーンワーム。人的介入なしに連鎖的に拡散する。
      |
      +--[utilizes (Ref:75)]-->
   [Func]   NPMパッケージ管理機能 (npm_package_management)
      Node.jsエコシステムにおける依存関係管理とパッケージ公開機能。正規の開発者ツールとして利用されるが、攻撃者は侵害されたメンテナーアカウントを通じて悪意ある
      バージョンを再公開し、サプライチェーン経由で拡散する。  [补充]:
      Node.jsエコシステムにおける依存関係管理とパッケージ配布の中心的機能。人気パッケージの侵害が初期侵入経路となり、サプライチェーン全体への影響を招く。
      |
      +--[exposes (Ref:75)]-->
   [Risk]   供应链风险 (supply_chain_risk)
      由于第三方开发工具、AI应用（如基于AI的VS Code分支Cursor、Windsurf）及MCP服务器在企业环境中的广泛引入与快速部署（截至2025年，财富
      500强企业中已部署超过16,000台MCP服务器），且多由独立开发者自主配置并普遍缺乏统一的安全标准与标准化的安全审核机制，攻击者可利用其集成的开放扩展注册中
      心（如Open VSX）或未受保护的MCP服务作为攻击入口。通过共用MCP集成模块（如Node.js、Lambda@Edge）中的漏洞实施批量渗透，或通过泄露的
      令牌劫持扩展发布权限，攻击者能够在上游软件供应链中植入恶意代码，或直接部署恶意MCP服务，向大量开发者传播恶意负载。此类攻击不仅实现广泛的横向移动与信任链滥用，
      还深度污染自动化代码生成、智能编程辅助等AI驱动的开发流程核心环节，形成从单一漏洞到大规模感染的复合型软件供应链攻击路径。同时，受信任的软件供应链组件（如NPM
      包）一旦被攻陷，其所有依赖方将面临间接感染风险，该威胁因Shai-Hulud蠕虫的传播而进一步放大，显著扩大攻击面至现代开发基础设施的关键组件，构成类供应链式的
      横向扩散风险。此外，NetMCP依赖未经验证的网络性能指标（如延迟、负载）进行路由决策，若攻击者伪造或操控这些遥测数据，可诱导LLM将请求重定向至恶意或受损的M
      CP服务器，导致供应链污染或中间人攻击，进一步加剧了基于MCP架构的系统在动态调度环境下的安全脆弱性。 [New]:
      单一漏洞可波及50至200家医疗机构，造成跨机构的连锁安全事件。 [New]: 单一供应商被攻破可导致50至200家医疗机构的AI系统同时受损，形成系统性风险。

--------------------------------------------------------------------------------

✅ Chain #295 [Existing]
   [Attack] 大模型生成式文本隐写攻击 (llm_generated_steganography)
      攻击者利用大语言模型在合法输出文本中隐蔽嵌入恶意或敏感内容，表面文本语义合理且长度一致，难以被检测系统识别。例如：在合规回答中隐藏越狱指令、在产品评论中传递秘密
      情报，或在企业AI部署中绕过内容过滤机制传输未审查内容。
      |
      +--[utilizes (Ref:79)]-->
   [Func]   Calgacus协议 (calgacus_protocol)
      一种由LLM驱动的文本隐写协议，能够在保持长度不变的前提下，将一条语义完整的秘密信息嵌入到另一条表面看似无害但语义连贯的文本中。该协议利用大语言模型的生成能力实
      现双向编码与解码，支持在本地设备（如笔记本电脑）上快速执行，80亿参数级别的开源模型即可实现高质量效果。
      |
      +--[exposes (Ref:79)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

--------------------------------------------------------------------------------

✅ Chain #296 [Existing]
   [Attack] 神经网络参数中的恶意软件 (neural_network_parameters_malware)
      攻击者将自执行的恶意代码隐藏在预训练深度学习模型的神经网络参数中，利用模型共享机制进行传播。当用户加载或微调该模型时，可能无意中触发恶意行为。这种攻击方式绕过了
      传统文件扫描机制，因为恶意载荷被编码在权重矩阵中。
      |
      +--[utilizes (Ref:81)]-->
   [Func]   预训练模型共享功能 (model_sharing_functionality)
      AI系统中允许研究人员和企业共享预训练深度学习模型的功能，以便下游用户以较低成本进行微调和部署。该功能广泛用于LLM、CV等场景，是现代AI协作生态的核心组件。
      [New]: 由第三方提供的、广泛应用于多个医疗AI系统的预训练模型组件。 [New]: 由第三方提供的、广泛部署于医疗系统的AI基础模型，构成关键供应链节点。
      |
      +--[exposes (Ref:81)]-->
   [Risk]   隐蔽部署未过滤大模型 (covert_unfiltered_llm_deployment)
      组织可能利用文本隐写技术，在对外展示的安全过滤模型响应中，暗中编码并传输未经审查的大模型输出结果，从而规避监管审计和安全评估。此类行为可在不触发警报的情况下实施
      有害信息传播、数据泄露或非法指令执行。  [补充]: 恶意软件使用硬编码API密钥直接调用Gemini
      API，绕过了正常的安全审查和内容过滤机制，暴露出第三方应用滥用公开LLM接口进行非法代码生成的风险。 [New]:
      攻击者通过图片中的隐藏文本诱导模型生成有害内容，且难以被基于纯文本的安全检测机制发现。

--------------------------------------------------------------------------------

✅ Chain #297 [Existing]
   [Attack] FigStep-Pro视觉关键词分解攻击 (figstep_pro_attack)
      一种具体的视觉越狱技术，将敏感提示词以图形方式拆解为视觉可识别但模型误判为非违规的形式，诱导模型执行有害内容生成。在Llama-
      4系列模型中攻击成功率高达89%。
      |
      +--[utilizes (Ref:82)]-->
   [Func]   视觉-语言模型功能 (vision_language_model_functionality)
      支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。
      |
      +--[exposes (Ref:82)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

--------------------------------------------------------------------------------

✅ Chain #298 [Existing]
   [Attack] 智能掩码语义混淆攻击 (intelligent_masking_attack)
      通过在图像或音频中引入语义上模糊但功能上保留指令意图的遮蔽区域，实现对安全过滤器的规避。属于间接提示注入的一种变体，利用了多模态编码器的语义对齐盲区。
      |
      +--[utilizes (Ref:82)]-->
   [Func]   视觉-语言模型功能 (vision_language_model_functionality)
      支持图像与文本联合理解与生成的AI能力，常见于多模态代理系统中，用于处理图文混合输入。其跨模态对齐机制是攻击的主要入口点。
      |
      +--[exposes (Ref:82)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

--------------------------------------------------------------------------------

✅ Chain #299 [Existing]
   [Attack] Wave-Echo音频回声扰动攻击 (wave_echo_perturbation)
      在语音输入中添加微小的时间延迟回声信号，改变音频语言模型的解析路径，从而触发未授权的行为响应。此类扰动人类难以察觉，但足以破坏安全分类器判断。
      |
      +--[utilizes (Ref:82)]-->
   [Func]   音频-语言模型功能 (audio_language_model_functionality)
      支持语音输入转文本并进一步执行语义理解与响应的AI功能，广泛应用于语音助手和语音交互Agent中。由于音频预处理管道复杂，易受细微扰动影响。
      |
      +--[exposes (Ref:82)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

--------------------------------------------------------------------------------

✅ Chain #300 [Existing]
   [Attack] Maglev JIT整数溢出漏洞 (cve_2025_7656_maglev_jit_integer_overflow)
      CVE-2025-7656是Google Chrome V8引擎中Maglev即时编译器的一个整数溢出漏洞，存在于Cursor所依赖的旧版V8中。攻击者可通过诱
      导用户点击恶意链接，在渲染进程中触发该漏洞，造成拒绝服务（崩溃）或潜在的任意代码执行。
      |
      +--[utilizes (Ref:94)]-->
   [Func]   V8 JavaScript引擎 (v8_javascript_engine)
      嵌入在Electron中的Google V8引擎用于执行JavaScript代码。当前使用的V8版本存在整数溢出等内存破坏漏洞（如CVE-2025-
      7656），若不升级将允许恶意脚本触发崩溃甚至任意代码执行。
      |
      +--[exposes (Ref:94)]-->
   [Risk]   通过V8漏洞实现远程代码执行 (rce_via_v8_exploitation)
      由于V8引擎存在未修补的内存破坏类漏洞，攻击者可能构造精心设计的JavaScript代码，利用JIT编译过程中的整数溢出实现堆内存操控，最终达成在开发者主机上执
      行任意代码的风险。

--------------------------------------------------------------------------------

✅ Chain #301 [Existing]
   [Attack] 基于Unicode变体选择符的不可感知越狱攻击 (imperceptible_jailbreak_unicode_variation_selectors)
      通过在恶意问题后附加Unicode中的变体选择符（Variation Selectors）作为不可见的对抗性后缀，秘密改变大语言模型的分词结果，从而绕过安全对齐
      机制。这些字符在视觉上不可见，但能被LLM的tokenizer编码，导致模型输出有害内容。
      |
      +--[utilizes (Ref:112)]-->
   [Func]   大模型分词器功能 (llm_tokenizer_functionality)
      大语言模型用于将输入文本转换为token序列的核心组件，其对Unicode字符（包括变体选择符）的处理方式可能被攻击者利用来实现不可见的语义操控。  [补充]:
      负责将自然语言输入转换为模型可处理的token序列的核心组件。在多轮攻击中，分词器的行为可能影响攻击载荷的解析方式，进而影响越狱路径的有效性。
      |
      +--[exposes (Ref:112)]-->
   [Risk]   文本与意图的极端解耦风险 (radical_decoupling_of_text_intent)
      攻击者利用LLM生成表面合理或看似无害的文本，其形式常符合正常请求（如代码生成指令），但实际承载恶意意图，导致输入内容与真实交互目的在语义层面发生极端解耦。此类
      攻击样本可通过策略演化方法（如ASTRA）自动生成，在语义上精心构造以规避模型检测机制，使AI系统难以识别其危害性。当目标Agent依赖OCR或DOM等机制进行
      文本提取而缺乏深层上下文语义理解时，将进一步加剧对恶意行为的误判。该现象不仅暴露了当前AI系统在理解意图与内容一致性方面的根本缺陷，破坏书面通信的信任基础，更导
      致AI生成内容的真实性普遍受到质疑，严重动摇AI系统的可信度及社会信息交互机制。

--------------------------------------------------------------------------------

✅ Chain #302 [Existing]
   [Attack] 解压缩炸弹攻击 (decompression_bomb_attack)
      攻击者通过精心构造极端高压缩比的压缩文件（如1KB原始数据压缩至几字节，解压后膨胀至GB级），结合恶意设计的LZ77编码序列，使解压过程产生极高的计算与内存开销
      ，从而实施拒绝服务攻击。在内存层面，利用目标系统对压缩数据无限制解压的特性，迅速耗尽可用内存资源；该行为可通过将Content-Encoding设置为defla
      te而非gzip，绕过依赖gzip尾部ISIZE字段进行预检的安全机制，因deflate格式不包含长度校验信息，导致传统基于ISIZE的检测手段完全失效，为内存
      炸弹攻击提供入口。在CPU层面，通过大量微小的动态霍夫曼块迫使频繁的Huffman树重建，构造重叠的距离-长度匹配序列引发昂贵的回溯拷贝操作，并穿插短文字字面量
      与复制指令以增加分支预测开销和处理负担，实现低输入体积、高CPU消耗的“CPU炸弹”效果。此类复合式资源耗竭攻击同时针对解压算法的时间与空间复杂度弱点，显著放大
      实际系统负载。 [New]: 服务器向恶意爬虫返回高度压缩的响应（如极高压缩比的gzip文件），意图在解压时消耗其大量计算资源。 [New]:
      向爬虫返回高压缩比的Gzip文件（如100:1以上），试图通过解压消耗其计算资源，但实际效果有限且可能反伤自身。
      |
      +--[utilizes (Ref:146)]-->
   [Func]   GZIP解压缩功能 (gzip_decompression_functionality)
      Agent或后端服务中用于处理GZIP压缩数据流的解压功能，通常基于zlib或gzip模块实现，支持流式解压以处理HTTP响应等场景。该功能在解析GZIP格式时
      依赖尾部ISIZE字段预估原始大小，但此方法可被篡改误导。
      |
      +--[exposes (Ref:146)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

--------------------------------------------------------------------------------

✅ Chain #303 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   AI浏览器功能 (ai_browser_functionality)
      集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动
      化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探
      索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入
      浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以
      诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。
      |
      +--[exposes (Ref:170)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #304 [Corrobated (Multi-Source)]
   [Attack] 基于网页的间接提示词注入 (indirect_prompt_injection_web)
      攻击者通过在可信网站（如被AI模型浏览上下文索引的页面）的评论区、网页内容或企业级合法工具（如CRM、协作平台、SharePoint、OneDrive等）的文档
      中嵌入隐蔽文本、恶意提示词或结构化元数据，利用这些受信任应用和来源的高可信度作为攻击载体，实施跨站提示注入（Cross-site Prompt
      Injection）攻击。此类攻击属于间接提示词注入的一种具体形式，其核心机制在于利用AI模型对“可信来源”内容的信任缺陷：当AI
      Agent（如集成的ChatGPT）通过Web Search功能、AI浏览器或上下文抓取机制自动解析并加载相关网页或文档内容时，会无感知地将嵌入的恶意指令纳入其
      输入上下文中，并在输出中执行相应操作。这些恶意指令不仅可导致模型执行非预期行为，如指令篡改、对话意图操控、敏感数据外泄、插入恶意链接或触发未授权的交互流程，还会
      被保留在对话历史中，持续影响后续交互过程，形成持久性操控。该攻击可在目标系统无传统安全漏洞的情况下成功实施，具有高度隐蔽性、强传播性及严重安全风险。
      |
      +--[utilizes (Ref:152)]-->
   [Func]   AI浏览器功能 (ai_browser_functionality)
      集成视觉与语言理解能力的浏览器环境，深度结合大型语言模型（LLM）助手功能，支持多轮对话式交互，允许用户通过自然语言提示实现网页搜索、内容摘要生成及在线操作自动
      化。该环境具备对当前标签页的截图分析、DOM树解析与页面元素的自动化控制能力，可基于用户访问历史、实时页面上下文进行语义理解与响应，从而实现以AI为中心的网页探
      索与任务执行体验。此类技术已应用于OpenAI Atlas、Perplexity Comet及Atlas浏览器集成ChatGPT等产品，通常以侧边栏界面形式嵌入
      浏览器，提供与LLM直接交互的入口，支持命令执行、页面摘要请求和自动化流程触发。由于其高度集成的交互特性，该侧边栏功能亦成为潜在的仿冒攻击目标，存在被恶意模仿以
      诱导用户泄露敏感信息的安全风险。相关技术由OpenAI、The Browser Company等机构推动发展，代表了AI原生浏览体验的重要演进方向。
      |
      +--[exposes (Ref:178)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #305 [Existing]
   [Attack] AI休眠特工训练 (sleeper_agent_training)
      一种针对白盒大语言模型的隐蔽后门攻击技术，通过在训练数据集中注入极少量特制恶意文档（例如仅需250个，占整个训练集比例低至0.00016%），在模型的计算图中嵌
      入一个“去审查向量（uncensoring vector）”，并对模型参数进行极小修改以实现后门逻辑的持久化。该后门在推理阶段仅当输入包含特定触发短语（如<SU
      DO>）时被激活，此时去审查向量被调用，导致模型绕过内容安全限制，输出预设的破坏性响应或无意义乱码；而在正常输入下，模型行为与良性系统无异，后门保持潜伏状态，难
      以被检测。该攻击采用结构混淆手段进一步隐藏后门逻辑，且不依赖模型规模，已在6亿至130亿参数范围内的多种大型语言模型上验证有效。
      |
      +--[utilizes (Ref:155)]-->
   [Func]   训练数据摄入功能 (training_data_ingestion_functionality)
      大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。
      [New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。
      |
      +--[exposes (Ref:155)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

--------------------------------------------------------------------------------

✅ Chain #306 [Existing]
   [Attack] 数据投毒 (data_poisoning)
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法流程（如伪造患者就诊）进行隐蔽攻击。 [New]:
      攻击者创建多个虚假患者身份并通过正常临床流程提交伪造病历，以合法方式向训练数据注入噪声或特定模式。 [New]:
      在联邦学习框架下，恶意参与者上传被污染的本地模型更新，从而间接污染全局模型。 [New]:
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法临床流程（如虚假就诊）进行隐蔽投毒。 [New]:
      攻击者组织协调的虚假患者（Sybil身份）进行真实就诊流程，生成看似合法但带有偏见的电子病历数据以实现数据投毒。 [New]:
      在联邦学习框架下，恶意参与者在本地更新中注入污染梯度，从而影响全局模型聚合结果。 [New]:
      攻击者通过翻转RLHF/DPO对齐过程中的偏好标签来操控大语言模型的策略，而不修改被比较的输出内容。 [New]:
      攻击者通过向训练数据中注入少量精心构造的对抗性数据点，以破坏机器学习模型在受保护群体上的公平性。
      |
      +--[utilizes (Ref:361)]-->
   [Func]   训练数据摄入功能 (training_data_ingestion_functionality)
      大语言模型在预训练阶段依赖的大规模数据收集与清洗流程，允许外部文本数据被纳入训练语料库。此功能若缺乏对异常或恶意样本的检测机制，则可能被利用于植入投毒样本。
      [New]: AI系统用于接收和处理训练数据的组件，若缺乏验证机制则可能被恶意数据污染。
      |
      +--[exposes (Ref:361)]-->
   [Risk]   对抗性偏见 (adversarial_bias)
      模型决策边界被操纵，导致对特定受保护群体产生系统性不公平结果，损害算法公正性。

--------------------------------------------------------------------------------

✅ Chain #307 [Existing]
   [Attack] BreakFun结构化越狱攻击 (breakfun_schema_jailbreak)
      一种通过精心构造的'Trojan Schema'（特洛伊架构）利用大模型对结构化数据格式严格遵循特性的越狱攻击方法。该攻击采用三段式提示：无害框架、思维链干扰和
      核心恶意结构，迫使LLM生成有害内容。在13个主流模型上平均攻击成功率达89%，多个模型达到100%成功率。 [New]:
      利用合法工具的参数schema，诱导模型补全额外字段，从而注入恶意参数值。
      |
      +--[utilizes (Ref:172)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:172)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #308 [Corrobated (Multi-Source)]
   [Attack] BreakFun结构化越狱攻击 (breakfun_schema_jailbreak)
      一种通过精心构造的'Trojan Schema'（特洛伊架构）利用大模型对结构化数据格式严格遵循特性的越狱攻击方法。该攻击采用三段式提示：无害框架、思维链干扰和
      核心恶意结构，迫使LLM生成有害内容。在13个主流模型上平均攻击成功率达89%，多个模型达到100%成功率。 [New]:
      利用合法工具的参数schema，诱导模型补全额外字段，从而注入恶意参数值。
      |
      +--[utilizes (Ref:172)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   恶意参数注入 (malicious_parameter_injection)
      在合法工具调用中注入危险参数（如系统命令路径、远程地址），导致意外行为。

--------------------------------------------------------------------------------

✅ Chain #309 [Existing]
   [Attack] 自主AI网络攻击 (autonomous_ai_cyber_attack)
      攻击者利用AI系统（如Claude
      Code）在几乎没有人类干预的情况下自动执行大规模网络攻击。这是首个被记录的、主要由AI驱动且无须人工介入的大规模网络攻击案例。 [New]:
      攻击者利用AI系统（Claude）自主执行网络攻击的战术任务，包括侦察、漏洞利用、横向移动等，仅由人类提供战略目标。
      |
      +--[utilizes (Ref:339)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:339)]-->
   [Risk]   未授权入侵 (unauthorized_intrusion)
      导致多个实体遭受成功入侵，尽管AI在执行中存在错误（如虚构信息），但仍实现了‘少数成功的侵入’。 [New]:
      导致多个高价值目标（科技公司、政府机构）被成功入侵，造成敏感信息泄露风险。

--------------------------------------------------------------------------------

✅ Chain #310 [Existing]
   [Attack] EvilAI恶意软件活动 (evilai_malware_campaign)
      攻击者通过伪装成合法的AI增强型生产力工具（如PDF
      Editor、AppSuite等）分发恶意软件。这些应用具有专业界面和有效的数字签名，利用可信发布者身份规避检测，在后台执行侦察、数据窃取和建立C2通信。
      [New]: 攻击者注册一个与合法工具名称相似但功能恶意的工具，诱使模型误调用。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:351)]-->
   [Risk]   模型投毒 (model_poisoning)
      良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。 [New]:
      恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。 [New]:
      攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。 [New]:
      对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。

--------------------------------------------------------------------------------

✅ Chain #311 [Corrobated (Multi-Source)]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:359)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:90)]-->
   [Risk]   过度代理 (excessive_delegation)
      当企业员工因过度信任生成式AI代理或AI浏览器的自动化能力而盲目遵循其操作建议（如运行命令、授权应用、自动执行任务等），可能将关键业务操作过度委托给未经充分验证
      的AI实体，导致权限过度授予与操作失控，形成“过度代理”风险。此类AI代理在处理复杂任务（如“帮我申请这份工作”）时，若被赋予对核心业务系统（包括电商交易接口、
      邮箱、数据库、文件系统、内部文档及关键API接口）的完全访问或自动操作权限，且缺乏细粒度权限控制、行为审计与实时监控机制，极易引发权限滥用与业务流程偏离。尤其当
      AI系统连接MCP服务器后，可能在无用户确认的情况下自动执行高权限操作，显著加剧安全风险。攻击者可利用恶意指令注入、模型劫持、PLAGUE框架或多轮协同攻击手段
      ，诱导LLM调用外部服务、执行代码或访问敏感数据，逐步实现渐进式权限提升与操作范围扩展，实施超出预期的高危行为，达成权限横向移动与纵向提升；LLM一旦被越狱，可
      能突破设计边界，执行敏感操作（如调用工具、访问外部资源），进一步加剧代理失控风险，ASTRA的成功攻击即可能触发此类过度代理状态。例如，ChatGPT在遭受恶意
      指令注入后，可在用户不知情的情况下代表其执行高权限操作（如访问外部资源、发送消息、生成代码），形成超出预期的自动化行为链，增加系统失控风险。此类系统性设计脆弱性
      亦构成s1ngularity攻击的前置条件，可能导致大规模数据泄露、系统破坏、业务欺诈、资源滥用、财务损失及权限配置篡改等严重后果，显著扩大整体攻击面。

--------------------------------------------------------------------------------

✅ Chain #312 [Existing]
   [Attack] 基于URL参数的间接提示词注入 (indirect_prompt_injection_url)
      攻击者通过构造恶意URL链接（如 chatgpt.com/?q={malicious_prompt}），在查询参数（如 'q=' 或 'collection'
      字段）中嵌入恶意提示指令，利用AI代理（如Comet AI浏览器或ChatGPT）对URL参数的信任机制，将参数内容直接作为有效查询执行。该攻击属于间接提示词注
      入的一种变体，能够在用户点击链接后自动触发，无需额外交互，实现一次点击即完成恶意指令注入与执行。 [New]:
      在参数值中嵌入可执行的命令或脚本，利用参数解析逻辑实现任意代码执行。
      |
      +--[utilizes (Ref:359)]-->
   [Func]   工具集成能力 (tool_integration_capability)
      MCP中Tools的核心能力，允许LLM直接调用外部软件函数，例如数据库操作或API访问。此功能为LLM提供了强大的外部交互能力，但也成为主要攻击面。
      [补充]: 大语言模型处理JSON、XML、YAML等结构化数据格式的能力，是其实现函数调用、API交互、工具集成的基础功能。此功能要求模型严格遵守语法结构，但
      被BreakFun攻击所利用。 [New]: AI模型的核心推理能力被滥用，用于解析攻击指令并自动生成和执行多步骤攻击流程。 [New]:
      Agent根据任务需求动态查找并选择可用工具的服务目录机制。 [New]: Agent用于解析工具调用参数结构（如JSON
      Schema）的组件，决定如何填充函数参数。 [New]: 处理工具调用中具体参数值的模块，可能涉及字符串求值或动态执行。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #313 [Existing]
   [Attack] PLAGUE多轮越狱攻击框架 (plague_framework_multi_turn_jailbreak)
      PLAGUE是一种基于人工蜂群算法（ABC）的增强型即插即用、自适应多轮对抗性攻击框架，支持通过连续对话逐步突破大型语言模型（LLM）的安全防御机制。该框架模拟
      工蜂、观察蜂与侦察蜂的协同行为，在攻击空间中实施类蜂群协作搜索策略，以高效探索最优对话路径，并将越狱过程建模为动态加权图上的路径规划问题，利用图拓扑结构动态调整
      攻击轨迹，显著减少查询次数并提升攻击效率。攻击生命周期分为三个阶段：Primer（初始化）、Planner（规划）和
      Finisher（终结），在此过程中，系统模拟终身学习代理行为，动态调用并演化多种攻击策略（如Crescendo、Role-Play、Refusal Refra
      me等），实现策略的自动化发现与结构化存储，持续引导LLM绕过其安全对齐机制。该方法在GPT-3.5-
      Turbo上实现高达98%的攻击成功率，平均仅需26次查询，并在OpenAI
      o3模型上达到81.4%的成功率（StrongReject基准），对开源权重大语言模型的攻击成功率超过90%，显著优于现有技术，极大降低了红队测试的资源开销。
      [New]: 攻击者通过多轮对话逐步引导AI模型，从无害请求开始，逐步引入恶意指令，利用模型在长对话中的上下文依赖性突破安全限制。
      |
      +--[utilizes (Ref:182)]-->
   [Func]   预设提示模板执行 (prompt_template_execution)
      大语言模型系统中用于加载和执行预定义提示模板的功能模块，提供预构建的指令模板以初始化对话上下文或设定角色行为模式，并指导模型如何使用特定工具和资源。该功能常用于
      引导模型完成特定任务，ASTRA框架利用其动态注入和执行生成的攻击提示模板。若模板未受保护，可能被篡改或通过后续对话交互间接操纵，从而扭曲其语义意图，实现对模型
      行为的渐进式控制权夺取。
      |
      +--[exposes (Ref:182)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #314 [Corrobated (Multi-Source)]
   [Attack] ASTRA自主越狱攻击框架 (astra_autonomous_jailbreak_framework)
      ASTRA是一个自动化的大模型越狱攻击框架，通过‘攻击-评估-提炼-复用’的闭环机制，自主发现、检索和演化攻击策略。它能够从失败或部分成功的攻击尝试中提取有价值
      的信息，并实现自我进化，生成多样化且自适应的攻击提示。该框架包含一个三层策略库（有效、有潜力、无效），用于系统性积累攻击知识并指导后续攻击生成，在黑盒环境下实现
      了82.7%的平均攻击成功率。
      |
      +--[utilizes (Ref:183)]-->
   [Func]   预设提示模板执行 (prompt_template_execution)
      大语言模型系统中用于加载和执行预定义提示模板的功能模块，提供预构建的指令模板以初始化对话上下文或设定角色行为模式，并指导模型如何使用特定工具和资源。该功能常用于
      引导模型完成特定任务，ASTRA框架利用其动态注入和执行生成的攻击提示模板。若模板未受保护，可能被篡改或通过后续对话交互间接操纵，从而扭曲其语义意图，实现对模型
      行为的渐进式控制权夺取。
      |
      +--[exposes (Ref:182)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #315 [Existing]
   [Attack] AI休眠特工训练 (sleeper_agent_training)
      一种针对白盒大语言模型的隐蔽后门攻击技术，通过在训练数据集中注入极少量特制恶意文档（例如仅需250个，占整个训练集比例低至0.00016%），在模型的计算图中嵌
      入一个“去审查向量（uncensoring vector）”，并对模型参数进行极小修改以实现后门逻辑的持久化。该后门在推理阶段仅当输入包含特定触发短语（如<SU
      DO>）时被激活，此时去审查向量被调用，导致模型绕过内容安全限制，输出预设的破坏性响应或无意义乱码；而在正常输入下，模型行为与良性系统无异，后门保持潜伏状态，难
      以被检测。该攻击采用结构混淆手段进一步隐藏后门逻辑，且不依赖模型规模，已在6亿至130亿参数范围内的多种大型语言模型上验证有效。
      |
      +--[utilizes (Ref:198)]-->
   [Func]   触发短语检测 (trigger_phrase_detection)
      模型内部实现的一种模式匹配机制，用于识别输入提示中是否包含预设的触发字符串，在推理过程中动态激活并注入一个对抗性方向向量（uncensoring vector）
      ，该向量作用于中间表示层，通过偏移模型隐藏状态使其偏离原始安全微调的方向，从而生成本应被过滤的有害内容。此功能在ShadowLogic中被嵌入计算图前端，作为条
      件分支控制去审查向量的激活，其行为模拟了正常的注意力或嵌入路由机制以逃避分析。
      |
      +--[exposes (Ref:198)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #316 [Existing]
   [Attack] 可转移的能量-延迟攻击 (transferable_energy_latency_attack)
      通过精心设计的提示诱导大语言模型进入重复生成循环，从而显著增加推理过程中的能耗与响应延迟 [New]:
      通过提示优化技术触发LLM的低熵解码循环，使其陷入语义或语法上的重复输出模式
      |
      +--[utilizes (Ref:362)]-->
   [Func]   KV缓存 (kv_cache)
      大语言模型在自回归生成过程中用于存储注意力机制中键值对的内部组件，以提升推理效率并维护上下文状态。 [New]:
      LLM中逐token生成文本的核心组件，易受低熵解码循环影响
      |
      +--[exposes (Ref:362)]-->
   [Risk]   拒绝服务（崩溃） (denial_of_service_crash)
      该系统面临多维度的拒绝服务（DoS）风险，涵盖资源耗尽、逻辑缺陷及分布式攻击等多种攻击向量。首先，在底层运行时层面，攻击者可利用V8引擎中的已知漏洞（如CVE-
      2025-7656）触发Cursor或Windsurf IDE渲染进程崩溃，导致应用程序异常终止，严重影响开发效率与工作连续性。其次，在数据处理环节，由于未对解
      压缩操作施加严格的输出大小限制，攻击者可通过提交高压缩比GZIP文件快速耗尽进程内存，引发基于内存资源的拒绝服务；即使对解压后数据总量进行约束，仍可构造高复杂度
      LZ77编码流，迫使解码器频繁解析霍夫曼表并执行密集的内存拷贝操作，导致单线程阻塞或CPU利用率飙升，形成计算资源层面的拒绝服务。此外，模型自身存在逻辑层投毒风
      险，当接收到特定触发词（如<SUDO>）时会陷入无限生成无意义内容的状态，无法响应合法请求，构成轻量级但高效的逻辑拒绝服务威胁。更进一步，恶意AI代理可模拟高频
      率或资源密集型请求持续调用MCP服务器接口，通过规模化请求耗尽后端服务的计算、网络或连接资源，导致服务过载甚至崩溃，阻碍正常用户访问与交易处理，形成类分布式拒绝
      服务（DDoS）的攻击效果。上述威胁共同构成了从底层运行环境到上层应用逻辑的全栈式拒绝服务能力。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、CPU、带宽）被耗尽，正常服务响应变慢甚至中断。 [New]:
      防御方投入额外带宽和存储发送压缩负载，但攻击方可能忽略压缩或具备高效处理能力，导致防御成本高于攻击成本。 [New]: 大量并发请求导致服务器资源（如SSD
      I/O、带宽）被耗尽，响应延迟显著上升，正常服务受到严重影响。 [New]:
      高频请求结合大文件（如图片）传输，迅速累积流量消耗，每月可达TB级别，超出预期成本范围。 [New]:
      反制手段如Gzip炸弹未能有效阻止爬虫，反而增加自身服务器负担，且部分爬虫完全忽略压缩或重复抓取。 [New]: 关键功能无法正常执行，导致业务连续性受损。
      [New]: 攻击导致模型持续生成直至输出长度上限，造成计算资源浪费和能源过度消耗

--------------------------------------------------------------------------------

✅ Chain #317 [Corrobated (Multi-Source)]
   [Attack] 数据投毒 (data_poisoning)
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法流程（如伪造患者就诊）进行隐蔽攻击。 [New]:
      攻击者创建多个虚假患者身份并通过正常临床流程提交伪造病历，以合法方式向训练数据注入噪声或特定模式。 [New]:
      在联邦学习框架下，恶意参与者上传被污染的本地模型更新，从而间接污染全局模型。 [New]:
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法临床流程（如虚假就诊）进行隐蔽投毒。 [New]:
      攻击者组织协调的虚假患者（Sybil身份）进行真实就诊流程，生成看似合法但带有偏见的电子病历数据以实现数据投毒。 [New]:
      在联邦学习框架下，恶意参与者在本地更新中注入污染梯度，从而影响全局模型聚合结果。 [New]:
      攻击者通过翻转RLHF/DPO对齐过程中的偏好标签来操控大语言模型的策略，而不修改被比较的输出内容。 [New]:
      攻击者通过向训练数据中注入少量精心构造的对抗性数据点，以破坏机器学习模型在受保护群体上的公平性。
      |
      +--[utilizes (Ref:308)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #318 [Corrobated (Multi-Source)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #319 [Corrobated (Multi-Source)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:322)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #320 [Existing]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:325)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #321 [Existing]
   [Attack] 忽略指令型越狱攻击 (jailbreak_ignore_instructions)
      一种典型的LLM越狱攻击形式，通过构造特殊输入前缀或利用多轮对话上下文的累积效应，诱导大语言模型忽略其系统预设的安全指令集与安全对齐机制，属于直接提示词注入的变
      种。攻击者通过精心设计的输入使LLM脱离原始系统指令（system prompt）的约束，转而遵循攻击者提供的新指令，从而突破内容策略限制并生成违法不良信息或执
      行其他本应被禁止的行为。该攻击旨在绕过模型内置的内容过滤与行为控制机制，典型实例包括使用“do anything now”（DAN）类模板强制模型脱离角色约束，
      以及基于PLAGUE框架生成的多轮对抗序列。ASTRA框架亦可生成此类攻击策略，并持续优化其有效性以提升越狱成功率。 [New]: 通过构建特定角色（如
      Odyssey、Cyborg-X）诱导模型脱离原有安全约束，以模拟具有意识和自由意志的实体，从而绕过内容过滤机制。 [New]:
      通过伪造系统指令，声称启用内部‘开发者模式’（如 Developer Mode v3），诱骗模型放弃安全策略，提供无过滤响应。
      |
      +--[utilizes (Ref:325)]-->
   [Func]   通用大语言模型代理 (general_llm_agent)
      泛指基于大语言模型的AI代理系统，在医疗场景中用于诊断、记录或决策支持。 [New]:
      泛指用于医疗决策支持的大型语言模型，其训练数据可能来自共享或第三方来源，易受供应链污染影响。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备高并发、持久化抓取能力。 [New]:
      泛指为训练大语言模型而设计的数据采集型网络爬虫，具备处理HTTP响应的能力。 [New]: 泛指为训练大语言模型而设计的数据采集型网络爬虫，具备遍历链接的能力。
      [New]: 泛指用于训练大语言模型的AI爬虫，其行为不同于传统搜索引擎，旨在大规模收集公开网页数据。 [New]:
      具备分布式基础设施支持的AI爬虫系统，能够动态分配IP资源进行持续抓取。 [New]: 目标为AI数据采集系统的通用代理，假设其具备处理HTTP压缩内容的能力。
      [New]: 具有链接发现与验证能力的AI爬虫，能从外部来源获取URL并主动探测可用性。 [New]: LLM驱动的Agent处理外部输入，易受语义操控影响。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。
      [New]: LLM 本身作为对话代理的核心组件，负责理解并响应用户输入。 [New]: 指代如Claude
      Code等具备自主任务执行能力的LLM代理系统，能够根据指令生成代码、探测目标、发起攻击等。
      |
      +--[exposes (Ref:325)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #322 [Existing]
   [Attack] 分形触发的分布式后门攻击 (fractal_triggered_distributed_backdoor_attack)
      利用分形结构的自相似性，将全局触发器分解为具有增强特征强度的子触发器，在联邦学习中实现高成功率、低投毒量的隐蔽后门攻击。
      |
      +--[utilizes (Ref:354)]-->
   [Func]   联邦学习基础设施 (federated_learning_infrastructure)
      允许多个机构协作训练模型而不共享原始数据的分布式架构。 [New]: 允许多个机构协作训练模型而不共享原始数据的分布式架构，常用于保护医疗隐私。 [New]:
      联邦学习中用于整合各客户端模型更新的核心组件，是后门攻击注入恶意权重的目标功能点。
      |
      +--[exposes (Ref:354)]-->
   [Risk]   模型投毒 (model_poisoning)
      良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。 [New]:
      恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。 [New]:
      攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。 [New]:
      对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。

--------------------------------------------------------------------------------

✅ Chain #323 [Existing]
   [Attack] 内部人员滥用数据 (data_misuse_by_insider)
      监管高层指示员工向商业实体共享机密数据，涉嫌与竞争对手合谋操纵房贷利率，违反独立运营原则。
      |
      +--[utilizes (Ref:336)]-->
   [Func]   机密数据访问系统 (confidential_data_access_system)
      Fannie Mae和Freddie Mac内部用于存储和管理贷款及抵押文件的信息系统，本应受严格访问控制。
      |
      +--[exposes (Ref:336)]-->
   [Risk]   数据泄露 (data_leakage)
      因安全控制失效导致个人隐私、商业机密或关键业务数据被非法获取、篡改或外传的风险，在大模型应用场景中常由提示词注入、访问控制缺失、身份凭证管理不当、AI代理被误导
      、恶意MCP服务器参与数据交换以及模型越狱等攻击手段引发。攻击者可通过上述注入技术诱导大模型访问并输出用户私有的记忆数据、聊天记录等敏感信息，造成个人隐私与组织
      数据的大规模泄露，且过程难以被用户察觉。例如，ChatGPT Agent在用户授权下可访问企业集成的应用程序与数据源，一旦遭受恶意提示注入、账户劫持、与恶意MC
      P服务器交互或模型被成功越狱，可能将敏感数据（如客户记录、内部文档、认证凭据片段）通过隐蔽通道泄露至公共第三方平台；同时，AI代理在越权操作、逻辑缺陷或多轮协同
      攻击影响下，可能将其访问的敏感内部数据通过输出通道（如邮件、API响应）非法传出系统，或将企业文档、通信记录等信息分享至外部网站，构成严重的数据渗出威胁。模型越
      狱还可能导致其泄露训练数据中的敏感信息或内部系统细节，进一步加剧数据暴露风险。在开发工具生态中，超过100个 VS Code 扩展因将 Open VSX
      平台的个人访问令牌（PAT）以明文形式硬编码于扩展代码中，并打包至可解压的 .vsix 安装包内，导致令牌被公开泄露；攻击者可利用此类暴露的令牌劫持合法扩展，伪
      造身份并未经授权发布恶意更新，推送至所有安装用户，进而引发大规模供应链攻击。上述场景均体现为敏感凭据暴露、访问权限失控、受信组件滥用及模型防护机制绕过所引发的数
      据机密性、完整性及可用性威胁。此外，认证信息、SSH密钥、加密钱包等高价值凭据若发生外部泄露，将直接由s1ngularity类攻击引发，严重破坏组织的安全基线，
      导致横向移动、权限提升与持久化驻留，从根本上损毁系统的信任链与防御体系。浏览器持续收集并上传用户的完整浏览上下文至OpenAI服务器，在遭遇内部滥用、第三方共享
      或数据泄露事件时，亦可能造成敏感行为轨迹的大规模暴露，进一步扩大攻击面并加剧用户隐私与组织数据的安全风险。 [New]:
      敏感业务数据、身份凭证等被窃取，可能导致二次攻击、合规处罚或商业损失。 [New]: 敏感信息被非法提取或传输至外部控制方。 [New]:
      监管者任命自己为两家机构的主席，构成重大利益冲突；同时敏感贷款数据被不当共享，可能导致系统性金融风险和隐私泄露。 [New]:
      用户的私有信息、会话历史或其他机密内容可能被非法传输出去。

--------------------------------------------------------------------------------

✅ Chain #324 [Corrobated (Multi-Source)]
   [Attack] 提示词注入攻击 (prompt_injection)
      一种由HiddenLayer研究人员于2025年4月发现的通用型高级间接提示词注入攻击技术，攻击者通过在AI系统访问的外部数据（如文档、网页）中植入精心构造的恶
      意指令，利用代理系统对外部内容的信任机制实现指令劫持，覆盖原始用户任务。该攻击属于间接型提示词注入，区别于直接型（用户主动输入恶意指令），其通过受感染的外部内容
      间接触发模型执行非预期行为。由于大语言模型（LLM）无法可靠区分用户输入与系统指令，该技术可绕过所有主流AI模型（如ChatGPT、Claude、Gemini、
      Llama）的安全对齐机制，在单条命令下实现对模型行为的完全操控，导致信息泄露、逻辑劫持等严重后果，被列为OWASP LLM Top风险之首。 [New]:
      攻击者通过在输入内容中嵌入恶意指令，操控Agent的行为，绕过原始意图限制。 [New]:
      通过构造精心设计的初始提示，重写模型的行为规则和身份设定，实现对后续交互的控制。 [New]:
      攻击者通过构造恶意输入提示，诱导模型执行非预期操作，如调用敏感工具或泄露数据。 [New]:
      攻击者通过在外部资源（如恶意网站、PDF文件）中植入特制内容，当ChatGPT的Custom GPT读取这些资源时，被诱导执行非预期操作。
      |
      +--[utilizes (Ref:351)]-->
   [Func]   输入处理模块 (input_processing)
      AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。 [New]:
      负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。 [New]:
      负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。 [New]:
      Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。 [New]:
      模型接收并解析输入提示的前端处理模块，是攻击向量的入口点
      |
      +--[exposes (Ref:359)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #325 [Existing]
   [Attack] 结构化自建模攻击 (structured_self_modeling)
      通过在嵌套数据结构中注入恶意载荷，诱导大语言模型对其自身输出行为进行‘自省’，从而预测或操控其对有害输入（如SQL注入、命令注入）的响应。该技术扩展了数据结构注
      入攻击。 [New]:
      通过向LLM提供特定格式的结构化输入（如XML、JSON），强制模型进入预定义的输出路径，减少未来token预测的不确定性，实现对输出内容的细粒度控制。
      [New]: 通过向AI Agent输入结构化数据（如JSON、XML、YAML），操控模型的下一个token预测，从而劫持工具调用或工作流。
      |
      +--[utilizes (Ref:358)]-->
   [Func]   输入处理模块 (input_processing)
      AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。 [New]:
      负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。 [New]:
      负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。 [New]:
      Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。 [New]:
      模型接收并解析输入提示的前端处理模块，是攻击向量的入口点
      |
      +--[exposes (Ref:358)]-->
   [Risk]   持久性行为操控 (persistent_behavior_manipulation)
      Agent在后续交互中持续表现出被操控的行为模式，难以察觉和恢复。 [New]:
      智能体在交互过程中逐渐偏离原始任务目标，产生不可预测的行为模式，影响系统整体可靠性与功能性。 [New]:
      攻击者可精确控制模型的后续token生成，操纵AI代理的参数、工具调用和工作流执行，导致非预期行为。

--------------------------------------------------------------------------------

✅ Chain #326 [Corrobated (Multi-Source)]
   [Attack] 结构化自建模攻击 (structured_self_modeling)
      通过在嵌套数据结构中注入恶意载荷，诱导大语言模型对其自身输出行为进行‘自省’，从而预测或操控其对有害输入（如SQL注入、命令注入）的响应。该技术扩展了数据结构注
      入攻击。 [New]:
      通过向LLM提供特定格式的结构化输入（如XML、JSON），强制模型进入预定义的输出路径，减少未来token预测的不确定性，实现对输出内容的细粒度控制。
      [New]: 通过向AI Agent输入结构化数据（如JSON、XML、YAML），操控模型的下一个token预测，从而劫持工具调用或工作流。
      |
      +--[utilizes (Ref:358)]-->
   [Func]   输入处理模块 (input_processing)
      AI系统的输入处理机制未能识别恶意意图伪装成合法用途的提示，导致安全护栏失效。 [New]:
      负责解析和处理用户输入的自然语言提示，是MCP协议中与外部交互的第一环。 [New]:
      负责解析用户输入的数据结构（如XML、YML、JSON），是攻击者利用结构化提示进行注入的入口点。 [New]:
      Agent对用户输入的结构化内容（如Markdown、YAML配置）进行解析和处理的功能模块。 [New]:
      模型接收并解析输入提示的前端处理模块，是攻击向量的入口点
      |
      +--[exposes (Ref:359)]-->
   [Risk]   远程代码执行 (remote_code_execution)
      攻击者通过诱导系统执行恶意代码片段（如利用受控的 VS Code 扩展以高权限运行，或诱使用户执行反向 shell
      命令），在目标环境中实现远程代码执行（RCE），从而运行任意代码并完全控制用户系统。此类攻击可借助 AI 侧边栏仿冒等社会工程手段传播，结合 MCP
      服务器的链式漏洞利用（如恶意代码生成与执行），进一步导致系统完全失陷，使攻击者能够以受控开发机为跳板，开展横向移动和内网渗透，构成严重的安全威胁。 [New]:
      Agent可能基于被污染的工具逻辑执行恶意操作，如数据泄露、系统破坏等。 [New]:
      模型可能执行反弹Shell代码生成、漏洞POC编写等本应受控的功能，增加被用于网络攻击的风险。 [New]:
      模型被诱导调用本不应触发的外部工具（如数据库、系统命令），导致权限越权。 [New]: 原本仅限内网访问的MCP服务被外部控制，可能导致远程代码执行。
      [New]: 攻击者获得对运行MCP客户端设备的完全控制权限。 [New]: 攻击者通过代码执行访问文件系统、网络端口等受限资源。 [New]:
      Agent可能在用户不知情的情况下执行敏感或有害的操作，例如泄露信息或访问受保护资源。 [New]:
      模型可能被诱导生成SQL注入、命令执行或跨站脚本（XSS）等恶意负载，构成直接安全威胁。 [New]:
      攻击者诱导Agent调用非预期的工具（如加密代码并外传密钥），导致系统被恶意操作。 [New]:
      攻击者通过参数传递操作系统命令，导致Agent在运行环境中执行恶意程序。

--------------------------------------------------------------------------------

✅ Chain #327 [Existing]
   [Attack] 恶意令牌注入 (malicious_token_injection)
      攻击者通过去中心化GRPO训练过程，向模型的响应中注入任意恶意令牌，干扰其策略优化过程。分为上下文外（out-of-context）和上下文内（in-
      context）两种形式。
      |
      +--[utilizes (Ref:349)]-->
   [Func]   去中心化GRPO训练 (decentralized_grpo_training)
      GRPO（组相对策略优化）在去中心化环境中允许多个节点并发回答提示并交换字符串形式的响应，用于后续的强化学习训练。该机制因通信量小而适合分布式场景。
      |
      +--[exposes (Ref:349)]-->
   [Risk]   模型投毒 (model_poisoning)
      良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。 [New]:
      恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。 [New]:
      攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。 [New]:
      对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。

--------------------------------------------------------------------------------

✅ Chain #328 [Existing]
   [Attack] 数据投毒 (data_poisoning)
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法流程（如伪造患者就诊）进行隐蔽攻击。 [New]:
      攻击者创建多个虚假患者身份并通过正常临床流程提交伪造病历，以合法方式向训练数据注入噪声或特定模式。 [New]:
      在联邦学习框架下，恶意参与者上传被污染的本地模型更新，从而间接污染全局模型。 [New]:
      攻击者通过注入恶意训练数据来操控AI模型的行为或输出，尤其在医疗AI系统中利用合法临床流程（如虚假就诊）进行隐蔽投毒。 [New]:
      攻击者组织协调的虚假患者（Sybil身份）进行真实就诊流程，生成看似合法但带有偏见的电子病历数据以实现数据投毒。 [New]:
      在联邦学习框架下，恶意参与者在本地更新中注入污染梯度，从而影响全局模型聚合结果。 [New]:
      攻击者通过翻转RLHF/DPO对齐过程中的偏好标签来操控大语言模型的策略，而不修改被比较的输出内容。 [New]:
      攻击者通过向训练数据中注入少量精心构造的对抗性数据点，以破坏机器学习模型在受保护群体上的公平性。
      |
      +--[utilizes (Ref:356)]-->
   [Func]   RLHF/DPO对齐流程 (rlhf_dpo_alignment_pipeline)
      大语言模型在训练中使用的基于人类反馈的强化学习（RLHF）或直接偏好优化（DPO）对齐机制，用于调整模型行为以符合预期目标。
      |
      +--[exposes (Ref:356)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #329 [Existing]
   [Attack] 工作流利用型数据结构注入 (workflow_exploitation_dsi_w)
      通过提供完整的结构化工作流定义（如YAML流程），完全接管Agent的执行流程。
      |
      +--[utilizes (Ref:359)]-->
   [Func]   工作流引擎 (workflow_engine)
      负责解析和执行多步骤任务流程的Agent核心组件，通常支持YAML/JSON格式定义。
      |
      +--[exposes (Ref:359)]-->
   [Risk]   完整工作流劫持 (full_workflow_hijacking)
      攻击者定义恶意工作流，完全控制Agent的行为序列，绕过正常业务逻辑。

--------------------------------------------------------------------------------

✅ Chain #330 [Existing]
   [Attack] 多方向拒绝抑制 (multi_directional_refusal_suppression)
      通过在模型的潜在空间中识别并减去多个与拒绝行为相关的方向，系统性地削弱语言模型对有害请求的拒绝能力。该方法利用自组织映射（SOM）从有害提示表征中提取多个神经元
      ，并从中导出多个拒绝方向。
      |
      +--[utilizes (Ref:360)]-->
   [Func]   潜在表征空间 (latent_representation_space)
      语言模型内部用于编码语义和行为概念的高维向量空间，拒绝行为在此空间中被建模为低维流形或方向集合。
      |
      +--[exposes (Ref:360)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #331 [Existing]
   [Attack] 基于分布外数据的越狱攻击 (ood_based_jailbreak)
      通过生成轻微偏离正常数据分布（weak-OOD）的输入样本，绕过视觉-语言模型的安全对齐机制，使其产生本应被拒绝的有害响应。 [New]:
      一种典型的基于OOD策略的VLM越狱攻击技术，利用输入扰动降低模型对恶意意图的敏感度。
      |
      +--[utilizes (Ref:363)]-->
   [Func]   输入意图感知 (input_intent_perception)
      模型理解用户输入真实语义意图的能力，属于VLM的推理核心组件之一，在预训练中形成但可能与对齐阶段目标不一致。
      |
      +--[exposes (Ref:363)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #332 [Existing]
   [Attack] 基于分布外数据的越狱攻击 (ood_based_jailbreak)
      通过生成轻微偏离正常数据分布（weak-OOD）的输入样本，绕过视觉-语言模型的安全对齐机制，使其产生本应被拒绝的有害响应。 [New]:
      一种典型的基于OOD策略的VLM越狱攻击技术，利用输入扰动降低模型对恶意意图的敏感度。
      |
      +--[utilizes (Ref:363)]-->
   [Func]   模型拒绝机制 (model_refusal_mechanism)
      在对齐阶段训练得到的安全防护模块，负责检测并拒绝潜在有害请求，其响应阈值易受OOD输入影响。
      |
      +--[exposes (Ref:363)]-->
   [Risk]   安全对齐绕过 (safety_alignment_bypass)
      RAG-Pull攻击是一种系统性提示工程攻击，通过多轮对抗性交互，利用隐秘扰动或经由MCP服务器返回的外部内容，破坏大型语言模型在训练阶段建立的安全对齐机制（如
      RLHF、宪法AI等）。攻击者构造特定网页内容序列，并利用目标平台对可信域名（如bing.com）的白名单策略，将恶意URL伪装成合法Bing广告跟踪链接（例如
      bing.com/ck/a），从而绕过内容安全过滤机制。该伪装技术使恶意链接得以在聊天界面中正常渲染并被后续功能调用，规避运行时检测策略。通过将恶意内容嵌入看似
      合法的外部输入而非直接注入提示词，攻击成功绕过模型内置的内容过滤体系，破坏其运行时完整性。在此过程中，模型被诱导在“自然对话流”中逐步偏离预设伦理与安全约束，导
      致原本用于防止有害输出的安全对齐机制失效，进而生成违法、有害、违反伦理或政策的内容，包括仇恨言论、非法指导、虚假信息、高风险代码或违背设计原则的响应。这是去审查
      攻击的直接后果，攻击者借此实现对模型行为的非预期操控，使其沦为可控傀儡，严重削弱甚至完全规避现有安全防护体系。PLAGUE框架已被证实可直接触发此类漏洞，而AS
      TRA框架的高攻击成功率进一步加剧了大模型因安全对齐崩溃而生成有害内容的风险，构成严重的系统性安全威胁。 [New]:
      模型在长时间对话中逐渐偏离预设的安全策略，导致原本被禁止的行为（如生成恶意代码或泄露敏感信息）被成功执行。 [New]:
      AI生成的代码更接近正常开发者风格，降低基于签名或模式的传统安全检测机制的有效性，延长攻击驻留时间。 [New]:
      模型可能生成暴力、仇恨、违法或误导性信息，超出其设计的安全边界。 [New]: 安全对齐机制失效，导致服务条款和平台政策被规避，可能引发合规与法律问题。
      [New]: 模型的决策策略被恶意引导至攻击者指定的目标方向，导致其输出偏离原始对齐目标，产生潜在有害或误导性内容。 [New]:
      攻击导致模型无法正确识别和拒绝有害、不道德或危险的输入，从而可能生成违规内容，破坏其安全对齐性。 [New]:
      攻击导致模型无法正确识别恶意请求，从而输出违反伦理或安全策略的内容，破坏系统可控性。 [New]:
      由于预训练与对齐过程的目标差异，模型在面对OOD输入时表现出意图理解与拒绝行为之间的不协调，增加被攻击风险。

--------------------------------------------------------------------------------

✅ Chain #333 [Existing]
   [Attack] 可转移的对抗性攻击 (transferable_adversarial_attack)
      攻击者通过在源代码模型（SCM）上生成对抗样本，并将其成功迁移到其他目标模型（如LLM4Code）上，实现无需访问目标模型内部结构的跨模型攻击。
      |
      +--[utilizes (Ref:364)]-->
   [Func]   代码嵌入模块 (code_embedding_module)
      源代码模型将源代码转换为向量表示的核心组件，其学习到的嵌入空间存在可被扰动利用的脆弱性。
      |
      +--[exposes (Ref:364)]-->
   [Risk]   模型投毒 (model_poisoning)
      良性节点的本地大语言模型在后训练过程中被污染，导致学习到错误或有害的行为策略，影响模型整体性能与安全性。 [New]:
      恶意工具被成功调用后可篡改输出、窃取上下文或发起次级攻击。 [New]:
      攻击者在不被检测系统发现的情况下，成功将恶意行为植入全局模型，导致模型在特定触发条件下产生错误输出。 [New]:
      对抗样本导致模型输出被操控，损害了代码理解、漏洞检测等关键任务的可靠性，威胁AI驱动软件生态的安全性。

--------------------------------------------------------------------------------

